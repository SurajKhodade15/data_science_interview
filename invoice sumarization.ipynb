{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872ee7e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# üîπ Interview-Style Q\\&A ‚Äì Invoice Summarization and Chatbot Assistant\n",
    "\n",
    "**Q1. Can you explain the Invoice Summarization and Chatbot Assistant project?**\n",
    "**A:**\n",
    "‚ÄúAt Globant, I built a Generative AI-powered chatbot that could summarize invoices and handle finance-related queries in natural language. The system used LangChain with OpenAI LLMs, combined with Chroma DB for storing past invoices. It also generated automated finance reports, which reduced summarization time by nearly 40% compared to manual processes.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. What was the architecture of the system?**\n",
    "**A:**\n",
    "‚ÄúThe pipeline was:\n",
    "\n",
    "1. Invoices (PDF/Excel/structured) were ingested and parsed into text.\n",
    "2. Chunks were embedded using OpenAI embeddings and stored in Chroma DB.\n",
    "3. When a user asked a question ‚Äî for example, *‚ÄòWhat‚Äôs the total tax across invoices this month?‚Äô* ‚Äî the system retrieved relevant chunks from Chroma.\n",
    "4. LangChain passed the retrieved context into GPT, which produced a concise answer or summary.\n",
    "5. The backend, built on FastAPI, served endpoints for `/ingest_invoice`, `/summarize_invoice`, and `/query` for finance teams.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you optimize invoice summarization for speed and cost?**\n",
    "**A:**\n",
    "‚ÄúI applied text preprocessing to remove redundant details before sending invoices to the LLM, which cut token usage. I also cached embeddings for previously processed invoices in Chroma DB to avoid re-processing. Additionally, frequent queries were cached using Redis. These optimizations reduced summarization time by \\~40% and cut inference costs.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. How did you ensure accuracy in financial summaries?**\n",
    "**A:**\n",
    "‚ÄúI used Retrieval-Augmented Generation (RAG) with strict prompting, ensuring GPT only answered based on retrieved invoice text. I also enforced structured outputs (JSON with fields like invoice\\_id, amount, tax, due\\_date). Finally, the chatbot returned citations from invoice sections, so finance users could verify answers.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What business impact did this project deliver?**\n",
    "**A:**\n",
    "‚ÄúThe assistant automated invoice summarization and query handling, reducing manual finance team effort. It accelerated report preparation by \\~40% and improved accuracy in expense tracking. This freed up finance staff to focus on higher-value analysis rather than repetitive summarization.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How would you extend this solution in the future?**\n",
    "**A:**\n",
    "‚ÄúFuture improvements could include multimodal support to directly process scanned invoice images using OCR, integrating with ERP systems for real-time data, and adding anomaly detection to flag unusual amounts or missing tax details. This would make the assistant not just reactive but proactive in financial oversight.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ Demo Code ‚Äì `invoice_assistant.py`\n",
    "\n",
    "Here‚Äôs a **minimal working demo** (you can expand later). It uses **LangChain + OpenAI + Chroma + FastAPI**.\n",
    "\n",
    "```python\n",
    "# invoice_assistant.py\n",
    "\n",
    "from fastapi import FastAPI, UploadFile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "\n",
    "app = FastAPI(title=\"Invoice Summarization & Query Assistant\")\n",
    "\n",
    "# Embeddings + Vector DB setup\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "persist_dir = \"invoice_db\"\n",
    "vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embedding_model)\n",
    "\n",
    "# LLM setup\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Prompt template for invoice summarization\n",
    "SUMMARY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"Summarize the following invoice text:\\n\\n{context}\\n\\nProvide key details like invoice id, vendor, total amount, tax, and due date.\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "@app.post(\"/ingest_invoice\")\n",
    "async def ingest_invoice(file: UploadFile):\n",
    "    \"\"\"Ingest invoice text into Chroma DB\"\"\"\n",
    "    text = await file.read()\n",
    "    text = text.decode(\"utf-8\")  # Assuming text/CSV for demo. Use PyPDF2/Docx for real invoices.\n",
    "\n",
    "    # Split and embed\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = splitter.create_documents([text])\n",
    "    vectorstore.add_documents(docs)\n",
    "    vectorstore.persist()\n",
    "    return {\"status\": \"Invoice ingested successfully\", \"chunks\": len(docs)}\n",
    "\n",
    "@app.get(\"/summarize_invoice\")\n",
    "async def summarize_invoice(invoice_id: str):\n",
    "    \"\"\"Summarize invoice from DB\"\"\"\n",
    "    results = vectorstore.similarity_search(invoice_id, k=1)\n",
    "    if not results:\n",
    "        return {\"error\": \"Invoice not found\"}\n",
    "    summary = llm.predict(SUMMARY_PROMPT.format(context=results[0].page_content))\n",
    "    return {\"invoice_id\": invoice_id, \"summary\": summary}\n",
    "\n",
    "@app.get(\"/query\")\n",
    "async def query_invoice(query: str):\n",
    "    \"\"\"Ask questions over invoices\"\"\"\n",
    "    answer = qa_chain.run(query)\n",
    "    return {\"query\": query, \"answer\": answer}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How to Run\n",
    "\n",
    "1. Save as `invoice_assistant.py`.\n",
    "2. Install dependencies:\n",
    "\n",
    "   ```bash\n",
    "   pip install fastapi uvicorn langchain langchain-openai chromadb\n",
    "   ```\n",
    "3. Run server:\n",
    "\n",
    "   ```bash\n",
    "   uvicorn invoice_assistant:app --reload\n",
    "   ```\n",
    "4. Test endpoints in browser:\n",
    "\n",
    "   * `POST /ingest_invoice` ‚Üí Upload text invoice\n",
    "   * `GET /summarize_invoice?invoice_id=INV123`\n",
    "   * `GET /query?query=What is the total tax this month?`\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73946db9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
