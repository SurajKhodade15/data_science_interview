{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3f9c21",
   "metadata": {},
   "source": [
    "\n",
    "# üìå Q1. How do you treat heteroscedasticity in regression?\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Definition**\n",
    "\n",
    "Heteroscedasticity means that the **variance of errors (residuals) is not constant** across all levels of the independent variable(s).\n",
    "This violates one of the key assumptions of **linear regression** (constant variance of errors = *homoscedasticity*).\n",
    "\n",
    "* When present, it can make:\n",
    "\n",
    "  * Coefficient estimates still **unbiased**,\n",
    "  * But **standard errors wrong** ‚Üí leading to incorrect p-values & unreliable hypothesis tests.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Causes**\n",
    "\n",
    "* Outliers or influential data points\n",
    "* Skewed distribution of variables\n",
    "* Wrong functional form (e.g., using linear when relationship is nonlinear)\n",
    "* Scale differences in predictors\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Detection**\n",
    "\n",
    "* **Residual vs Fitted plot** ‚Üí Funnel shape indicates heteroscedasticity\n",
    "* **Breusch‚ÄìPagan test**\n",
    "* **White test**\n",
    "* **Goldfeld‚ÄìQuandt test**\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Treatment Methods**\n",
    "\n",
    "1. **Transform the dependent variable**\n",
    "\n",
    "   * Apply transformations like:\n",
    "\n",
    "     * Log(y), ‚àöy, Box-Cox\n",
    "   * Helps stabilize variance.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   y_transformed = np.log(y)   # Example\n",
    "   ```\n",
    "\n",
    "2. **Weighted Least Squares (WLS)**\n",
    "\n",
    "   * Give smaller weights to data points with higher variance.\n",
    "   * Regression minimizes **weighted residuals**.\n",
    "\n",
    "   ```python\n",
    "   import statsmodels.api as sm\n",
    "   wls_model = sm.WLS(y, X, weights=1/(abs(residuals))).fit()\n",
    "   ```\n",
    "\n",
    "3. **Robust Standard Errors (Heteroscedasticity-consistent SE)**\n",
    "\n",
    "   * Use **HC standard errors** (e.g., White‚Äôs correction).\n",
    "   * Coefficients remain same, but SEs are adjusted.\n",
    "\n",
    "   ```python\n",
    "   ols_model = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "   ```\n",
    "\n",
    "4. **Model Redesign**\n",
    "\n",
    "   * Add missing variables\n",
    "   * Use nonlinear models (polynomial regression, tree-based methods, etc.)\n",
    "   * Feature scaling / transformation of predictors\n",
    "\n",
    "5. **Generalized Least Squares (GLS)**\n",
    "\n",
    "   * Explicitly models error structure.\n",
    "   * Useful if heteroscedasticity pattern is well understood.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Interview-Style Summary**\n",
    "\n",
    "üëâ Heteroscedasticity means error variance is unequal.\n",
    "üëâ It does not bias coefficients but makes hypothesis testing unreliable.\n",
    "üëâ Detection: residual plots, BP test, White test.\n",
    "üëâ Treatment: log/Box-Cox transforms, WLS, GLS, or robust standard errors.\n",
    "üëâ In practice, **robust standard errors or log transformation** are the most common fixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd9e54",
   "metadata": {},
   "source": [
    "### ‚ùì Q2. What is Multicollinearity, and how do you treat it?\n",
    "\n",
    "**üîπ Definition:**\n",
    "Multicollinearity occurs when two or more independent (predictor) variables in a regression model are **highly correlated** with each other.\n",
    "\n",
    "* This makes it difficult for the model to determine the **unique effect** of each predictor on the target variable.\n",
    "* In extreme cases, it leads to unstable coefficients and inflated standard errors.\n",
    "\n",
    "---\n",
    "\n",
    "**üîπ Example:**\n",
    "Suppose you are predicting `house_price` using `size_in_sqft` and `number_of_rooms`.\n",
    "\n",
    "* Since larger houses generally have more rooms, these two predictors may be highly correlated, causing multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "**üîπ Problems caused by Multicollinearity:**\n",
    "\n",
    "1. Coefficients become **unstable** (small changes in data ‚Üí large changes in Œ≤).\n",
    "2. Inflated **standard errors** ‚Üí t-tests may wrongly show predictors as insignificant.\n",
    "3. Difficulty in interpreting predictor importance.\n",
    "\n",
    "---\n",
    "\n",
    "**üîπ Detection Methods:**\n",
    "\n",
    "* **Correlation Matrix:** High correlation (‚â• 0.8 or 0.9) between predictors.\n",
    "* **Variance Inflation Factor (VIF):**\n",
    "\n",
    "  * VIF > 5 (sometimes > 10) indicates high multicollinearity.\n",
    "\n",
    "```python\n",
    "# Example: Checking VIF in Python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[['size_in_sqft', 'num_rooms', 'num_bathrooms']]  # predictors\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üîπ Treatment / Remedies:**\n",
    "\n",
    "1. **Remove one of the correlated variables** (e.g., drop `num_rooms` if highly correlated with `size_in_sqft`).\n",
    "2. **Combine variables** using **PCA (Principal Component Analysis)** or feature engineering.\n",
    "3. **Regularization (Ridge/Lasso Regression):**\n",
    "\n",
    "   * Ridge shrinks correlated coefficients toward each other.\n",
    "   * Lasso can drop redundant variables by assigning zero coefficients.\n",
    "4. **Collect more data** (sometimes multicollinearity is reduced with a larger dataset).\n",
    "5. **Centering variables (standardization/mean-centering):** Helps reduce correlation in polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Interview Tip:**\n",
    "\n",
    "> ‚ÄúMulticollinearity doesn‚Äôt reduce the predictive power of the model drastically, but it affects interpretability. So, whether to treat it depends on the objective ‚Äì prediction vs. interpretation.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24123f5",
   "metadata": {},
   "source": [
    "## Q3. What is Market Basket Analysis? How would you do it in Python?\n",
    "\n",
    "### üìå Theory & Intuition\n",
    "\n",
    "* **Market Basket Analysis (MBA)** is a technique used in retail and e-commerce to understand the **purchase behavior of customers**.\n",
    "* It identifies **associations or co-occurrence relationships** between items purchased together.\n",
    "* Example: If a customer buys *bread*, they are more likely to buy *butter*.\n",
    "\n",
    "### üîπ Key Concepts\n",
    "\n",
    "* **Association Rule Mining**: Finds relationships between items.\n",
    "* **Support**: Probability of items appearing together in transactions.\n",
    "\n",
    "  $$\n",
    "  \\text{Support(A ‚Üí B)} = \\frac{\\text{Transactions containing (A ‚à™ B)}}{\\text{Total transactions}}\n",
    "  $$\n",
    "* **Confidence**: Probability of buying B given A.\n",
    "\n",
    "  $$\n",
    "  \\text{Confidence(A ‚Üí B)} = \\frac{\\text{Support(A ‚à™ B)}}{\\text{Support(A)}}\n",
    "  $$\n",
    "* **Lift**: Strength of association relative to independence.\n",
    "\n",
    "  $$\n",
    "  \\text{Lift(A ‚Üí B)} = \\frac{\\text{Confidence(A ‚Üí B)}}{\\text{Support(B)}}\n",
    "  $$\n",
    "\n",
    "  * Lift > 1 ‚Üí Positive association\n",
    "  * Lift = 1 ‚Üí Independent\n",
    "  * Lift < 1 ‚Üí Negative association\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Python Example (Using `mlxtend`)\n",
    "\n",
    "```python\n",
    "# Install mlxtend if not available\n",
    "# !pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Sample dataset: Transactions\n",
    "dataset = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'diapers', 'beer'],\n",
    "    ['milk', 'bread', 'diapers', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'diapers', 'beer', 'cola']\n",
    "]\n",
    "\n",
    "# Convert dataset into one-hot encoded DataFrame\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "print(\"One-Hot Encoded Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 1: Find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "print(\"\\nFrequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Step 2: Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents','consequents','support','confidence','lift']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Output Interpretation\n",
    "\n",
    "* The rules table will show:\n",
    "\n",
    "  * **Antecedents ‚Üí Consequents** (e.g., `{milk} ‚Üí {bread}`)\n",
    "  * **Support** (how often they occur together)\n",
    "  * **Confidence** (likelihood of consequent given antecedent)\n",
    "  * **Lift** (strength of the relationship)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Use Cases\n",
    "\n",
    "* **Retail**: Product bundling (e.g., ‚ÄúBuy chips, get soda‚Äù).\n",
    "* **E-commerce**: Recommender systems (‚ÄúCustomers also bought‚Ä¶‚Äù).\n",
    "* **Healthcare**: Drug prescription patterns.\n",
    "* **Finance**: Fraud detection (suspicious transaction combinations).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ In interviews, mention both **theory (support, confidence, lift)** and **implementation (Apriori in `mlxtend`)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa95be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "894ff5ad",
   "metadata": {},
   "source": [
    "\n",
    "## Q4. What is Association Analysis? Where is it used?\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Definition\n",
    "\n",
    "* Association Analysis is a **data mining technique** used to discover **relationships, patterns, or associations** between variables/items in large datasets.\n",
    "* It identifies **if-then rules** (called **association rules**) of the form:\n",
    "\n",
    "  ```\n",
    "  IF item A ‚Üí THEN item B\n",
    "  ```\n",
    "\n",
    "  Example: \"If a customer buys bread, they are likely to buy butter.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Concepts\n",
    "\n",
    "* **Support** ‚Üí Frequency of an itemset in the dataset.\n",
    "* **Confidence** ‚Üí Likelihood that item B is bought when item A is bought.\n",
    "* **Lift** ‚Üí Strength of association compared to random chance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Where is it used?\n",
    "\n",
    "1. **Market Basket Analysis**\n",
    "\n",
    "   * Retail stores to identify products often bought together.\n",
    "   * Example: Amazon \"Frequently Bought Together\".\n",
    "\n",
    "2. **Recommender Systems**\n",
    "\n",
    "   * Suggesting items/movies based on association rules.\n",
    "\n",
    "3. **Cross-selling / Upselling**\n",
    "\n",
    "   * Banks recommending credit cards with savings accounts.\n",
    "\n",
    "4. **Healthcare**\n",
    "\n",
    "   * Finding correlations between symptoms and diseases.\n",
    "\n",
    "5. **Fraud Detection**\n",
    "\n",
    "   * Discover unusual item combinations in financial transactions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Python Example (Using Apriori)\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'jam']\n",
    "]\n",
    "\n",
    "# Convert dataset to one-hot encoded DataFrame\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "data = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(data, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "\n",
    "# Generate rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "print(rules[['antecedents','consequents','support','confidence','lift']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Interview Tip\n",
    "\n",
    "üëâ Always connect **Association Analysis** with **Market Basket Analysis**, as that‚Äôs the most common use case.\n",
    "üëâ Highlight **Support, Confidence, Lift** since interviewers often test your grasp on these metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781c809",
   "metadata": {},
   "source": [
    "## ‚ùì Q5. What is KNN Classifier?\n",
    "\n",
    "### üîπ Intuition\n",
    "\n",
    "The **K-Nearest Neighbors (KNN)** algorithm is a **supervised learning method** used for both **classification and regression**.\n",
    "It makes predictions based on the **majority class (for classification)** or **average values (for regression)** of the *k closest data points* in the feature space.\n",
    "\n",
    "It assumes:\n",
    "\n",
    "> \"Similar data points exist close to each other in the feature space.\"\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ How It Works\n",
    "\n",
    "1. Choose a value of **k** (number of neighbors).\n",
    "2. Compute the **distance** (Euclidean, Manhattan, or Minkowski) between the test sample and all training samples.\n",
    "3. Select the **k nearest neighbors**.\n",
    "4. Perform:\n",
    "\n",
    "   * **Classification:** Assign the most frequent class among neighbors.\n",
    "   * **Regression:** Take the average of neighbors‚Äô values.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Advantages\n",
    "\n",
    "* Simple and intuitive.\n",
    "* No training phase (lazy learning).\n",
    "* Works well for small datasets.\n",
    "\n",
    "### üîπ Disadvantages\n",
    "\n",
    "* Computationally expensive for large datasets.\n",
    "* Sensitive to irrelevant/noisy features and scaling.\n",
    "* Performance depends on the choice of **k** and distance metric.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Applications\n",
    "\n",
    "* **Recommendation Systems** (e.g., suggesting movies based on similar users).\n",
    "* **Medical Diagnosis** (classifying diseases based on symptoms).\n",
    "* **Anomaly Detection** (detecting fraud transactions).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Python Example\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In short:**\n",
    "KNN is a **distance-based, instance-learning algorithm** that classifies points based on the majority vote of nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bda428",
   "metadata": {},
   "source": [
    "### **Q6. What is Pipeline in sklearn?**\n",
    "\n",
    "**Answer:**\n",
    "In machine learning, particularly when using **scikit-learn**, a `Pipeline` is a high-level utility that helps streamline workflows by chaining multiple data preprocessing steps and model training steps into a single, cohesive object. The main purpose is to ensure that the exact sequence of transformations and modeling is executed consistently, reproducibly, and without data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Pipeline is important?**\n",
    "\n",
    "1. **Avoids Data Leakage:**\n",
    "   When we fit transformations (e.g., scaling, encoding, imputation) on the full dataset before splitting into train/test, the model unintentionally gains information from the test set. A Pipeline prevents this because transformations are fit only on the training data within cross-validation or train-test split.\n",
    "\n",
    "2. **Improves Reproducibility:**\n",
    "   Instead of manually remembering the sequence of steps (impute ‚Üí scale ‚Üí encode ‚Üí model), the pipeline bundles them into one object. Running `.fit()` and `.predict()` executes the entire workflow consistently.\n",
    "\n",
    "3. **Simplifies Model Tuning:**\n",
    "   With scikit-learn‚Äôs `GridSearchCV` or `RandomizedSearchCV`, we can optimize hyperparameters not just for the model, but also for preprocessing steps inside the pipeline.\n",
    "\n",
    "4. **Production Readiness:**\n",
    "   A pipeline object can be easily saved (`joblib` or `pickle`) and deployed, ensuring the exact preprocessing logic used during training is applied during inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Pipeline is Important?\n",
    "\n",
    "Prevents Data Leakage ‚Äì transformations are fitted only on training data.\n",
    "\n",
    "Ensures Reproducibility ‚Äì the same sequence of steps is applied every time.\n",
    "\n",
    "Simplifies Hyperparameter Tuning ‚Äì integrates with GridSearchCV/RandomizedSearchCV.\n",
    "\n",
    "Deployment Friendly ‚Äì preprocessing + model bundled as one deployable object.\n",
    "\n",
    "Cleaner Code ‚Äì avoids repetitive manual preprocessing steps.\n",
    "\n",
    "### How it Works:\n",
    "\n",
    "Pipeline is defined as a sequence of (name, transformer/estimator) steps.\n",
    "\n",
    "All steps except the last must be transformers (fit, transform).\n",
    "\n",
    "The last step is usually a model/estimator (fit, predict).\n",
    "### **How it works?**\n",
    "\n",
    "A pipeline is built as a list of `(name, transformer/model)` pairs. Every step except the last one must be a **transformer** (implementing `.fit` and `.transform`). The final step is typically an **estimator** (like Logistic Regression, Random Forest, etc.), which implements `.fit` and `.predict`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define pipeline steps\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),   # handle missing values\n",
    "    ('scaler', StandardScaler()),                  # feature scaling\n",
    "    ('model', LogisticRegression())                # final estimator\n",
    "])\n",
    "\n",
    "# Fit and predict using pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "* First, missing values are imputed.\n",
    "* Then, the data is scaled.\n",
    "* Finally, Logistic Regression is trained.\n",
    "\n",
    "When you call `pipeline.predict(X_test)`, it automatically applies the **same sequence** of steps to your test data before predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases in Real-World Projects:**\n",
    "\n",
    "* **Customer Churn Prediction:** Handle categorical encoding, imputation, and classification in one pipeline.\n",
    "* **Image or Text Classification:** Combine feature extraction (e.g., TF-IDF, PCA) and classification models.\n",
    "* **Model Deployment:** Ensure data received in production is preprocessed in exactly the same way as during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Statement (Good to Close in Interview):**\n",
    "\n",
    "\"In short, sklearn‚Äôs `Pipeline` is a powerful abstraction that enforces clean, reproducible, and leak-free workflows. It not only simplifies code maintenance but also makes hyperparameter tuning and deployment much more reliable in real-world machine learning projects.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d18b0",
   "metadata": {},
   "source": [
    "\n",
    "### **Q7. What is Principal Component Analysis (PCA), and why do we use it?**\n",
    "\n",
    "‚úÖ **Definition:**\n",
    "\n",
    "* **Principal Component Analysis (PCA)** is a statistical technique used for **dimensionality reduction**.\n",
    "* It transforms the original correlated features into a new set of **uncorrelated variables** called **principal components**.\n",
    "* These components capture the **maximum variance** present in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics of PCA:**\n",
    "\n",
    "1. **Linear Transformation** ‚Äì projects data into a new coordinate system.\n",
    "2. **Principal Components (PCs):**\n",
    "\n",
    "   * 1st PC ‚Üí captures the maximum variance.\n",
    "   * 2nd PC ‚Üí captures the next highest variance orthogonal to the first.\n",
    "   * And so on.\n",
    "3. **Orthogonality:** Components are uncorrelated and independent in direction.\n",
    "4. **Variance Retention:** Usually, only the top *k* components are selected to retain most of the information while reducing dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Do We Use PCA?**\n",
    "\n",
    "1. **Dimensionality Reduction** ‚Äì reduce features while preserving most of the information.\n",
    "2. **Noise Reduction** ‚Äì removes less informative (low variance) components.\n",
    "3. **Improves Model Efficiency** ‚Äì fewer features ‚Üí faster training and lower computation cost.\n",
    "4. **Avoids Multicollinearity** ‚Äì transforms correlated features into independent components.\n",
    "5. **Visualization** ‚Äì helps in visualizing high-dimensional data in 2D or 3D.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in PCA (How it Works):**\n",
    "\n",
    "1. **Standardize the Data** ‚Äì scale features (important because PCA is variance-based).\n",
    "2. **Compute Covariance Matrix** ‚Äì find relationships between features.\n",
    "3. **Eigen Decomposition / SVD** ‚Äì calculate eigenvalues & eigenvectors of covariance matrix.\n",
    "4. **Select Top-k Components** ‚Äì based on explained variance ratio.\n",
    "5. **Project Data** ‚Äì transform original features into new principal component space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in sklearn:**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_pca.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Use Cases:**\n",
    "\n",
    "* **Image Compression:** Reduce pixel features while preserving visual quality.\n",
    "* **Genomics / Bioinformatics:** Handle thousands of gene features.\n",
    "* **Finance:** Analyze correlated stock movements.\n",
    "* **Text Data (NLP):** Reduce high-dimensional embeddings before classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"PCA is a powerful dimensionality reduction technique that simplifies high-dimensional datasets by transforming features into orthogonal components, capturing maximum variance, reducing redundancy, and improving both interpretability and efficiency of ML models.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da56b6",
   "metadata": {},
   "source": [
    "### **Q10. How to evaluate that data does not have any outliers?**\n",
    "\n",
    "‚úÖ **Definition Context:**\n",
    "\n",
    "* Outliers are data points that deviate significantly from the general distribution.\n",
    "* Evaluating the presence (or absence) of outliers is crucial because they can **skew statistical analysis, bias model training, and reduce model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ways to Evaluate Outliers:**\n",
    "\n",
    "#### 1. **Statistical Methods**\n",
    "\n",
    "* **Z-Score / Standard Deviation Rule:**\n",
    "\n",
    "  * Data points with |Z| > 3 are potential outliers.\n",
    "  * Assumes data is approximately normal.\n",
    "* **IQR (Interquartile Range) Method:**\n",
    "\n",
    "  * Compute Q1 (25th percentile) and Q3 (75th percentile).\n",
    "  * Any point < Q1 ‚Äì 1.5*IQR or > Q3 + 1.5*IQR is flagged as an outlier.\n",
    "* **Modified Z-score (Robust Method):**\n",
    "\n",
    "  * Based on Median Absolute Deviation (MAD), works better for skewed data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Visualization Methods**\n",
    "\n",
    "* **Boxplot:** Outliers appear as individual points beyond whiskers.\n",
    "* **Histogram / Density Plot:** Extreme values can be visually detected.\n",
    "* **Scatter Plot (2D/3D):** Detects outliers in multi-feature relationships.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Model-Based / Multivariate Methods**\n",
    "\n",
    "* **Isolation Forest:** Detects anomalies by random partitioning.\n",
    "* **DBSCAN Clustering:** Points not belonging to any cluster may be outliers.\n",
    "* **Mahalanobis Distance:** Considers correlation between variables; points with high distance are outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Conclude ‚ÄúNo Outliers‚Äù?**\n",
    "\n",
    "1. **Statistical Check:** No points exceed thresholds (e.g., |Z| < 3, within IQR bounds).\n",
    "2. **Visual Check:** Plots show no extreme deviations.\n",
    "3. **Model-Based Check:** Anomaly detection algorithms mark very few or zero points as outliers.\n",
    "4. **Domain Validation:** Confirm with subject-matter experts‚Äîsometimes extreme values are legitimate (e.g., very high income in finance).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example in Python:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example using IQR\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['feature'] < (Q1 - 1.5 * IQR)) | (df['feature'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "if outliers.empty:\n",
    "    print(\"No outliers detected.\")\n",
    "else:\n",
    "    print(f\"{len(outliers)} outliers detected.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"To evaluate if data has no outliers, we use a mix of **statistical thresholds, visualization techniques, and anomaly detection algorithms**. If none of these methods flag unusual points, and domain experts confirm the ranges, we can reasonably conclude that the dataset is free from outliers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec5589",
   "metadata": {},
   "source": [
    "### **Q11. What do you do if there are outliers?**\n",
    "\n",
    "‚úÖ **Context:**\n",
    "\n",
    "* Outliers are data points that deviate significantly from the rest of the dataset.\n",
    "* They may be due to **measurement errors, data entry errors, rare events, or genuine extreme cases**.\n",
    "* The approach depends on the **context, dataset size, model sensitivity, and business requirements**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Identify the Nature of Outliers**\n",
    "\n",
    "1. **Error vs Legitimate Value**\n",
    "\n",
    "   * If it‚Äôs a data entry or sensor error ‚Üí correct or remove it.\n",
    "   * If it‚Äôs a valid rare event (e.g., fraud detection, high-value customer) ‚Üí keep it.\n",
    "2. **Univariate vs Multivariate**\n",
    "\n",
    "   * Check if outliers are extreme only in one variable or in combination of features.\n",
    "3. **Domain Knowledge Validation**\n",
    "\n",
    "   * Validate with subject-matter experts before removing rare but important cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Possible Strategies to Handle Outliers**\n",
    "\n",
    "1. **Remove Outliers (Deletion)**\n",
    "\n",
    "   * Use only if dataset is large and outliers are confirmed to be erroneous.\n",
    "   * Example: Removing sensor glitches or invalid negative ages.\n",
    "\n",
    "2. **Transformation / Scaling**\n",
    "\n",
    "   * Apply **log, square root, Box-Cox, or Yeo-Johnson** transformations to reduce impact.\n",
    "   * Robust scaling (median & IQR-based) instead of standard scaling.\n",
    "\n",
    "3. **Cap or Winsorize Values**\n",
    "\n",
    "   * Replace extreme values with upper/lower thresholds (e.g., 1st and 99th percentile).\n",
    "   * Common in financial datasets with long-tailed distributions.\n",
    "\n",
    "4. **Imputation**\n",
    "\n",
    "   * Replace extreme values with mean, median, or domain-specific values.\n",
    "   * Example: Cap extreme house prices at market-acceptable limits.\n",
    "\n",
    "5. **Model-Based Treatment**\n",
    "\n",
    "   * Use models robust to outliers (e.g., tree-based models, Random Forest, XGBoost).\n",
    "   * Apply anomaly detection algorithms (Isolation Forest, DBSCAN, LOF) to handle them separately.\n",
    "\n",
    "6. **Separate Treatment for Rare Events**\n",
    "\n",
    "   * If outliers represent rare but important cases (fraud, rare disease), create a separate class or special handling mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Example in Python**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Capping outliers using percentiles\n",
    "lower, upper = np.percentile(df['feature'], [1, 99])\n",
    "df['feature'] = np.clip(df['feature'], lower, upper)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"When dealing with outliers, the key is not to blindly remove them but to **analyze their cause, validate with domain knowledge, and choose a handling strategy**. Options include removal, transformation, capping, or using robust models. Importantly, if outliers represent rare but meaningful events, they should be retained and treated as critical information rather than discarded.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f525e1",
   "metadata": {},
   "source": [
    "\n",
    "### **Q12. What are the encoding techniques you have applied? Give examples.**\n",
    "\n",
    "‚úÖ **Context:**\n",
    "\n",
    "* Many ML algorithms work only with **numerical data**, so categorical features must be encoded.\n",
    "* The choice of encoding depends on the **type of categorical variable (nominal vs ordinal)**, **model type**, and **data size**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Label Encoding**\n",
    "\n",
    "* **Definition:** Assigns a unique integer to each category.\n",
    "* **Use Case:** Ordinal categorical variables (e.g., education level).\n",
    "* **Limitation:** Implies order even if not meaningful (problem for nominal data with linear models).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['education_level'] = le.fit_transform(df['education_level'])\n",
    "# ['High School','Bachelors','Masters'] ‚Üí [0,1,2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. One-Hot Encoding**\n",
    "\n",
    "* **Definition:** Creates binary (0/1) columns for each category.\n",
    "* **Use Case:** Nominal variables without natural order.\n",
    "* **Limitation:** Can lead to **high dimensionality** if many categories.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(df, columns=['gender'], drop_first=True)\n",
    "# 'Male','Female' ‚Üí one column ['gender_Male'] (1 if Male else 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ordinal Encoding**\n",
    "\n",
    "* **Definition:** Assigns integers based on order/rank.\n",
    "* **Use Case:** Ordinal variables with a defined hierarchy (e.g., low < medium < high).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder(categories=[['Low','Medium','High']])\n",
    "df['priority'] = oe.fit_transform(df[['priority']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Frequency / Count Encoding**\n",
    "\n",
    "* **Definition:** Replace categories with their frequency or count in the dataset.\n",
    "* **Use Case:** High-cardinality categorical variables.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "df['city_freq'] = df['city'].map(df['city'].value_counts())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Target Encoding (Mean Encoding)**\n",
    "\n",
    "* **Definition:** Replace category with the mean of the target variable for that category.\n",
    "* **Use Case:** Useful in classification problems with categorical features.\n",
    "* **Caution:** Must apply with cross-validation to avoid leakage.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "df['category_encoded'] = df.groupby('category')['target'].transform('mean')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Binary Encoding**\n",
    "\n",
    "* **Definition:** Converts categories into binary code, reduces dimensionality compared to one-hot.\n",
    "* **Use Case:** High-cardinality variables.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "!pip install category_encoders\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "be = BinaryEncoder(cols=['city'])\n",
    "df = be.fit_transform(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Hashing Encoding**\n",
    "\n",
    "* **Definition:** Maps categories into fixed-size integers using a hash function.\n",
    "* **Use Case:** Very high-cardinality categorical variables (e.g., user IDs).\n",
    "* **Limitation:** Possible collisions (different categories mapped to same value).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"In practice, I‚Äôve applied multiple encoding techniques such as **Label Encoding, One-Hot Encoding, Ordinal Encoding, Frequency Encoding, Target Encoding, and Binary Encoding** depending on the use case.\n",
    "* For tree-based models, label or frequency encoding often works fine since they handle splits naturally.\n",
    "* For linear models, I prefer one-hot or target encoding to capture categorical relationships without imposing artificial order.\"\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bf609",
   "metadata": {},
   "source": [
    "\n",
    "### **Q14. What is the difference between Type 1 and Type 2 error and their severity?**\n",
    "\n",
    "‚úÖ **Definition Context:**\n",
    "\n",
    "* In hypothesis testing, we make decisions based on sample data.\n",
    "* Errors occur when our decision does not match the true state of nature.\n",
    "* Two major types of errors: **Type I** and **Type II**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Type I Error (False Positive)**\n",
    "\n",
    "* **Definition:** Rejecting the null hypothesis (H‚ÇÄ) when it is actually true.\n",
    "* **Interpretation:** Concluding an effect exists when it does not.\n",
    "* **Probability:** Denoted by **Œ± (significance level)**, typically 0.05 or 5%.\n",
    "* **Example:**\n",
    "\n",
    "  * Medical Test ‚Üí Declaring a healthy person as \"diseased\".\n",
    "  * Fraud Detection ‚Üí Flagging a genuine transaction as fraud.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Type II Error (False Negative)**\n",
    "\n",
    "* **Definition:** Failing to reject the null hypothesis when it is actually false.\n",
    "* **Interpretation:** Missing an effect that truly exists.\n",
    "* **Probability:** Denoted by **Œ≤**. The power of a test is (1 ‚Äì Œ≤).\n",
    "* **Example:**\n",
    "\n",
    "  * Medical Test ‚Üí Declaring a diseased person as \"healthy\".\n",
    "  * Fraud Detection ‚Üí Missing an actual fraudulent transaction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Severity of Errors**\n",
    "\n",
    "* **Depends on Context / Domain:**\n",
    "\n",
    "  * **Medical Diagnosis:**\n",
    "\n",
    "    * Type I ‚Üí Unnecessary anxiety, cost of further tests.\n",
    "    * Type II ‚Üí Missing a life-threatening disease ‚Üí **More severe**.\n",
    "  * **Spam Detection:**\n",
    "\n",
    "    * Type I ‚Üí Genuine email goes to spam ‚Üí Annoying, but recoverable.\n",
    "    * Type II ‚Üí Spam email lands in inbox ‚Üí Possible phishing risk ‚Üí **More severe**.\n",
    "  * **Criminal Justice System (Hypothetical):**\n",
    "\n",
    "    * Type I ‚Üí Convicting an innocent person (false positive).\n",
    "    * Type II ‚Üí Letting a guilty person go free (false negative).\n",
    "    * Depending on philosophy of justice, usually Type I is considered **more severe**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Type I error is a **false positive** (rejecting a true null), while Type II error is a **false negative** (failing to reject a false null).\n",
    "* The severity is **context-dependent**: in medicine, Type II is often worse, while in legal or fraud detection systems, Type I can be more damaging. Hence, the acceptable trade-off between Type I and Type II error is guided by the problem domain and business priorities.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e14048",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Q16. What is the Mean, Median, Mode, and Standard Deviation for the sample and population?**\n",
    "\n",
    "‚úÖ **Context:**\n",
    "\n",
    "* These are **measures of central tendency** (Mean, Median, Mode) and **measure of spread** (Standard Deviation).\n",
    "* The calculation differs slightly when dealing with **sample vs population**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Mean (Average)**\n",
    "\n",
    "* **Definition:** Sum of all values divided by total number of observations.\n",
    "* **Formula (Population):**\n",
    "\n",
    "  $$\n",
    "  \\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n",
    "  $$\n",
    "* **Formula (Sample):**\n",
    "\n",
    "  $$\n",
    "  \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "  $$\n",
    "* **Example:** For values \\[2, 4, 6], mean = (2+4+6)/3 = 4.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Median (Middle Value)**\n",
    "\n",
    "* **Definition:** Middle value when data is ordered.\n",
    "* **Odd count:** Middle element.\n",
    "* **Even count:** Average of two middle elements.\n",
    "* **Same for population & sample.**\n",
    "* **Example:** For \\[1, 3, 5], median = 3; for \\[1, 3, 5, 7], median = (3+5)/2 = 4.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mode (Most Frequent Value)**\n",
    "\n",
    "* **Definition:** Value that occurs most frequently.\n",
    "* **Can be one (unimodal), more than one (multimodal), or none (no repetition).**\n",
    "* **Same for population & sample.**\n",
    "* **Example:** For \\[2, 2, 3, 4], mode = 2.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Standard Deviation (Measure of Spread)**\n",
    "\n",
    "* **Definition:** Average deviation of values from the mean.\n",
    "* **Population Formula:**\n",
    "\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\n",
    "  $$\n",
    "* **Sample Formula (uses n-1, Bessel‚Äôs correction):**\n",
    "\n",
    "  $$\n",
    "  s = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\n",
    "  $$\n",
    "* **Why different?**\n",
    "\n",
    "  * The denominator uses **n-1** for sample because sample mean is only an estimate of the true population mean.\n",
    "  * This adjustment removes bias in estimating the population variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example in Python:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = [2, 4, 6, 8, 10]\n",
    "\n",
    "mean = np.mean(data)                 # population mean\n",
    "median = np.median(data)             # median\n",
    "mode = max(set(data), key=data.count)# simple mode\n",
    "pop_std = np.std(data, ddof=0)       # population std\n",
    "sample_std = np.std(data, ddof=1)    # sample std\n",
    "\n",
    "print(mean, median, mode, pop_std, sample_std)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* **Mean, Median, and Mode** summarize the **central tendency** of data, while **Standard Deviation** measures its spread.\n",
    "* For **population**, we use formulas with $N$.\n",
    "* For **sample**, we use $n$ but adjust variance/standard deviation with **n‚Äì1 (Bessel‚Äôs correction)** to get an unbiased estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f4aaa",
   "metadata": {},
   "source": [
    "\n",
    "### **Q17. What is Mean Absolute Error (MAE)?**\n",
    "\n",
    "‚úÖ **Definition:**\n",
    "\n",
    "* **Mean Absolute Error (MAE)** is a regression evaluation metric that measures the **average magnitude of errors** between predicted values ($\\hat{y}$) and actual values ($y$), without considering their direction (positive or negative).\n",
    "* It simply takes the absolute difference between prediction and actual values, then averages it.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $y_i$ = actual value\n",
    "* $\\hat{y}_i$ = predicted value\n",
    "* $n$ = number of observations\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics:**\n",
    "\n",
    "1. **Non-Negative:** MAE ‚â• 0.\n",
    "2. **Interpretability:** Represents the average absolute deviation in the same units as the target variable.\n",
    "\n",
    "   * Example: If MAE = 3 (in days), predictions are off by **3 days on average**.\n",
    "3. **Robustness:** Less sensitive to outliers compared to Mean Squared Error (MSE).\n",
    "4. **Range:** 0 (perfect prediction) to ‚àû.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation:**\n",
    "\n",
    "Actual values = \\[3, 5, 7]\n",
    "Predicted values = \\[2, 5, 8]\n",
    "\n",
    "$$\n",
    "MAE = \\frac{|3-2| + |5-5| + |7-8|}{3} = \\frac{1 + 0 + 1}{3} = 0.67\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **In Python (sklearn):**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, 5, 7]\n",
    "y_pred = [2, 5, 8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\", mae)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use MAE:**\n",
    "\n",
    "* When interpretability is important (easy to explain to business stakeholders).\n",
    "* When you want to treat all errors equally (linear penalty).\n",
    "* When dataset may have outliers but you don‚Äôt want them to dominate error calculation (less aggressive than MSE).\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Other Metrics:**\n",
    "\n",
    "* **MAE vs MSE:**\n",
    "\n",
    "  * MAE uses absolute differences (linear penalty).\n",
    "  * MSE squares errors (quadratic penalty), so it‚Äôs more sensitive to outliers.\n",
    "* **MAE vs RMSE:**\n",
    "\n",
    "  * RMSE penalizes larger errors more, while MAE gives equal weight.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Mean Absolute Error is a simple and interpretable metric that gives the **average magnitude of prediction errors** in regression tasks. It is particularly useful when all errors should be treated equally and when we need results in the same scale as the target variable.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104c31e",
   "metadata": {},
   "source": [
    "### **Q19. What are the data normalization methods you have applied, and why?**\n",
    "\n",
    "‚úÖ **Context:**\n",
    "\n",
    "* **Normalization / Scaling** is the process of adjusting numerical features to a common scale without distorting differences in ranges.\n",
    "* It is critical because many ML algorithms (e.g., KNN, SVM, Gradient Descent-based models, PCA) are **distance or variance sensitive**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Min-Max Normalization (Rescaling to \\[0,1])**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  $$\n",
    "* **Why:** Ensures features are on the same 0‚Äì1 scale, useful for algorithms using **Euclidean distance** (e.g., KNN, Neural Networks).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Z-Score Standardization (Standard Scaling)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "* **Why:** Centers data at mean 0 with unit variance. Suitable for algorithms assuming **Gaussian distribution** (e.g., Logistic Regression, Linear Regression, PCA, SVM).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Robust Scaling (Based on Median & IQR)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - median}{IQR}\n",
    "  $$\n",
    "* **Why:** Used when dataset has **outliers**, since median & IQR are less affected by extreme values.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Log / Power Transformation**\n",
    "\n",
    "* **Why:** Handles **skewed distributions**, stabilizes variance, reduces impact of extreme values.\n",
    "* **Examples:**\n",
    "\n",
    "  * Log Transform: $x' = \\log(x+1)$\n",
    "  * Yeo-Johnson / Box-Cox Transform.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X_transformed = pt.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Unit Vector Normalization (L2 Norm)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x}{||x||}\n",
    "  $$\n",
    "* **Why:** Scales feature vectors to unit length, useful in **text mining / NLP** (e.g., TF-IDF vectors, cosine similarity).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"I have applied multiple normalization methods depending on the use case:\n",
    "\n",
    "  * **Min-Max Scaling** for distance-based models like KNN and Neural Networks.\n",
    "  * **Standard Scaling (Z-score)** for regression, PCA, and models sensitive to variance.\n",
    "  * **Robust Scaling** when handling datasets with outliers.\n",
    "  * **Log/Power Transformations** for skewed distributions.\n",
    "  * **L2 Normalization** for text/NLP problems.\n",
    "  * The choice always depends on the algorithm assumptions and data distribution.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec79c6",
   "metadata": {},
   "source": [
    "\n",
    "### **Q20. What is the difference between Normalization and Standardization?**\n",
    "\n",
    "‚úÖ **Context:**\n",
    "Both **Normalization** and **Standardization** are feature-scaling techniques used in data preprocessing, but they differ in **how they scale values and when they are applied**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Normalization (Min-Max Scaling)**\n",
    "\n",
    "* **Definition:** Rescales features to a fixed range, usually \\[0,1].\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  $$\n",
    "* **Effect:**\n",
    "\n",
    "  * All values fall between 0 and 1 (or -1 and 1, if chosen).\n",
    "  * Preserves the shape of the distribution, but compresses values into the chosen range.\n",
    "* **When to Use:**\n",
    "\n",
    "  * Distance-based algorithms like **KNN, Neural Networks, SVM**.\n",
    "  * When features have different ranges and you want them comparable.\n",
    "* **Example:**\n",
    "\n",
    "  * Age in range \\[18, 60] ‚Üí after normalization \\[0, 1].\n",
    "  * If age = 30 ‚Üí $(30-18)/(60-18) = 0.285$.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Standardization (Z-score Normalization)**\n",
    "\n",
    "* **Definition:** Transforms features to have mean = 0 and standard deviation = 1.\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "* **Effect:**\n",
    "\n",
    "  * Produces values centered around 0 with unit variance.\n",
    "  * Distribution is shifted and scaled, but not restricted to \\[0,1].\n",
    "* **When to Use:**\n",
    "\n",
    "  * Algorithms that assume **Gaussian distribution** (e.g., Linear Regression, Logistic Regression, PCA).\n",
    "  * Useful when outliers exist (less affected compared to Min-Max).\n",
    "* **Example:**\n",
    "\n",
    "  * Heights with mean = 170 cm, std = 10 cm.\n",
    "  * If person‚Äôs height = 180 cm ‚Üí $(180 - 170)/10 = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences (Side-by-Side):**\n",
    "\n",
    "| Aspect                 | Normalization (Min-Max)                 | Standardization (Z-score)                               |\n",
    "| ---------------------- | --------------------------------------- | ------------------------------------------------------- |\n",
    "| **Formula**            | $(x - x_{min}) / (x_{max} - x_{min})$   | $(x - \\mu) / \\sigma$                                    |\n",
    "| **Range**              | Scales to \\[0,1] (or \\[-1,1])           | Mean = 0, Std Dev = 1 (no fixed range)                  |\n",
    "| **Use Case**           | Distance-based models (KNN, NN, SVM)    | Models assuming Gaussian distribution (Regression, PCA) |\n",
    "| **Effect of Outliers** | Strongly affected (since min/max shift) | Less sensitive, but still influenced                    |\n",
    "| **Interpretability**   | Values between 0‚Äì1                      | Values as z-scores (relative to mean & std)             |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Normalization rescales features to a bounded range, typically \\[0,1], making it ideal for distance-based algorithms. Standardization, on the other hand, centers data at 0 with unit variance, making it better suited for algorithms assuming normal distribution. In practice, the choice depends on the algorithm and the distribution of the dataset.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2a56b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
