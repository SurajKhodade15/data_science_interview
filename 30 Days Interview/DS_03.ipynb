{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3f9c21",
   "metadata": {},
   "source": [
    "\n",
    "# 📌 Q1. How do you treat heteroscedasticity in regression?\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Definition**\n",
    "\n",
    "Heteroscedasticity means that the **variance of errors (residuals) is not constant** across all levels of the independent variable(s).\n",
    "This violates one of the key assumptions of **linear regression** (constant variance of errors = *homoscedasticity*).\n",
    "\n",
    "* When present, it can make:\n",
    "\n",
    "  * Coefficient estimates still **unbiased**,\n",
    "  * But **standard errors wrong** → leading to incorrect p-values & unreliable hypothesis tests.\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Causes**\n",
    "\n",
    "* Outliers or influential data points\n",
    "* Skewed distribution of variables\n",
    "* Wrong functional form (e.g., using linear when relationship is nonlinear)\n",
    "* Scale differences in predictors\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Detection**\n",
    "\n",
    "* **Residual vs Fitted plot** → Funnel shape indicates heteroscedasticity\n",
    "* **Breusch–Pagan test**\n",
    "* **White test**\n",
    "* **Goldfeld–Quandt test**\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Treatment Methods**\n",
    "\n",
    "1. **Transform the dependent variable**\n",
    "\n",
    "   * Apply transformations like:\n",
    "\n",
    "     * Log(y), √y, Box-Cox\n",
    "   * Helps stabilize variance.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   y_transformed = np.log(y)   # Example\n",
    "   ```\n",
    "\n",
    "2. **Weighted Least Squares (WLS)**\n",
    "\n",
    "   * Give smaller weights to data points with higher variance.\n",
    "   * Regression minimizes **weighted residuals**.\n",
    "\n",
    "   ```python\n",
    "   import statsmodels.api as sm\n",
    "   wls_model = sm.WLS(y, X, weights=1/(abs(residuals))).fit()\n",
    "   ```\n",
    "\n",
    "3. **Robust Standard Errors (Heteroscedasticity-consistent SE)**\n",
    "\n",
    "   * Use **HC standard errors** (e.g., White’s correction).\n",
    "   * Coefficients remain same, but SEs are adjusted.\n",
    "\n",
    "   ```python\n",
    "   ols_model = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "   ```\n",
    "\n",
    "4. **Model Redesign**\n",
    "\n",
    "   * Add missing variables\n",
    "   * Use nonlinear models (polynomial regression, tree-based methods, etc.)\n",
    "   * Feature scaling / transformation of predictors\n",
    "\n",
    "5. **Generalized Least Squares (GLS)**\n",
    "\n",
    "   * Explicitly models error structure.\n",
    "   * Useful if heteroscedasticity pattern is well understood.\n",
    "\n",
    "---\n",
    "\n",
    "## **🔹 Interview-Style Summary**\n",
    "\n",
    "👉 Heteroscedasticity means error variance is unequal.\n",
    "👉 It does not bias coefficients but makes hypothesis testing unreliable.\n",
    "👉 Detection: residual plots, BP test, White test.\n",
    "👉 Treatment: log/Box-Cox transforms, WLS, GLS, or robust standard errors.\n",
    "👉 In practice, **robust standard errors or log transformation** are the most common fixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd9e54",
   "metadata": {},
   "source": [
    "### ❓ Q2. What is Multicollinearity, and how do you treat it?\n",
    "\n",
    "**🔹 Definition:**\n",
    "Multicollinearity occurs when two or more independent (predictor) variables in a regression model are **highly correlated** with each other.\n",
    "\n",
    "* This makes it difficult for the model to determine the **unique effect** of each predictor on the target variable.\n",
    "* In extreme cases, it leads to unstable coefficients and inflated standard errors.\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 Example:**\n",
    "Suppose you are predicting `house_price` using `size_in_sqft` and `number_of_rooms`.\n",
    "\n",
    "* Since larger houses generally have more rooms, these two predictors may be highly correlated, causing multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 Problems caused by Multicollinearity:**\n",
    "\n",
    "1. Coefficients become **unstable** (small changes in data → large changes in β).\n",
    "2. Inflated **standard errors** → t-tests may wrongly show predictors as insignificant.\n",
    "3. Difficulty in interpreting predictor importance.\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 Detection Methods:**\n",
    "\n",
    "* **Correlation Matrix:** High correlation (≥ 0.8 or 0.9) between predictors.\n",
    "* **Variance Inflation Factor (VIF):**\n",
    "\n",
    "  * VIF > 5 (sometimes > 10) indicates high multicollinearity.\n",
    "\n",
    "```python\n",
    "# Example: Checking VIF in Python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[['size_in_sqft', 'num_rooms', 'num_bathrooms']]  # predictors\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 Treatment / Remedies:**\n",
    "\n",
    "1. **Remove one of the correlated variables** (e.g., drop `num_rooms` if highly correlated with `size_in_sqft`).\n",
    "2. **Combine variables** using **PCA (Principal Component Analysis)** or feature engineering.\n",
    "3. **Regularization (Ridge/Lasso Regression):**\n",
    "\n",
    "   * Ridge shrinks correlated coefficients toward each other.\n",
    "   * Lasso can drop redundant variables by assigning zero coefficients.\n",
    "4. **Collect more data** (sometimes multicollinearity is reduced with a larger dataset).\n",
    "5. **Centering variables (standardization/mean-centering):** Helps reduce correlation in polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Interview Tip:**\n",
    "\n",
    "> “Multicollinearity doesn’t reduce the predictive power of the model drastically, but it affects interpretability. So, whether to treat it depends on the objective – prediction vs. interpretation.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24123f5",
   "metadata": {},
   "source": [
    "## Q3. What is Market Basket Analysis? How would you do it in Python?\n",
    "\n",
    "### 📌 Theory & Intuition\n",
    "\n",
    "* **Market Basket Analysis (MBA)** is a technique used in retail and e-commerce to understand the **purchase behavior of customers**.\n",
    "* It identifies **associations or co-occurrence relationships** between items purchased together.\n",
    "* Example: If a customer buys *bread*, they are more likely to buy *butter*.\n",
    "\n",
    "### 🔹 Key Concepts\n",
    "\n",
    "* **Association Rule Mining**: Finds relationships between items.\n",
    "* **Support**: Probability of items appearing together in transactions.\n",
    "\n",
    "  $$\n",
    "  \\text{Support(A → B)} = \\frac{\\text{Transactions containing (A ∪ B)}}{\\text{Total transactions}}\n",
    "  $$\n",
    "* **Confidence**: Probability of buying B given A.\n",
    "\n",
    "  $$\n",
    "  \\text{Confidence(A → B)} = \\frac{\\text{Support(A ∪ B)}}{\\text{Support(A)}}\n",
    "  $$\n",
    "* **Lift**: Strength of association relative to independence.\n",
    "\n",
    "  $$\n",
    "  \\text{Lift(A → B)} = \\frac{\\text{Confidence(A → B)}}{\\text{Support(B)}}\n",
    "  $$\n",
    "\n",
    "  * Lift > 1 → Positive association\n",
    "  * Lift = 1 → Independent\n",
    "  * Lift < 1 → Negative association\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Python Example (Using `mlxtend`)\n",
    "\n",
    "```python\n",
    "# Install mlxtend if not available\n",
    "# !pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Sample dataset: Transactions\n",
    "dataset = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'diapers', 'beer'],\n",
    "    ['milk', 'bread', 'diapers', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'diapers', 'beer', 'cola']\n",
    "]\n",
    "\n",
    "# Convert dataset into one-hot encoded DataFrame\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_data = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_data, columns=te.columns_)\n",
    "\n",
    "print(\"One-Hot Encoded Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 1: Find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "print(\"\\nFrequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Step 2: Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents','consequents','support','confidence','lift']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Output Interpretation\n",
    "\n",
    "* The rules table will show:\n",
    "\n",
    "  * **Antecedents → Consequents** (e.g., `{milk} → {bread}`)\n",
    "  * **Support** (how often they occur together)\n",
    "  * **Confidence** (likelihood of consequent given antecedent)\n",
    "  * **Lift** (strength of the relationship)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Use Cases\n",
    "\n",
    "* **Retail**: Product bundling (e.g., “Buy chips, get soda”).\n",
    "* **E-commerce**: Recommender systems (“Customers also bought…”).\n",
    "* **Healthcare**: Drug prescription patterns.\n",
    "* **Finance**: Fraud detection (suspicious transaction combinations).\n",
    "\n",
    "---\n",
    "\n",
    "✅ In interviews, mention both **theory (support, confidence, lift)** and **implementation (Apriori in `mlxtend`)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa95be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "894ff5ad",
   "metadata": {},
   "source": [
    "\n",
    "## Q4. What is Association Analysis? Where is it used?\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Definition\n",
    "\n",
    "* Association Analysis is a **data mining technique** used to discover **relationships, patterns, or associations** between variables/items in large datasets.\n",
    "* It identifies **if-then rules** (called **association rules**) of the form:\n",
    "\n",
    "  ```\n",
    "  IF item A → THEN item B\n",
    "  ```\n",
    "\n",
    "  Example: \"If a customer buys bread, they are likely to buy butter.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Key Concepts\n",
    "\n",
    "* **Support** → Frequency of an itemset in the dataset.\n",
    "* **Confidence** → Likelihood that item B is bought when item A is bought.\n",
    "* **Lift** → Strength of association compared to random chance.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Where is it used?\n",
    "\n",
    "1. **Market Basket Analysis**\n",
    "\n",
    "   * Retail stores to identify products often bought together.\n",
    "   * Example: Amazon \"Frequently Bought Together\".\n",
    "\n",
    "2. **Recommender Systems**\n",
    "\n",
    "   * Suggesting items/movies based on association rules.\n",
    "\n",
    "3. **Cross-selling / Upselling**\n",
    "\n",
    "   * Banks recommending credit cards with savings accounts.\n",
    "\n",
    "4. **Healthcare**\n",
    "\n",
    "   * Finding correlations between symptoms and diseases.\n",
    "\n",
    "5. **Fraud Detection**\n",
    "\n",
    "   * Discover unusual item combinations in financial transactions.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Python Example (Using Apriori)\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'jam']\n",
    "]\n",
    "\n",
    "# Convert dataset to one-hot encoded DataFrame\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "data = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(data, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "\n",
    "# Generate rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "print(rules[['antecedents','consequents','support','confidence','lift']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Interview Tip\n",
    "\n",
    "👉 Always connect **Association Analysis** with **Market Basket Analysis**, as that’s the most common use case.\n",
    "👉 Highlight **Support, Confidence, Lift** since interviewers often test your grasp on these metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781c809",
   "metadata": {},
   "source": [
    "## ❓ Q5. What is KNN Classifier?\n",
    "\n",
    "### 🔹 Intuition\n",
    "\n",
    "The **K-Nearest Neighbors (KNN)** algorithm is a **supervised learning method** used for both **classification and regression**.\n",
    "It makes predictions based on the **majority class (for classification)** or **average values (for regression)** of the *k closest data points* in the feature space.\n",
    "\n",
    "It assumes:\n",
    "\n",
    "> \"Similar data points exist close to each other in the feature space.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 How It Works\n",
    "\n",
    "1. Choose a value of **k** (number of neighbors).\n",
    "2. Compute the **distance** (Euclidean, Manhattan, or Minkowski) between the test sample and all training samples.\n",
    "3. Select the **k nearest neighbors**.\n",
    "4. Perform:\n",
    "\n",
    "   * **Classification:** Assign the most frequent class among neighbors.\n",
    "   * **Regression:** Take the average of neighbors’ values.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Advantages\n",
    "\n",
    "* Simple and intuitive.\n",
    "* No training phase (lazy learning).\n",
    "* Works well for small datasets.\n",
    "\n",
    "### 🔹 Disadvantages\n",
    "\n",
    "* Computationally expensive for large datasets.\n",
    "* Sensitive to irrelevant/noisy features and scaling.\n",
    "* Performance depends on the choice of **k** and distance metric.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Applications\n",
    "\n",
    "* **Recommendation Systems** (e.g., suggesting movies based on similar users).\n",
    "* **Medical Diagnosis** (classifying diseases based on symptoms).\n",
    "* **Anomaly Detection** (detecting fraud transactions).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Python Example\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "KNN is a **distance-based, instance-learning algorithm** that classifies points based on the majority vote of nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bda428",
   "metadata": {},
   "source": [
    "### **Q6. What is Pipeline in sklearn?**\n",
    "\n",
    "**Answer:**\n",
    "In machine learning, particularly when using **scikit-learn**, a `Pipeline` is a high-level utility that helps streamline workflows by chaining multiple data preprocessing steps and model training steps into a single, cohesive object. The main purpose is to ensure that the exact sequence of transformations and modeling is executed consistently, reproducibly, and without data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Pipeline is important?**\n",
    "\n",
    "1. **Avoids Data Leakage:**\n",
    "   When we fit transformations (e.g., scaling, encoding, imputation) on the full dataset before splitting into train/test, the model unintentionally gains information from the test set. A Pipeline prevents this because transformations are fit only on the training data within cross-validation or train-test split.\n",
    "\n",
    "2. **Improves Reproducibility:**\n",
    "   Instead of manually remembering the sequence of steps (impute → scale → encode → model), the pipeline bundles them into one object. Running `.fit()` and `.predict()` executes the entire workflow consistently.\n",
    "\n",
    "3. **Simplifies Model Tuning:**\n",
    "   With scikit-learn’s `GridSearchCV` or `RandomizedSearchCV`, we can optimize hyperparameters not just for the model, but also for preprocessing steps inside the pipeline.\n",
    "\n",
    "4. **Production Readiness:**\n",
    "   A pipeline object can be easily saved (`joblib` or `pickle`) and deployed, ensuring the exact preprocessing logic used during training is applied during inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Pipeline is Important?\n",
    "\n",
    "Prevents Data Leakage – transformations are fitted only on training data.\n",
    "\n",
    "Ensures Reproducibility – the same sequence of steps is applied every time.\n",
    "\n",
    "Simplifies Hyperparameter Tuning – integrates with GridSearchCV/RandomizedSearchCV.\n",
    "\n",
    "Deployment Friendly – preprocessing + model bundled as one deployable object.\n",
    "\n",
    "Cleaner Code – avoids repetitive manual preprocessing steps.\n",
    "\n",
    "### How it Works:\n",
    "\n",
    "Pipeline is defined as a sequence of (name, transformer/estimator) steps.\n",
    "\n",
    "All steps except the last must be transformers (fit, transform).\n",
    "\n",
    "The last step is usually a model/estimator (fit, predict).\n",
    "### **How it works?**\n",
    "\n",
    "A pipeline is built as a list of `(name, transformer/model)` pairs. Every step except the last one must be a **transformer** (implementing `.fit` and `.transform`). The final step is typically an **estimator** (like Logistic Regression, Random Forest, etc.), which implements `.fit` and `.predict`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define pipeline steps\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),   # handle missing values\n",
    "    ('scaler', StandardScaler()),                  # feature scaling\n",
    "    ('model', LogisticRegression())                # final estimator\n",
    "])\n",
    "\n",
    "# Fit and predict using pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "* First, missing values are imputed.\n",
    "* Then, the data is scaled.\n",
    "* Finally, Logistic Regression is trained.\n",
    "\n",
    "When you call `pipeline.predict(X_test)`, it automatically applies the **same sequence** of steps to your test data before predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases in Real-World Projects:**\n",
    "\n",
    "* **Customer Churn Prediction:** Handle categorical encoding, imputation, and classification in one pipeline.\n",
    "* **Image or Text Classification:** Combine feature extraction (e.g., TF-IDF, PCA) and classification models.\n",
    "* **Model Deployment:** Ensure data received in production is preprocessed in exactly the same way as during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Statement (Good to Close in Interview):**\n",
    "\n",
    "\"In short, sklearn’s `Pipeline` is a powerful abstraction that enforces clean, reproducible, and leak-free workflows. It not only simplifies code maintenance but also makes hyperparameter tuning and deployment much more reliable in real-world machine learning projects.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d18b0",
   "metadata": {},
   "source": [
    "\n",
    "### **Q7. What is Principal Component Analysis (PCA), and why do we use it?**\n",
    "\n",
    "✅ **Definition:**\n",
    "\n",
    "* **Principal Component Analysis (PCA)** is a statistical technique used for **dimensionality reduction**.\n",
    "* It transforms the original correlated features into a new set of **uncorrelated variables** called **principal components**.\n",
    "* These components capture the **maximum variance** present in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics of PCA:**\n",
    "\n",
    "1. **Linear Transformation** – projects data into a new coordinate system.\n",
    "2. **Principal Components (PCs):**\n",
    "\n",
    "   * 1st PC → captures the maximum variance.\n",
    "   * 2nd PC → captures the next highest variance orthogonal to the first.\n",
    "   * And so on.\n",
    "3. **Orthogonality:** Components are uncorrelated and independent in direction.\n",
    "4. **Variance Retention:** Usually, only the top *k* components are selected to retain most of the information while reducing dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Do We Use PCA?**\n",
    "\n",
    "1. **Dimensionality Reduction** – reduce features while preserving most of the information.\n",
    "2. **Noise Reduction** – removes less informative (low variance) components.\n",
    "3. **Improves Model Efficiency** – fewer features → faster training and lower computation cost.\n",
    "4. **Avoids Multicollinearity** – transforms correlated features into independent components.\n",
    "5. **Visualization** – helps in visualizing high-dimensional data in 2D or 3D.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in PCA (How it Works):**\n",
    "\n",
    "1. **Standardize the Data** – scale features (important because PCA is variance-based).\n",
    "2. **Compute Covariance Matrix** – find relationships between features.\n",
    "3. **Eigen Decomposition / SVD** – calculate eigenvalues & eigenvectors of covariance matrix.\n",
    "4. **Select Top-k Components** – based on explained variance ratio.\n",
    "5. **Project Data** – transform original features into new principal component space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in sklearn:**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_pca.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Use Cases:**\n",
    "\n",
    "* **Image Compression:** Reduce pixel features while preserving visual quality.\n",
    "* **Genomics / Bioinformatics:** Handle thousands of gene features.\n",
    "* **Finance:** Analyze correlated stock movements.\n",
    "* **Text Data (NLP):** Reduce high-dimensional embeddings before classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"PCA is a powerful dimensionality reduction technique that simplifies high-dimensional datasets by transforming features into orthogonal components, capturing maximum variance, reducing redundancy, and improving both interpretability and efficiency of ML models.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da56b6",
   "metadata": {},
   "source": [
    "### **Q10. How to evaluate that data does not have any outliers?**\n",
    "\n",
    "✅ **Definition Context:**\n",
    "\n",
    "* Outliers are data points that deviate significantly from the general distribution.\n",
    "* Evaluating the presence (or absence) of outliers is crucial because they can **skew statistical analysis, bias model training, and reduce model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ways to Evaluate Outliers:**\n",
    "\n",
    "#### 1. **Statistical Methods**\n",
    "\n",
    "* **Z-Score / Standard Deviation Rule:**\n",
    "\n",
    "  * Data points with |Z| > 3 are potential outliers.\n",
    "  * Assumes data is approximately normal.\n",
    "* **IQR (Interquartile Range) Method:**\n",
    "\n",
    "  * Compute Q1 (25th percentile) and Q3 (75th percentile).\n",
    "  * Any point < Q1 – 1.5*IQR or > Q3 + 1.5*IQR is flagged as an outlier.\n",
    "* **Modified Z-score (Robust Method):**\n",
    "\n",
    "  * Based on Median Absolute Deviation (MAD), works better for skewed data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Visualization Methods**\n",
    "\n",
    "* **Boxplot:** Outliers appear as individual points beyond whiskers.\n",
    "* **Histogram / Density Plot:** Extreme values can be visually detected.\n",
    "* **Scatter Plot (2D/3D):** Detects outliers in multi-feature relationships.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Model-Based / Multivariate Methods**\n",
    "\n",
    "* **Isolation Forest:** Detects anomalies by random partitioning.\n",
    "* **DBSCAN Clustering:** Points not belonging to any cluster may be outliers.\n",
    "* **Mahalanobis Distance:** Considers correlation between variables; points with high distance are outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Conclude “No Outliers”?**\n",
    "\n",
    "1. **Statistical Check:** No points exceed thresholds (e.g., |Z| < 3, within IQR bounds).\n",
    "2. **Visual Check:** Plots show no extreme deviations.\n",
    "3. **Model-Based Check:** Anomaly detection algorithms mark very few or zero points as outliers.\n",
    "4. **Domain Validation:** Confirm with subject-matter experts—sometimes extreme values are legitimate (e.g., very high income in finance).\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example in Python:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example using IQR\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['feature'] < (Q1 - 1.5 * IQR)) | (df['feature'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "if outliers.empty:\n",
    "    print(\"No outliers detected.\")\n",
    "else:\n",
    "    print(f\"{len(outliers)} outliers detected.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"To evaluate if data has no outliers, we use a mix of **statistical thresholds, visualization techniques, and anomaly detection algorithms**. If none of these methods flag unusual points, and domain experts confirm the ranges, we can reasonably conclude that the dataset is free from outliers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec5589",
   "metadata": {},
   "source": [
    "### **Q11. What do you do if there are outliers?**\n",
    "\n",
    "✅ **Context:**\n",
    "\n",
    "* Outliers are data points that deviate significantly from the rest of the dataset.\n",
    "* They may be due to **measurement errors, data entry errors, rare events, or genuine extreme cases**.\n",
    "* The approach depends on the **context, dataset size, model sensitivity, and business requirements**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Identify the Nature of Outliers**\n",
    "\n",
    "1. **Error vs Legitimate Value**\n",
    "\n",
    "   * If it’s a data entry or sensor error → correct or remove it.\n",
    "   * If it’s a valid rare event (e.g., fraud detection, high-value customer) → keep it.\n",
    "2. **Univariate vs Multivariate**\n",
    "\n",
    "   * Check if outliers are extreme only in one variable or in combination of features.\n",
    "3. **Domain Knowledge Validation**\n",
    "\n",
    "   * Validate with subject-matter experts before removing rare but important cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Possible Strategies to Handle Outliers**\n",
    "\n",
    "1. **Remove Outliers (Deletion)**\n",
    "\n",
    "   * Use only if dataset is large and outliers are confirmed to be erroneous.\n",
    "   * Example: Removing sensor glitches or invalid negative ages.\n",
    "\n",
    "2. **Transformation / Scaling**\n",
    "\n",
    "   * Apply **log, square root, Box-Cox, or Yeo-Johnson** transformations to reduce impact.\n",
    "   * Robust scaling (median & IQR-based) instead of standard scaling.\n",
    "\n",
    "3. **Cap or Winsorize Values**\n",
    "\n",
    "   * Replace extreme values with upper/lower thresholds (e.g., 1st and 99th percentile).\n",
    "   * Common in financial datasets with long-tailed distributions.\n",
    "\n",
    "4. **Imputation**\n",
    "\n",
    "   * Replace extreme values with mean, median, or domain-specific values.\n",
    "   * Example: Cap extreme house prices at market-acceptable limits.\n",
    "\n",
    "5. **Model-Based Treatment**\n",
    "\n",
    "   * Use models robust to outliers (e.g., tree-based models, Random Forest, XGBoost).\n",
    "   * Apply anomaly detection algorithms (Isolation Forest, DBSCAN, LOF) to handle them separately.\n",
    "\n",
    "6. **Separate Treatment for Rare Events**\n",
    "\n",
    "   * If outliers represent rare but important cases (fraud, rare disease), create a separate class or special handling mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Example in Python**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Capping outliers using percentiles\n",
    "lower, upper = np.percentile(df['feature'], [1, 99])\n",
    "df['feature'] = np.clip(df['feature'], lower, upper)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"When dealing with outliers, the key is not to blindly remove them but to **analyze their cause, validate with domain knowledge, and choose a handling strategy**. Options include removal, transformation, capping, or using robust models. Importantly, if outliers represent rare but meaningful events, they should be retained and treated as critical information rather than discarded.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f525e1",
   "metadata": {},
   "source": [
    "\n",
    "### **Q12. What are the encoding techniques you have applied? Give examples.**\n",
    "\n",
    "✅ **Context:**\n",
    "\n",
    "* Many ML algorithms work only with **numerical data**, so categorical features must be encoded.\n",
    "* The choice of encoding depends on the **type of categorical variable (nominal vs ordinal)**, **model type**, and **data size**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Label Encoding**\n",
    "\n",
    "* **Definition:** Assigns a unique integer to each category.\n",
    "* **Use Case:** Ordinal categorical variables (e.g., education level).\n",
    "* **Limitation:** Implies order even if not meaningful (problem for nominal data with linear models).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['education_level'] = le.fit_transform(df['education_level'])\n",
    "# ['High School','Bachelors','Masters'] → [0,1,2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. One-Hot Encoding**\n",
    "\n",
    "* **Definition:** Creates binary (0/1) columns for each category.\n",
    "* **Use Case:** Nominal variables without natural order.\n",
    "* **Limitation:** Can lead to **high dimensionality** if many categories.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.get_dummies(df, columns=['gender'], drop_first=True)\n",
    "# 'Male','Female' → one column ['gender_Male'] (1 if Male else 0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ordinal Encoding**\n",
    "\n",
    "* **Definition:** Assigns integers based on order/rank.\n",
    "* **Use Case:** Ordinal variables with a defined hierarchy (e.g., low < medium < high).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder(categories=[['Low','Medium','High']])\n",
    "df['priority'] = oe.fit_transform(df[['priority']])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Frequency / Count Encoding**\n",
    "\n",
    "* **Definition:** Replace categories with their frequency or count in the dataset.\n",
    "* **Use Case:** High-cardinality categorical variables.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "df['city_freq'] = df['city'].map(df['city'].value_counts())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Target Encoding (Mean Encoding)**\n",
    "\n",
    "* **Definition:** Replace category with the mean of the target variable for that category.\n",
    "* **Use Case:** Useful in classification problems with categorical features.\n",
    "* **Caution:** Must apply with cross-validation to avoid leakage.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "df['category_encoded'] = df.groupby('category')['target'].transform('mean')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Binary Encoding**\n",
    "\n",
    "* **Definition:** Converts categories into binary code, reduces dimensionality compared to one-hot.\n",
    "* **Use Case:** High-cardinality variables.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "!pip install category_encoders\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "be = BinaryEncoder(cols=['city'])\n",
    "df = be.fit_transform(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Hashing Encoding**\n",
    "\n",
    "* **Definition:** Maps categories into fixed-size integers using a hash function.\n",
    "* **Use Case:** Very high-cardinality categorical variables (e.g., user IDs).\n",
    "* **Limitation:** Possible collisions (different categories mapped to same value).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"In practice, I’ve applied multiple encoding techniques such as **Label Encoding, One-Hot Encoding, Ordinal Encoding, Frequency Encoding, Target Encoding, and Binary Encoding** depending on the use case.\n",
    "* For tree-based models, label or frequency encoding often works fine since they handle splits naturally.\n",
    "* For linear models, I prefer one-hot or target encoding to capture categorical relationships without imposing artificial order.\"\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bf609",
   "metadata": {},
   "source": [
    "\n",
    "### **Q14. What is the difference between Type 1 and Type 2 error and their severity?**\n",
    "\n",
    "✅ **Definition Context:**\n",
    "\n",
    "* In hypothesis testing, we make decisions based on sample data.\n",
    "* Errors occur when our decision does not match the true state of nature.\n",
    "* Two major types of errors: **Type I** and **Type II**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Type I Error (False Positive)**\n",
    "\n",
    "* **Definition:** Rejecting the null hypothesis (H₀) when it is actually true.\n",
    "* **Interpretation:** Concluding an effect exists when it does not.\n",
    "* **Probability:** Denoted by **α (significance level)**, typically 0.05 or 5%.\n",
    "* **Example:**\n",
    "\n",
    "  * Medical Test → Declaring a healthy person as \"diseased\".\n",
    "  * Fraud Detection → Flagging a genuine transaction as fraud.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Type II Error (False Negative)**\n",
    "\n",
    "* **Definition:** Failing to reject the null hypothesis when it is actually false.\n",
    "* **Interpretation:** Missing an effect that truly exists.\n",
    "* **Probability:** Denoted by **β**. The power of a test is (1 – β).\n",
    "* **Example:**\n",
    "\n",
    "  * Medical Test → Declaring a diseased person as \"healthy\".\n",
    "  * Fraud Detection → Missing an actual fraudulent transaction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Severity of Errors**\n",
    "\n",
    "* **Depends on Context / Domain:**\n",
    "\n",
    "  * **Medical Diagnosis:**\n",
    "\n",
    "    * Type I → Unnecessary anxiety, cost of further tests.\n",
    "    * Type II → Missing a life-threatening disease → **More severe**.\n",
    "  * **Spam Detection:**\n",
    "\n",
    "    * Type I → Genuine email goes to spam → Annoying, but recoverable.\n",
    "    * Type II → Spam email lands in inbox → Possible phishing risk → **More severe**.\n",
    "  * **Criminal Justice System (Hypothetical):**\n",
    "\n",
    "    * Type I → Convicting an innocent person (false positive).\n",
    "    * Type II → Letting a guilty person go free (false negative).\n",
    "    * Depending on philosophy of justice, usually Type I is considered **more severe**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Type I error is a **false positive** (rejecting a true null), while Type II error is a **false negative** (failing to reject a false null).\n",
    "* The severity is **context-dependent**: in medicine, Type II is often worse, while in legal or fraud detection systems, Type I can be more damaging. Hence, the acceptable trade-off between Type I and Type II error is guided by the problem domain and business priorities.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e14048",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Q16. What is the Mean, Median, Mode, and Standard Deviation for the sample and population?**\n",
    "\n",
    "✅ **Context:**\n",
    "\n",
    "* These are **measures of central tendency** (Mean, Median, Mode) and **measure of spread** (Standard Deviation).\n",
    "* The calculation differs slightly when dealing with **sample vs population**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Mean (Average)**\n",
    "\n",
    "* **Definition:** Sum of all values divided by total number of observations.\n",
    "* **Formula (Population):**\n",
    "\n",
    "  $$\n",
    "  \\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n",
    "  $$\n",
    "* **Formula (Sample):**\n",
    "\n",
    "  $$\n",
    "  \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "  $$\n",
    "* **Example:** For values \\[2, 4, 6], mean = (2+4+6)/3 = 4.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Median (Middle Value)**\n",
    "\n",
    "* **Definition:** Middle value when data is ordered.\n",
    "* **Odd count:** Middle element.\n",
    "* **Even count:** Average of two middle elements.\n",
    "* **Same for population & sample.**\n",
    "* **Example:** For \\[1, 3, 5], median = 3; for \\[1, 3, 5, 7], median = (3+5)/2 = 4.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mode (Most Frequent Value)**\n",
    "\n",
    "* **Definition:** Value that occurs most frequently.\n",
    "* **Can be one (unimodal), more than one (multimodal), or none (no repetition).**\n",
    "* **Same for population & sample.**\n",
    "* **Example:** For \\[2, 2, 3, 4], mode = 2.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Standard Deviation (Measure of Spread)**\n",
    "\n",
    "* **Definition:** Average deviation of values from the mean.\n",
    "* **Population Formula:**\n",
    "\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\n",
    "  $$\n",
    "* **Sample Formula (uses n-1, Bessel’s correction):**\n",
    "\n",
    "  $$\n",
    "  s = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\n",
    "  $$\n",
    "* **Why different?**\n",
    "\n",
    "  * The denominator uses **n-1** for sample because sample mean is only an estimate of the true population mean.\n",
    "  * This adjustment removes bias in estimating the population variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example in Python:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = [2, 4, 6, 8, 10]\n",
    "\n",
    "mean = np.mean(data)                 # population mean\n",
    "median = np.median(data)             # median\n",
    "mode = max(set(data), key=data.count)# simple mode\n",
    "pop_std = np.std(data, ddof=0)       # population std\n",
    "sample_std = np.std(data, ddof=1)    # sample std\n",
    "\n",
    "print(mean, median, mode, pop_std, sample_std)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* **Mean, Median, and Mode** summarize the **central tendency** of data, while **Standard Deviation** measures its spread.\n",
    "* For **population**, we use formulas with $N$.\n",
    "* For **sample**, we use $n$ but adjust variance/standard deviation with **n–1 (Bessel’s correction)** to get an unbiased estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f4aaa",
   "metadata": {},
   "source": [
    "\n",
    "### **Q17. What is Mean Absolute Error (MAE)?**\n",
    "\n",
    "✅ **Definition:**\n",
    "\n",
    "* **Mean Absolute Error (MAE)** is a regression evaluation metric that measures the **average magnitude of errors** between predicted values ($\\hat{y}$) and actual values ($y$), without considering their direction (positive or negative).\n",
    "* It simply takes the absolute difference between prediction and actual values, then averages it.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula:**\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $y_i$ = actual value\n",
    "* $\\hat{y}_i$ = predicted value\n",
    "* $n$ = number of observations\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics:**\n",
    "\n",
    "1. **Non-Negative:** MAE ≥ 0.\n",
    "2. **Interpretability:** Represents the average absolute deviation in the same units as the target variable.\n",
    "\n",
    "   * Example: If MAE = 3 (in days), predictions are off by **3 days on average**.\n",
    "3. **Robustness:** Less sensitive to outliers compared to Mean Squared Error (MSE).\n",
    "4. **Range:** 0 (perfect prediction) to ∞.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation:**\n",
    "\n",
    "Actual values = \\[3, 5, 7]\n",
    "Predicted values = \\[2, 5, 8]\n",
    "\n",
    "$$\n",
    "MAE = \\frac{|3-2| + |5-5| + |7-8|}{3} = \\frac{1 + 0 + 1}{3} = 0.67\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **In Python (sklearn):**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, 5, 7]\n",
    "y_pred = [2, 5, 8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\", mae)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use MAE:**\n",
    "\n",
    "* When interpretability is important (easy to explain to business stakeholders).\n",
    "* When you want to treat all errors equally (linear penalty).\n",
    "* When dataset may have outliers but you don’t want them to dominate error calculation (less aggressive than MSE).\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Other Metrics:**\n",
    "\n",
    "* **MAE vs MSE:**\n",
    "\n",
    "  * MAE uses absolute differences (linear penalty).\n",
    "  * MSE squares errors (quadratic penalty), so it’s more sensitive to outliers.\n",
    "* **MAE vs RMSE:**\n",
    "\n",
    "  * RMSE penalizes larger errors more, while MAE gives equal weight.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Mean Absolute Error is a simple and interpretable metric that gives the **average magnitude of prediction errors** in regression tasks. It is particularly useful when all errors should be treated equally and when we need results in the same scale as the target variable.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104c31e",
   "metadata": {},
   "source": [
    "### **Q19. What are the data normalization methods you have applied, and why?**\n",
    "\n",
    "✅ **Context:**\n",
    "\n",
    "* **Normalization / Scaling** is the process of adjusting numerical features to a common scale without distorting differences in ranges.\n",
    "* It is critical because many ML algorithms (e.g., KNN, SVM, Gradient Descent-based models, PCA) are **distance or variance sensitive**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Min-Max Normalization (Rescaling to \\[0,1])**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  $$\n",
    "* **Why:** Ensures features are on the same 0–1 scale, useful for algorithms using **Euclidean distance** (e.g., KNN, Neural Networks).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Z-Score Standardization (Standard Scaling)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "* **Why:** Centers data at mean 0 with unit variance. Suitable for algorithms assuming **Gaussian distribution** (e.g., Logistic Regression, Linear Regression, PCA, SVM).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Robust Scaling (Based on Median & IQR)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - median}{IQR}\n",
    "  $$\n",
    "* **Why:** Used when dataset has **outliers**, since median & IQR are less affected by extreme values.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Log / Power Transformation**\n",
    "\n",
    "* **Why:** Handles **skewed distributions**, stabilizes variance, reduces impact of extreme values.\n",
    "* **Examples:**\n",
    "\n",
    "  * Log Transform: $x' = \\log(x+1)$\n",
    "  * Yeo-Johnson / Box-Cox Transform.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X_transformed = pt.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Unit Vector Normalization (L2 Norm)**\n",
    "\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x}{||x||}\n",
    "  $$\n",
    "* **Why:** Scales feature vectors to unit length, useful in **text mining / NLP** (e.g., TF-IDF vectors, cosine similarity).\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"I have applied multiple normalization methods depending on the use case:\n",
    "\n",
    "  * **Min-Max Scaling** for distance-based models like KNN and Neural Networks.\n",
    "  * **Standard Scaling (Z-score)** for regression, PCA, and models sensitive to variance.\n",
    "  * **Robust Scaling** when handling datasets with outliers.\n",
    "  * **Log/Power Transformations** for skewed distributions.\n",
    "  * **L2 Normalization** for text/NLP problems.\n",
    "  * The choice always depends on the algorithm assumptions and data distribution.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec79c6",
   "metadata": {},
   "source": [
    "\n",
    "### **Q20. What is the difference between Normalization and Standardization?**\n",
    "\n",
    "✅ **Context:**\n",
    "Both **Normalization** and **Standardization** are feature-scaling techniques used in data preprocessing, but they differ in **how they scale values and when they are applied**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Normalization (Min-Max Scaling)**\n",
    "\n",
    "* **Definition:** Rescales features to a fixed range, usually \\[0,1].\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "  $$\n",
    "* **Effect:**\n",
    "\n",
    "  * All values fall between 0 and 1 (or -1 and 1, if chosen).\n",
    "  * Preserves the shape of the distribution, but compresses values into the chosen range.\n",
    "* **When to Use:**\n",
    "\n",
    "  * Distance-based algorithms like **KNN, Neural Networks, SVM**.\n",
    "  * When features have different ranges and you want them comparable.\n",
    "* **Example:**\n",
    "\n",
    "  * Age in range \\[18, 60] → after normalization \\[0, 1].\n",
    "  * If age = 30 → $(30-18)/(60-18) = 0.285$.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Standardization (Z-score Normalization)**\n",
    "\n",
    "* **Definition:** Transforms features to have mean = 0 and standard deviation = 1.\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $$\n",
    "* **Effect:**\n",
    "\n",
    "  * Produces values centered around 0 with unit variance.\n",
    "  * Distribution is shifted and scaled, but not restricted to \\[0,1].\n",
    "* **When to Use:**\n",
    "\n",
    "  * Algorithms that assume **Gaussian distribution** (e.g., Linear Regression, Logistic Regression, PCA).\n",
    "  * Useful when outliers exist (less affected compared to Min-Max).\n",
    "* **Example:**\n",
    "\n",
    "  * Heights with mean = 170 cm, std = 10 cm.\n",
    "  * If person’s height = 180 cm → $(180 - 170)/10 = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences (Side-by-Side):**\n",
    "\n",
    "| Aspect                 | Normalization (Min-Max)                 | Standardization (Z-score)                               |\n",
    "| ---------------------- | --------------------------------------- | ------------------------------------------------------- |\n",
    "| **Formula**            | $(x - x_{min}) / (x_{max} - x_{min})$   | $(x - \\mu) / \\sigma$                                    |\n",
    "| **Range**              | Scales to \\[0,1] (or \\[-1,1])           | Mean = 0, Std Dev = 1 (no fixed range)                  |\n",
    "| **Use Case**           | Distance-based models (KNN, NN, SVM)    | Models assuming Gaussian distribution (Regression, PCA) |\n",
    "| **Effect of Outliers** | Strongly affected (since min/max shift) | Less sensitive, but still influenced                    |\n",
    "| **Interpretability**   | Values between 0–1                      | Values as z-scores (relative to mean & std)             |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary (Closing Statement):**\n",
    "\n",
    "* \"Normalization rescales features to a bounded range, typically \\[0,1], making it ideal for distance-based algorithms. Standardization, on the other hand, centers data at 0 with unit variance, making it better suited for algorithms assuming normal distribution. In practice, the choice depends on the algorithm and the distribution of the dataset.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2a56b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
