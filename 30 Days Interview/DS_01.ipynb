{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb89cacf",
   "metadata": {},
   "source": [
    "# Day 1 ‚Äì Machine Learning Interview Q&A (with extra context)\n",
    "\n",
    "---\n",
    "\n",
    "### Q1. What is the difference between AI, Data Science, ML, and DL?\n",
    "\n",
    "**Answer:**\n",
    "- **Artificial Intelligence (AI):**\n",
    "  - Big umbrella concept ‚Üí Machines mimicking human intelligence (reasoning, decision making, natural language understanding).\n",
    "  - Two types: **General AI** (human-like, not yet achieved) and **Applied AI** (specific tasks like self-driving cars).\n",
    "  - Example: Siri or Alexa (speech + decision making).\n",
    "\n",
    "- **Machine Learning (ML):**\n",
    "  - Subset of AI ‚Üí Focused on teaching machines to **learn from data** instead of explicit programming.\n",
    "  - Example: Netflix recommending movies by learning your past choices.\n",
    "\n",
    "- **Deep Learning (DL):**\n",
    "  - Subset of ML ‚Üí Uses **neural networks with many layers**.\n",
    "  - Good for unstructured data like **images, audio, video, text**.\n",
    "  - Example: Face recognition on Facebook.\n",
    "\n",
    "- **Data Science:**\n",
    "  - Broader field involving **data collection, cleaning, visualization, statistics, ML, deployment**.\n",
    "  - Not only models, but also storytelling with data, dashboards, decision systems.\n",
    "  - Example: A data scientist analyzing customer behavior using SQL, Python, ML models, and then making a business recommendation.\n",
    "\n",
    "üëâ **Hierarchy to remember:**  \n",
    "AI ‚Üí ML ‚Üí DL  \n",
    "Data Science overlaps with all because it deals with **end-to-end handling of data and insights**.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. What is the difference between Supervised, Unsupervised, and Reinforcement Learning?\n",
    "\n",
    "**Answer:**\n",
    "- **Supervised Learning:**\n",
    "  - Trained on **labeled data** (input ‚Üí known output).\n",
    "  - Model learns mapping between X ‚Üí Y.\n",
    "  - Examples:\n",
    "    - Regression: Predicting house prices.\n",
    "    - Classification: Spam vs. not spam.\n",
    "\n",
    "- **Unsupervised Learning:**\n",
    "  - Works on **unlabeled data** (only inputs, no outputs).\n",
    "  - Goal = discover patterns, groups, or structure.\n",
    "  - Examples:\n",
    "    - Clustering: Grouping customers by spending habits.\n",
    "    - Anomaly detection: Fraud detection.\n",
    "\n",
    "- **Reinforcement Learning:**\n",
    "  - Agent learns by **trial and error** in an environment.\n",
    "  - Receives **rewards or penalties** for actions.\n",
    "  - Goal = maximize long-term reward.\n",
    "  - Examples:\n",
    "    - AlphaGo beating humans in Go.\n",
    "    - Robot learning to walk.\n",
    "\n",
    "üëâ Mnemonic:\n",
    "- Supervised ‚Üí Teacher with answers.  \n",
    "- Unsupervised ‚Üí No teacher, just structure.  \n",
    "- Reinforcement ‚Üí Learn by trial & error with feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. Describe the general architecture of Machine Learning.\n",
    "\n",
    "**Answer:**\n",
    "Machine learning is not just about training models. It‚Äôs an **end-to-end pipeline**:\n",
    "\n",
    "1. **Business Understanding:**\n",
    "   - Define the problem clearly (e.g., ‚Äúpredict churn‚Äù).\n",
    "   - Know the domain (finance, healthcare, retail).\n",
    "\n",
    "2. **Data Collection & Understanding:**\n",
    "   - Gather data from databases, APIs, logs.\n",
    "   - Perform EDA (exploratory data analysis) ‚Üí find missing values, distributions.\n",
    "\n",
    "3. **Feature Engineering & Selection:**\n",
    "   - Scale/normalize numerical features.\n",
    "   - Encode categorical variables.\n",
    "   - Drop irrelevant features using correlation, PCA, or domain knowledge.\n",
    "\n",
    "4. **Model Training:**\n",
    "   - Choose algorithms (linear regression, decision trees, neural nets).\n",
    "   - Train on training data.\n",
    "\n",
    "5. **Model Evaluation:**\n",
    "   - Metrics: accuracy, precision, recall (classification); RMSE, MAE (regression).\n",
    "   - Use cross-validation to ensure robustness.\n",
    "\n",
    "6. **Hyperparameter Tuning:**\n",
    "   - Improve model via GridSearch, RandomSearch, Bayesian optimization.\n",
    "\n",
    "7. **Deployment:**\n",
    "   - Serve the model (API, cloud, mobile).\n",
    "   - Example: recommendation engine in production.\n",
    "\n",
    "8. **Monitoring:**\n",
    "   - Track drift ‚Üí if new data distribution changes, retrain model.\n",
    "\n",
    "üëâ **Lifecycle mnemonic:** Define ‚Üí Collect ‚Üí Prepare ‚Üí Train ‚Üí Evaluate ‚Üí Deploy ‚Üí Monitor\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. What is Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "- Linear regression is a supervised ML algorithm to predict **continuous values**.\n",
    "- It assumes a **linear relationship** between inputs and outputs.\n",
    "- Equation:\n",
    "  \\[\n",
    "  Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "  \\]\n",
    "- **Simple Linear Regression:** one input feature.  \n",
    "- **Multiple Linear Regression:** multiple input features.\n",
    "\n",
    "**Example:**\n",
    "Predicting salary (Y) from years of experience (X).\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1],[2],[3],[4]])   # years\n",
    "y = np.array([30, 35, 40, 45])    # salary in k\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"Prediction for 5 years exp:\", model.predict([[5]])[0])\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. What is Ordinary Least Squares (OLS)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* OLS is the method used to find the best-fit line in linear regression.\n",
    "* It minimizes the **sum of squared residuals (errors)**:\n",
    "\n",
    "  $$\n",
    "  \\text{Error} = \\sum (y_i - \\hat{y_i})^2\n",
    "  $$\n",
    "* OLS gives estimates for coefficients Œ≤ that minimize this error.\n",
    "* In Python (using statsmodels), OLS also gives:\n",
    "\n",
    "  * **t-values, p-values** ‚Üí to check which features are statistically significant.\n",
    "  * Helps in **feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. What is L1 Regularization (Lasso)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Lasso adds **absolute values of coefficients** as penalty to loss function.\n",
    "* Effect:\n",
    "\n",
    "  * Shrinks less important feature coefficients to **0**.\n",
    "  * Does **feature selection** automatically.\n",
    "* Useful when dataset has many irrelevant features.\n",
    "* Good for **sparse models**.\n",
    "\n",
    "üëâ Think of Lasso as a model that ‚Äúkeeps only important features and ignores the rest.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. What is L2 Regularization (Ridge Regression)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Ridge adds **squared values of coefficients** as penalty.\n",
    "* Effect:\n",
    "\n",
    "  * Shrinks coefficients closer to zero but **never exactly zero**.\n",
    "  * Keeps all features but reduces their influence.\n",
    "* Good when all features are useful but we want to avoid overfitting.\n",
    "* Not robust to outliers (because large errors are squared).\n",
    "\n",
    "üëâ Think of Ridge as a model that ‚Äúuses all features but controls their influence.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### Q8. What is R-squared? Where to use and where not?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* R¬≤ measures how much variance in target is explained by model.\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "  $$\n",
    "* Range: 0 ‚Üí 1\n",
    "\n",
    "  * 0 ‚Üí model explains nothing.\n",
    "  * 1 ‚Üí model explains everything.\n",
    "* **Limitation:** Always increases with more features, even if useless.\n",
    "* **Adjusted R¬≤:** Penalizes addition of irrelevant features ‚Üí better for comparing models.\n",
    "\n",
    "---\n",
    "\n",
    "### Q9. What is Mean Squared Error (MSE)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* MSE = average of squared differences between actual and predicted.\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  MSE = \\frac{1}{n}\\sum (y_i - \\hat{y_i})^2\n",
    "  $$\n",
    "* Properties:\n",
    "\n",
    "  * Penalizes large errors heavily.\n",
    "  * Sensitive to outliers.\n",
    "* RMSE = ‚àöMSE ‚Üí interpretable in same units as target.\n",
    "\n",
    "**Example:**\n",
    "If true values = \\[2, 4], predicted = \\[3, 5],\n",
    "Errors = \\[‚àí1, ‚àí1] ‚Üí Squared = \\[1, 1] ‚Üí MSE = 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Q10. Why Support Vector Regression (SVR)? Difference from Linear Regression?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Linear Regression:** Fits a straight line by minimizing overall error.\n",
    "* **SVR:** Tries to fit a line (or hyperplane) such that most points lie within a margin of tolerance (epsilon). Only points outside margin (support vectors) influence the model.\n",
    "* Concepts:\n",
    "\n",
    "  * **Hyperplane:** Best fit line.\n",
    "  * **Epsilon (Œµ):** Error margin where small deviations are ignored.\n",
    "  * **Support Vectors:** Critical points that define boundary.\n",
    "  * **Kernel:** Enables SVR to handle non-linear relationships.\n",
    "\n",
    "üëâ Use SVR when:\n",
    "\n",
    "* You want to allow small tolerance in errors.\n",
    "* Data is non-linear (use kernels).\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77104442",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
