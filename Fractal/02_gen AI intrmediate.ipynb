{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bccf7d",
   "metadata": {},
   "source": [
    "\n",
    "# **Intermediate-Level GenAI Interview Q&A**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. How does an LLM actually generate the next token?**\n",
    "\n",
    "**Answer:**\n",
    "The model computes the probability distribution of all possible next tokens using its learned parameters, contextual embeddings, and attention outputs. It then selects a token based on the decoding strategy—greedy, sampling, top-k, top-p, or beam search. The choice of strategy directly influences creativity, determinism, and output quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is the role of positional embeddings in transformers?**\n",
    "\n",
    "**Answer:**\n",
    "Since transformers process tokens in parallel, they lack inherent sequence awareness. Positional embeddings inject order information into the architecture, enabling the model to understand token position, sentence flow, and long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Can you explain LoRA and why it is widely used?**\n",
    "\n",
    "**Answer:**\n",
    "LoRA (Low-Rank Adaptation) freezes the base model and injects small trainable matrices into attention layers to learn task-specific adaptations. This drastically reduces training cost, GPU footprint, and risk of catastrophic forgetting, making it ideal for enterprise fine-tuning scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is QLoRA? How is it different from LoRA?**\n",
    "\n",
    "**Answer:**\n",
    "QLoRA applies 4-bit quantization to the base model weights and conducts fine-tuning on top using LoRA adapters. This creates a memory-efficient pipeline capable of fine-tuning large models on a single GPU without significant performance degradation.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. How does a vector database improve a RAG workflow?**\n",
    "\n",
    "**Answer:**\n",
    "A vector database enables high-performance similarity search using embeddings. It supports indexing, filtering, and real-time retrieval at scale, ensuring low-latency context delivery to the LLM. This architecture enhances grounding, ensures more relevant retrieval, and reduces hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What is chunking? Why is it critical in RAG?**\n",
    "\n",
    "**Answer:**\n",
    "Chunking is the process of splitting documents into semantically coherent segments that fit within the model’s context limits. Effective chunking enhances recall and precision of retrieval, prevents context dilution, and improves answer grounding.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Explain the concept of context window.**\n",
    "\n",
    "**Answer:**\n",
    "The context window defines the maximum number of tokens an LLM can process in a single request. A larger context window enables multi-document reasoning and better retrieval comprehension but increases memory usage and inference latency.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. What is prompt leakage and how do you mitigate it?**\n",
    "\n",
    "**Answer:**\n",
    "Prompt leakage occurs when an LLM exposes system instructions or proprietary content. Mitigation includes:\n",
    "\n",
    "* Strong system-level policy enforcement\n",
    "* Output filtering and redaction\n",
    "* Guardrail models trained to suppress restricted content\n",
    "* Structured prompt isolation techniques\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What is the difference between semantic search and keyword search?**\n",
    "\n",
    "**Answer:**\n",
    "Keyword search matches literal text, whereas semantic search uses embeddings to capture meaning and intent. Semantic search retrieves conceptually relevant results even when exact words differ, making it the default engine in RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. What are the common decoding strategies for LLMs?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Greedy decoding:** Deterministic but low creativity\n",
    "* **Beam search:** Balanced exploration; often used for structured generation\n",
    "* **Top-k sampling:** Picks from top-k probable tokens\n",
    "* **Top-p sampling:** Selects from tokens whose cumulative probability ≤ p\n",
    "* **Temperature scaling:** Adjusts randomness\n",
    "\n",
    "---\n",
    "\n",
    "### **11. How does an LLM handle multi-turn conversation?**\n",
    "\n",
    "**Answer:**\n",
    "Conversation is managed through context accumulation. Each interaction is appended to the conversation buffer as system, user, or assistant messages. The model relies on this consolidated history to maintain state and logical continuity.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. What is instruction tuning?**\n",
    "\n",
    "**Answer:**\n",
    "Instruction tuning trains the model on datasets consisting of task instructions and ideal responses. This aligns model behaviors with user intent, improves generalization across tasks, and makes the model more controllable in enterprise workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. What is DPO (Direct Preference Optimization)?**\n",
    "\n",
    "**Answer:**\n",
    "DPO is a fine-tuning technique that aligns model responses with human preferences without using a reinforcement learning loop. It simplifies alignment, reduces computational overhead, and provides more stable optimization compared to traditional RLHF.\n",
    "\n",
    "---\n",
    "\n",
    "### **14. What is the role of guardrail frameworks in GenAI applications?**\n",
    "\n",
    "**Answer:**\n",
    "Guardrails provide operational safety by validating and filtering model input and output. They enforce role-based policies, prevent harmful content, mitigate hallucinations, and ensure regulatory compliance—critical for enterprise adoption.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. What are the key challenges when scaling GenAI systems to production?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Latency and throughput management\n",
    "* Cost governance for inference workloads\n",
    "* Model drift monitoring\n",
    "* Ensuring factual consistency across versions\n",
    "* Data governance, privacy, and compliance\n",
    "* Observability across pipelines\n",
    "* Continuous improvement feedback loops\n",
    "\n",
    "---\n",
    "\n",
    "### **16. What is the significance of embeddings dimensionality?**\n",
    "\n",
    "**Answer:**\n",
    "Higher dimensionality allows richer semantic representation but increases storage and compute overhead. Selecting the appropriate dimension balances retrieval accuracy, latency, and cost—key for enterprise RAG deployments.\n",
    "\n",
    "---\n",
    "\n",
    "### **17. What is chain-of-thought prompting?**\n",
    "\n",
    "**Answer:**\n",
    "It instructs the model to reveal intermediate reasoning steps. This improves logical accuracy, arithmetic reasoning, and multi-step problem solving. However, in production, chain-of-thought is often replaced with structured reasoning to prevent leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### **18. What is model quantization and why is it useful?**\n",
    "\n",
    "**Answer:**\n",
    "Quantization reduces model precision (e.g., fp16 → int8/int4) to decrease memory footprint and accelerate inference. It provides near-lossless performance while enabling deployment on smaller hardware.\n",
    "\n",
    "---\n",
    "\n",
    "### **19. What are hallucination evaluation methods?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Faithfulness scoring\n",
    "* Context relevance\n",
    "* Consistency checks\n",
    "* Grounding validation using RAGAS or custom benchmarks\n",
    "* Dual-model verification loops\n",
    "\n",
    "---\n",
    "\n",
    "### **20. How do you decide between fine-tuning and RAG?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Use RAG** when you need factual grounding, frequent data updates, or low-risk adaptation.\n",
    "* **Use fine-tuning** when you need behavioral transformation, domain-specific writing style, or task specialization.\n",
    "  Enterprises often combine both for optimal performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73aa2b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
