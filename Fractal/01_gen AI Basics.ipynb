{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32321449",
   "metadata": {},
   "source": [
    "# **Interview-Styled Q&A: Basic GenAI Fundamentals**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What is Generative AI?**\n",
    "\n",
    "**Answer:**\n",
    "Generative AI is a class of machine learning systems that create new content—text, images, audio, code, or structured data—by learning patterns from large-scale datasets. These models do not follow deterministic logic; instead, they leverage probability distributions to generate contextually relevant outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How is Generative AI different from traditional machine learning?**\n",
    "\n",
    "**Answer:**\n",
    "Traditional ML primarily focuses on prediction and classification. Generative AI, in contrast, focuses on content creation. Instead of mapping inputs to labels, GenAI models learn the underlying distribution of data and synthesize new artifacts that resemble the learned patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What are foundation models?**\n",
    "\n",
    "**Answer:**\n",
    "Foundation models are large, pre-trained, multi-purpose models—such as GPT, Llama, Claude, and Gemini—trained on massive datasets using self-supervision. They offer broad generalization capabilities and can be adapted for multiple downstream use cases through fine-tuning, prompting, or RAG.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is a Large Language Model (LLM)?**\n",
    "\n",
    "**Answer:**\n",
    "An LLM is a transformer-based neural network trained on extensive text corpora to understand and generate human-like language. It predicts the next token in a sequence, thereby enabling conversational reasoning, summarization, translation, and code generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Can you explain the Transformer architecture in simple terms?**\n",
    "\n",
    "**Answer:**\n",
    "Transformers rely on an attention mechanism that evaluates how each word relates to every other word in a sentence. This enables parallel processing, long-context understanding, and superior semantic learning. Compared to RNNs or LSTMs, transformers scale more efficiently and deliver higher-quality language modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What is attention? Why is it important?**\n",
    "\n",
    "**Answer:**\n",
    "Attention measures the relevance of different tokens when generating an output. It helps the model focus on the most meaningful parts of the input, improving contextual understanding and reducing information loss. It is the core enabler of modern LLM performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What is prompt engineering?**\n",
    "\n",
    "**Answer:**\n",
    "Prompt engineering is the practice of structuring input instructions to steer the model toward accurate, high-fidelity responses. It includes techniques such as role prompting, few-shot prompting, and chain-of-thought prompting. Effective prompting reduces hallucinations and improves task reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. What is the difference between pre-training and fine-tuning?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Pre-training:** The model learns general language patterns from large text corpora.\n",
    "* **Fine-tuning:** The model is further trained on domain-specific datasets to specialize its behavior for targeted business use cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What is RAG (Retrieval Augmented Generation)?**\n",
    "\n",
    "**Answer:**\n",
    "RAG integrates external knowledge retrieval into the generation workflow. The model fetches relevant documents from a knowledge base and uses them as grounding context. This reduces hallucinations, enhances factual accuracy, and allows enterprises to leverage proprietary data securely.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Why do LLMs hallucinate?**\n",
    "\n",
    "**Answer:**\n",
    "Hallucinations occur because LLMs are probability-driven systems that generate the most statistically plausible next token—not necessarily factual truth. Lack of grounding, ambiguous prompts, insufficient training data, or model overconfidence increases hallucination rates.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. What is an embedding?**\n",
    "\n",
    "**Answer:**\n",
    "Embeddings are vector representations of text or other media that capture semantic meaning. They enable similarity search, clustering, recommendation, and retrieval workflows in RAG pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. What is tokenization?**\n",
    "\n",
    "**Answer:**\n",
    "Tokenization breaks text into smaller units—characters, subwords, or words—that the model can process. Techniques like BPE or SentencePiece balance vocabulary size and sequence length to optimize model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. How do you reduce hallucinations in GenAI applications?**\n",
    "\n",
    "**Answer:**\n",
    "Key strategies include:\n",
    "\n",
    "* Enhancing grounding via RAG\n",
    "* Strengthening the prompt with explicit constraints\n",
    "* Applying guardrails and policy filters\n",
    "* Leveraging fine-tuning for domain-specific correctness\n",
    "* Ensuring clean, high-quality enterprise data\n",
    "\n",
    "---\n",
    "\n",
    "### **14. What are common GenAI use cases in enterprises?**\n",
    "\n",
    "**Answer:**\n",
    "Document summarization, conversational assistants, automated knowledge retrieval, report generation, code generation, forecasting enrichment, and workflow automation across BFSI, retail, healthcare, and supply chain domains.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. What are the risks associated with GenAI?**\n",
    "\n",
    "**Answer:**\n",
    "Data leakage, hallucinations, biases, prompt injection attacks, misinformation, and operational risks like cost overruns and uncontrolled model drift. Robust governance and safeguard frameworks are essential.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45de2d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
