{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84472638",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ **Evaluation Metrics — Interview-Style Q&A**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What are evaluation metrics in NLP?**\n",
    "\n",
    "Evaluation metrics are methods to measure how well a model’s output matches the expected result.\n",
    "They help check accuracy, quality, and reliability of NLP or GenAI models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What are common evaluation metrics for text generation?**\n",
    "\n",
    "Some widely used metrics are:\n",
    "\n",
    "* **BLEU** – checks how similar generated text is to reference text\n",
    "* **ROUGE** – measures overlap between generated text and reference text\n",
    "* **METEOR** – checks similarity including synonyms\n",
    "* **BERTScore** – uses embeddings to measure semantic similarity\n",
    "\n",
    "These are commonly used for summarization, translation, and content generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is BLEU score?**\n",
    "\n",
    "BLEU compares the n-grams of generated text with reference text.\n",
    "Higher BLEU means the generated text closely matches the expected output.\n",
    "It is mostly used for translation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is ROUGE score?**\n",
    "\n",
    "ROUGE measures how much the generated text overlaps with the reference text.\n",
    "It is widely used for summarization because it checks how much important content was captured.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What is perplexity?**\n",
    "\n",
    "Perplexity measures how well a language model predicts the next word.\n",
    "Lower perplexity means the model is more confident and better at predicting text.\n",
    "It is a common metric during LLM training.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What is BERTScore?**\n",
    "\n",
    "BERTScore compares embeddings from a model like BERT to measure similarity.\n",
    "It checks whether the generated text has the same meaning, not just the same words.\n",
    "Useful for semantic evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What metrics are used for classification tasks?**\n",
    "\n",
    "Common metrics include:\n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "* AUC-ROC\n",
    "\n",
    "These measure how well the model classifies or detects categories.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. What metrics are used in RAG evaluation?**\n",
    "\n",
    "For RAG specifically, the main metrics are:\n",
    "\n",
    "* **Recall@K** – Are the correct documents retrieved in top K?\n",
    "* **Precision@K** – How many retrieved docs are relevant?\n",
    "* **Similarity Score** – Cosine similarity between embeddings\n",
    "* **Groundedness** – Does the answer stick to retrieved context?\n",
    "\n",
    "These measure how well retrieval supports generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What metrics help detect hallucinations?**\n",
    "\n",
    "Some approaches include:\n",
    "\n",
    "* Groundedness scoring\n",
    "* Faithfulness metrics\n",
    "* Fact-checking consistency\n",
    "* Human evaluation\n",
    "* Retrieval coverage (is relevant context present?)\n",
    "\n",
    "These help ensure the model’s output is aligned with real facts.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. How do you evaluate embeddings?**\n",
    "\n",
    "By checking:\n",
    "\n",
    "* Retrieval accuracy (Recall@K)\n",
    "* Relevance ranking\n",
    "* Cosine similarity distribution\n",
    "* Clustering performance\n",
    "\n",
    "Better embeddings bring more relevant documents into the top results.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. What metrics do you use for summarization evaluation?**\n",
    "\n",
    "* ROUGE\n",
    "* BLEU\n",
    "* BERTScore\n",
    "* Human evaluation for quality, relevance, and completeness\n",
    "\n",
    "---\n",
    "\n",
    "### **12. In enterprise GenAI, which evaluation method is most reliable?**\n",
    "\n",
    "Human evaluation combined with groundedness checks is the most reliable.\n",
    "Automated metrics are useful, but they don’t fully capture correctness or safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50861ecc",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ **1. BLEU Score (Text Generation / Translation)**\n",
    "\n",
    "```python\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = [\"The cat is sitting on the mat\".split()]\n",
    "candidate = \"The cat sits on the mat\".split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(\"BLEU Score:\", score)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **2. ROUGE Score (Summarization)**\n",
    "\n",
    "```python\n",
    "from rouge import Rouge\n",
    "\n",
    "reference = \"The cat is sitting on the mat\"\n",
    "candidate = \"The cat sits on the mat\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(candidate, reference)\n",
    "\n",
    "print(scores)\n",
    "```\n",
    "\n",
    "This prints **ROUGE-1**, **ROUGE-2**, and **ROUGE-L** scores.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **3. BERTScore (Semantic Similarity)**\n",
    "\n",
    "```python\n",
    "import bert_score\n",
    "\n",
    "candidate = [\"The cat sits on the mat\"]\n",
    "reference = [\"A cat is sitting on a mat\"]\n",
    "\n",
    "P, R, F1 = bert_score.score(candidate, reference, lang=\"en\")\n",
    "print(\"BERTScore F1:\", F1.mean().item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **4. Perplexity (Language Model Evaluation)**\n",
    "\n",
    "Using a **GPT-2 model** for demonstration.\n",
    "\n",
    "```python\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import math\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"Generative AI is transforming industries.\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    perplexity = math.exp(loss)\n",
    "\n",
    "print(\"Perplexity:\", perplexity)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **5. Cosine Similarity (Embedding Evaluation)**\n",
    "\n",
    "Used in **RAG** and **semantic search**.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Example embeddings\n",
    "query_emb = np.array([0.1, 0.2, 0.3])\n",
    "doc_emb = np.array([0.1, 0.25, 0.35])\n",
    "\n",
    "score = cosine_similarity([query_emb], [doc_emb])\n",
    "print(\"Cosine Similarity:\", score[0][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **6. Recall@K (RAG Retrieval Evaluation)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# true relevant documents\n",
    "relevant_docs = {\"doc2\", \"doc5\"}\n",
    "\n",
    "# top-K retrieved documents\n",
    "retrieved_docs = [\"doc5\", \"doc7\", \"doc2\"]\n",
    "\n",
    "intersection = len(set(relevant_docs).intersection(set(retrieved_docs)))\n",
    "recall_at_k = intersection / len(relevant_docs)\n",
    "\n",
    "print(\"Recall@K:\", recall_at_k)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **7. Precision@K (RAG Retrieval Evaluation)**\n",
    "\n",
    "```python\n",
    "relevant_docs = {\"doc2\", \"doc5\"}\n",
    "retrieved_docs = [\"doc5\", \"doc7\", \"doc2\"]\n",
    "\n",
    "intersection = len(set(relevant_docs).intersection(set(retrieved_docs)))\n",
    "precision_at_k = intersection / len(retrieved_docs)\n",
    "\n",
    "print(\"Precision@K:\", precision_at_k)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **8. Groundedness Check (Hallucination Detection)**\n",
    "\n",
    "Simple prompt evaluation example.\n",
    "\n",
    "```python\n",
    "generated_answer = \"Payroll tax rate is 12% in 2024.\"\n",
    "retrieved_context = \"Payroll tax rate is 10% in 2024.\"\n",
    "\n",
    "is_grounded = generated_answer in retrieved_context\n",
    "print(\"Grounded?\", is_grounded)\n",
    "```\n",
    "\n",
    "(Real systems use advanced checks, but this is good for interview demonstration.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74937d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
