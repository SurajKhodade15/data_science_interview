{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcaaef42",
   "metadata": {},
   "source": [
    "# ✅ **Embeddings — Interview-Style Q&A (Clean & Professional)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What are embeddings in NLP or GenAI?**\n",
    "\n",
    "Embeddings are numerical vector representations of text that capture meaning.\n",
    "Words or sentences with similar meaning get closer vectors.\n",
    "They allow models to understand semantic relationships rather than just matching exact words.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why do we need embeddings?**\n",
    "\n",
    "Models cannot work with raw text, so embeddings convert text into vectors that the model can compare and understand.\n",
    "They improve semantic search, RAG, classification, and clustering by enabling meaning-based comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is the difference between word embeddings and sentence embeddings?**\n",
    "\n",
    "Word embeddings represent individual words in isolation.\n",
    "Sentence embeddings represent the full meaning of an entire sentence.\n",
    "Sentence embeddings capture more context and are commonly used in RAG systems and search applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Can you give an example of words with similar embeddings?**\n",
    "\n",
    "Words like “doctor” and “hospital” will have embeddings that are close to each other because they are related in meaning.\n",
    "“Doctor” and “banana” will have very distant embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What embedding models have you used?**\n",
    "\n",
    "Models like OpenAI embeddings, bge-large, bge-base, SBERT, and Instructor models.\n",
    "These work well for tasks such as RAG, document similarity, and semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. How do you choose the right embedding model?**\n",
    "\n",
    "Selection depends on accuracy needs, domain specificity, cost, and latency.\n",
    "General-purpose tasks work well with OpenAI or bge models, while domain-heavy use cases benefit from models trained on domain-rich data.\n",
    "I also consider dimensionality and inference cost.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What similarity metrics are commonly used with embeddings?**\n",
    "\n",
    "Cosine similarity and dot product.\n",
    "Cosine similarity measures how similar the direction of the vectors is.\n",
    "Dot product is often used for high-performance large-scale retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Why is embedding normalization important?**\n",
    "\n",
    "Normalization ensures all vectors have the same length.\n",
    "It stabilizes similarity calculations and improves search consistency, especially with cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. How do embeddings help in RAG systems?**\n",
    "\n",
    "In RAG, both the query and documents are converted into embeddings.\n",
    "The system retrieves documents whose embeddings are closest to the query embedding.\n",
    "This enables semantic search rather than exact keyword matching.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. What does embedding dimensionality mean?**\n",
    "\n",
    "Dimensionality is the size of the vector—for example, 768 or 1536.\n",
    "Higher dimensions capture more semantic detail but increase memory and computation requirements.\n",
    "Choosing the right dimension is a balance between accuracy and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. What are some commonly used embedding models?**\n",
    "\n",
    "OpenAI text-embedding-3-large, text-embedding-3-small, bge-large-en, bge-base-en, Sentence-BERT, and Instructor models.\n",
    "These are widely used for semantic search, clustering, RAG, and ranking.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. How do you evaluate embedding performance?**\n",
    "\n",
    "By checking retrieval relevance through:\n",
    "\n",
    "* Recall@k\n",
    "* Precision@k\n",
    "* Cosine similarity distribution\n",
    "* Manual validation\n",
    "  Better embedding models bring more relevant documents into the top results.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. What is embedding drift?**\n",
    "\n",
    "Embedding drift happens when either the embedding model or the data changes over time.\n",
    "This can reduce retrieval accuracy.\n",
    "Monitoring drift and re-embedding documents is necessary to maintain performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **14. Can embedding models handle multiple languages?**\n",
    "\n",
    "Yes, multilingual embedding models like bge-m3, LaBSE, and multilingual MiniLM create shared vector spaces for many languages.\n",
    "This helps in cross-lingual search and multilingual applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **15. Do embeddings capture context?**\n",
    "\n",
    "Yes. Modern transformer-based embedding models understand context.\n",
    "For example, “bank” in “river bank” and “money bank” will produce different embeddings because the sentence meaning changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272107d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cf95a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (c:\\Users\\Suraj Khodade\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize client (make sure OPENAI_API_KEY is set in env variables)\u001b[39;00m\n\u001b[32m      4\u001b[39m client = OpenAI()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'OpenAI' from 'openai' (c:\\Users\\Suraj Khodade\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "documents = [\n",
    "    \"Payroll processing guidelines for 2024.\",\n",
    "    \"Leave policy for contract employees.\",\n",
    "    \"Reimbursement process for travel expenses.\"\n",
    "]\n",
    "\n",
    "# 1. Create embeddings for documents\n",
    "doc_response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=documents\n",
    ")\n",
    "doc_embeddings = [item.embedding for item in doc_response.data]\n",
    "\n",
    "# 2. User query\n",
    "query = \"How do I claim travel bills?\"\n",
    "query_response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ")\n",
    "query_embedding = np.array(query_response.data[0].embedding)\n",
    "\n",
    "# 3. Compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarities = [cosine_similarity(query_embedding, emb) for emb in doc_embeddings]\n",
    "\n",
    "# 4. Get best matching document\n",
    "best_index = int(np.argmax(similarities))\n",
    "print(\"Query:\", query)\n",
    "print(\"Most similar document:\", documents[best_index])\n",
    "print(\"Similarity scores:\", similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b11fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5990d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
