{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prototype: Generative AI–Powered Web Data Summarization\n",
    "\n",
    "Objective:\n",
    "- Scrape an EU energy project / proposal web page.\n",
    "- Extract main text content.\n",
    "- Use OpenAI GPT model to summarize into a structured format:\n",
    "    - project_name\n",
    "    - location\n",
    "    - duration\n",
    "    - start_date\n",
    "    - end_date (optional)\n",
    "    - budget\n",
    "    - funding_program (if any)\n",
    "    - key_objectives\n",
    "    - key_activities\n",
    "    - website_url\n",
    "\n",
    "Stack:\n",
    "    - Python\n",
    "    - requests\n",
    "    - beautifulsoup4\n",
    "    - openai (new client: from openai import OpenAI)\n",
    "\n",
    "Install:\n",
    "    pip install requests beautifulsoup4 openai\n",
    "\n",
    "Env:\n",
    "    export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "\n",
    "Usage:\n",
    "    python web_energy_summarizer.py \"https://example.com/project-page\"\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Data model for structured project summary\n",
    "# -------------------------------------------------------------------\n",
    "@dataclass\n",
    "class ProjectSummary:\n",
    "    project_name: str | None = None\n",
    "    location: str | None = None\n",
    "    duration: str | None = None\n",
    "    start_date: str | None = None\n",
    "    end_date: str | None = None\n",
    "    budget: str | None = None\n",
    "    funding_program: str | None = None\n",
    "    key_objectives: str | None = None\n",
    "    key_activities: str | None = None\n",
    "    website_url: str | None = None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Fetch HTML from URL\n",
    "# -------------------------------------------------------------------\n",
    "def fetch_page(url: str) -> str:\n",
    "    \"\"\"Download the HTML for a given URL.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; GenAI-Summarizer/1.0)\"\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Extract main text from HTML (simple heuristic)\n",
    "# -------------------------------------------------------------------\n",
    "def extract_main_text(html: str) -> str:\n",
    "    \"\"\"Extract visible text from HTML using BeautifulSoup.\n",
    "\n",
    "    For a prototype, we keep it simple:\n",
    "    - Remove script/style tags\n",
    "    - Extract body text\n",
    "    - Collapse excessive whitespace\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove scripts and styles\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    body = soup.body\n",
    "    if body is None:\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        text = body.get_text(separator=\"\\n\")\n",
    "\n",
    "    # Normalize whitespace\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    text = \"\\n\".join([line for line in lines if line])\n",
    "\n",
    "    # Truncate very long pages (to keep token usage reasonable)\n",
    "    max_chars = 8000\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Call OpenAI GPT to create structured summary\n",
    "# -------------------------------------------------------------------\n",
    "def summarize_project_with_llm(page_text: str, url: str) -> ProjectSummary:\n",
    "    \"\"\"Use OpenAI GPT model to extract structured proposal info.\"\"\"\n",
    "    client = OpenAI()  # uses OPENAI_API_KEY from env\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert analyst of European energy project proposals.\n",
    "    Your task is to read the project web page content and extract\n",
    "    a structured summary with the following fields:\n",
    "\n",
    "    - project_name\n",
    "    - location (country/region, or 'Unknown' if not clear)\n",
    "    - duration (e.g. \"36 months\", or a clear textual description)\n",
    "    - start_date (DD/MM/YYYY or Month YYYY if available, otherwise 'Unknown')\n",
    "    - end_date (if available, otherwise 'Unknown')\n",
    "    - budget (total budget or funding amount, including currency)\n",
    "    - funding_program (e.g. Horizon Europe, CEF, etc., or 'Unknown')\n",
    "    - key_objectives (2–4 bullet points of main goals)\n",
    "    - key_activities (2–4 bullet points of main activities/work packages)\n",
    "    - website_url (the input URL)\n",
    "\n",
    "    Important:\n",
    "    - If information is missing, set the value to \"Unknown\".\n",
    "    - Return ONLY valid JSON, no extra text, no explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Below is the raw text extracted from an energy project proposal web page.\n",
    "\n",
    "    WEBSITE URL:\n",
    "    {url}\n",
    "\n",
    "    PAGE CONTENT:\n",
    "    \\\"\\\"\\\"\n",
    "    {page_text}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Now extract the structured summary in JSON with keys:\n",
    "    project_name, location, duration, start_date, end_date,\n",
    "    budget, funding_program, key_objectives, key_activities, website_url.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4.1-mini\" / \"gpt-3.5-turbo\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": textwrap.dedent(system_prompt).strip()},\n",
    "            {\"role\": \"user\", \"content\": textwrap.dedent(user_prompt).strip()},\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # Parse JSON safely\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try to clean minor formatting issues\n",
    "        content_clean = content.strip().strip(\"```\").replace(\"json\", \"\")\n",
    "        data = json.loads(content_clean)\n",
    "\n",
    "    summary = ProjectSummary(\n",
    "        project_name=data.get(\"project_name\"),\n",
    "        location=data.get(\"location\"),\n",
    "        duration=data.get(\"duration\"),\n",
    "        start_date=data.get(\"start_date\"),\n",
    "        end_date=data.get(\"end_date\"),\n",
    "        budget=data.get(\"budget\"),\n",
    "        funding_program=data.get(\"funding_program\"),\n",
    "        key_objectives=data.get(\"key_objectives\"),\n",
    "        key_activities=data.get(\"key_activities\"),\n",
    "        website_url=data.get(\"website_url\", url),\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. End-to-end pipeline for a single URL\n",
    "# -------------------------------------------------------------------\n",
    "def process_project_url(url: str) -> ProjectSummary:\n",
    "    print(f\"[INFO] Fetching page: {url}\")\n",
    "    html = fetch_page(url)\n",
    "\n",
    "    print(\"[INFO] Extracting main text content...\")\n",
    "    page_text = extract_main_text(html)\n",
    "\n",
    "    print(f\"[INFO] Extracted {len(page_text)} characters of text.\")\n",
    "    print(\"[INFO] Calling OpenAI for structured summarization...\")\n",
    "    summary = summarize_project_with_llm(page_text, url)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# CLI entry point\n",
    "# -------------------------------------------------------------------\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python web_energy_summarizer.py <project_url>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    url = sys.argv[1]\n",
    "    summary = process_project_url(url)\n",
    "\n",
    "    print(\"\\n=== Structured Project Summary ===\")\n",
    "    import json\n",
    "\n",
    "    print(json.dumps(asdict(summary), indent=4, ensure_ascii=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
