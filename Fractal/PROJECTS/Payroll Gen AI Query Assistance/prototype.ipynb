{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c467ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prototype: Payroll Gen AI Query Assistant (RAG over Payroll PDFs)\n",
    "\n",
    "Features:\n",
    "- Upload payroll PDF\n",
    "- Store content in Chroma vector DB\n",
    "- Ask questions and get context-aware answers\n",
    "- LangSmith enabled for observability\n",
    "\n",
    "Requirements (pip):\n",
    "    fastapi\n",
    "    uvicorn\n",
    "    langchain\n",
    "    langchain-openai\n",
    "    langchain-community\n",
    "    chromadb\n",
    "    sentence-transformers\n",
    "    pypdf\n",
    "\n",
    "Env variables:\n",
    "    OPENAI_API_KEY=...\n",
    "    LANGCHAIN_TRACING_V2=true\n",
    "    LANGCHAIN_API_KEY=...\n",
    "    LANGCHAIN_PROJECT=Payroll-GenAI\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = Path(__file__).parent\n",
    "UPLOAD_DIR = BASE_DIR / \"uploads\"\n",
    "VECTOR_DB_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# LangSmith (set via env; here just for clarity)\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your_langsmith_key\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Payroll-GenAI\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Global objects (simple prototype style)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "app = FastAPI(title=\"Payroll Gen AI Query Assistant\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Embeddings (Hugging Face â€“ light & good for demo)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Vector store (Chroma)\n",
    "vectorstore: Optional[Chroma] = None\n",
    "qa_chain: Optional[RetrievalQA] = None\n",
    "\n",
    "\n",
    "def init_vectorstore() -> Chroma:\n",
    "    \"\"\"Initialize or reload Chroma vector store.\"\"\"\n",
    "    return Chroma(\n",
    "        collection_name=\"payroll_docs\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=str(VECTOR_DB_DIR),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_qa_chain(store: Chroma) -> RetrievalQA:\n",
    "    \"\"\"Build RetrievalQA chain on top of Chroma + OpenAI LLM.\"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    retriever = store.as_retriever(\n",
    "        search_kwargs={\"k\": 4}\n",
    "    )\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Initialize vector store if already exists\n",
    "if any(VECTOR_DB_DIR.iterdir()):\n",
    "    vectorstore = init_vectorstore()\n",
    "    qa_chain = build_qa_chain(vectorstore)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def save_uploaded_file(upload_file: UploadFile) -> Path:\n",
    "    \"\"\"Save uploaded PDF to disk and return path.\"\"\"\n",
    "    if not upload_file.filename.lower().endswith(\".pdf\"):\n",
    "        raise HTTPException(status_code=400, detail=\"Only PDF files are supported.\")\n",
    "\n",
    "    file_path = UPLOAD_DIR / upload_file.filename\n",
    "    with file_path.open(\"wb\") as buffer:\n",
    "        shutil.copyfileobj(upload_file.file, buffer)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def index_pdf_to_chroma(pdf_path: Path) -> None:\n",
    "    \"\"\"Load PDF, split, embed, and store in Chroma.\"\"\"\n",
    "    global vectorstore, qa_chain\n",
    "\n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    documents = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    if vectorstore is None:\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=\"payroll_docs\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=str(VECTOR_DB_DIR),\n",
    "        )\n",
    "\n",
    "    vectorstore.add_documents(chunks)\n",
    "    vectorstore.persist()\n",
    "\n",
    "    # Rebuild QA chain whenever new docs are added\n",
    "    qa_chain = build_qa_chain(vectorstore)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# API Endpoints\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"ok\", \"message\": \"Payroll Gen AI Assistant is running\"}\n",
    "\n",
    "\n",
    "@app.post(\"/upload-pdf\")\n",
    "async def upload_pdf(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Upload a payroll/HR policy PDF.\n",
    "    The content will be indexed into Chroma and used for Q&A.\n",
    "    \"\"\"\n",
    "    file_path = save_uploaded_file(file)\n",
    "    index_pdf_to_chroma(file_path)\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"filename\": file.filename,\n",
    "        \"message\": \"PDF indexed successfully into vector store.\",\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(question: str = Form(...)):\n",
    "    \"\"\"\n",
    "    Ask a question related to payroll/HR.\n",
    "    The answer is generated using RAG over uploaded documents.\n",
    "    \"\"\"\n",
    "    global qa_chain\n",
    "\n",
    "    if qa_chain is None:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"No documents indexed yet. Please upload a payroll PDF first.\",\n",
    "        )\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    sources = [\n",
    "        {\"source\": doc.metadata.get(\"source\", \"\"), \"page\": doc.metadata.get(\"page\", None)}\n",
    "        for doc in result.get(\"source_documents\", [])\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Run command:\n",
    "#   uvicorn main:app --reload\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
