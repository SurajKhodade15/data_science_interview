{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57a6c02",
   "metadata": {},
   "source": [
    "\n",
    "# ‚úÖ **Tokenization ‚Äî Interview-Ready Explanation**\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 1:**\n",
    "\n",
    "**What is tokenization in LLMs?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúTokenization is the process of breaking text into smaller units called tokens.\n",
    "These tokens are usually words or sub-words.\n",
    "LLMs don‚Äôt understand raw text, so tokenization converts the text into numerical IDs that the model can work with.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 2:**\n",
    "\n",
    "**Why is tokenization important?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúIt allows the model to process text efficiently.\n",
    "Tokenization reduces the vocabulary size, handles rare words, and helps the model understand text in a structured way.\n",
    "Without tokenization, the model cannot convert text into a form it can learn from.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 3:**\n",
    "\n",
    "**What types of tokenization are commonly used in LLMs?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúMost LLMs use sub-word tokenization techniques such as:\n",
    "\n",
    "* **BPE (Byte Pair Encoding)**\n",
    "* **WordPiece**\n",
    "* **SentencePiece**\n",
    "\n",
    "These methods split long or rare words into smaller pieces so the model can handle any type of text.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 4:**\n",
    "\n",
    "**Can you give an example of tokenization?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúIf we take the word *‚Äòunbelievable‚Äô*, a subword tokenizer may split it as:\n",
    "`un + believ + able`\n",
    "This helps the model understand the structure and meaning even if the full word is rare.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 5:**\n",
    "\n",
    "**How do tokenizers handle unknown or new words?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúSub-word tokenizers break unknown words into smaller known pieces.\n",
    "So even if a word is new, the model can still understand it by combining the sub-parts.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 6:**\n",
    "\n",
    "**What is the connection between tokenization and model context length?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúModels process text in tokens, not words.\n",
    "So context length is measured in tokens.\n",
    "If a model has a 4,000-token limit, it can only handle text up to 4,000 tokens, regardless of how many words that is.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Interview Question 7:**\n",
    "\n",
    "**Does tokenization impact cost and performance?**\n",
    "\n",
    "### **Simple Interview Answer:**\n",
    "\n",
    "‚ÄúYes.\n",
    "More tokens mean higher inference costs and slower processing.\n",
    "So efficient tokenization helps reduce cost and improves speed.‚Äù\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b704d",
   "metadata": {},
   "source": [
    "Below is a **simple and interview-friendly coding example** that shows how tokenization works in practice.\n",
    "I‚Äôll give two examples:\n",
    "\n",
    "1. **Using HuggingFace Tokenizer** (industry standard)\n",
    "2. **Manual Tokenization** (so you can explain the logic in interviews)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **1. Tokenization Using HuggingFace (Real-World Example)**\n",
    "\n",
    "This is how tokenization happens in modern LLMs like GPT-2, Llama, Mistral, etc.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer (GPT-2 for example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Generative AI is transforming industries.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decode back\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "```\n",
    "\n",
    "### **Output (sample):**\n",
    "\n",
    "```\n",
    "Tokens: ['Gener', 'ative', ' AI', ' is', ' transforming', ' industries', '.']\n",
    "Token IDs: [41471, 7296, 483, 318, 3941, 6522, 13]\n",
    "Decoded Text: Generative AI is transforming industries.\n",
    "```\n",
    "\n",
    "### **What this demonstrates in interview style:**\n",
    "\n",
    "* How text is split into subword tokens\n",
    "* How token IDs are generated\n",
    "* How the model converts IDs back to text\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **2. Simple Manual Tokenization (For Explaining the Concept)**\n",
    "\n",
    "This is NOT how LLMs work internally, but it helps demonstrate the idea.\n",
    "\n",
    "```python\n",
    "text = \"Generative AI is transforming industries.\"\n",
    "\n",
    "# Simple whitespace tokenization\n",
    "tokens = text.split()\n",
    "print(\"Tokens:\", tokens)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "\n",
    "```\n",
    "Tokens: ['Generative', 'AI', 'is', 'transforming', 'industries.']\n",
    "```\n",
    "\n",
    "You can use this to explain that traditional tokenization is word-based,\n",
    "but LLMs use **subword tokenization** for better handling of rare words.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **3. Subword Example Using SentencePiece (Llama/Llama2 style)**\n",
    "\n",
    "This shows how words are broken into smaller pieces.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "text = \"unbelievable performance\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "### Sample Output:\n",
    "\n",
    "```\n",
    "['un', '##bel', '##iev', '##able', 'performance']\n",
    "```\n",
    "\n",
    "This is exactly what you can explain in the interview:\n",
    "\n",
    "* Unknown/long words get broken into meaningful chunks.\n",
    "* This makes the model robust to new vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a7c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suraj Khodade\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Gener', 'ative', 'ƒ†AI', 'ƒ†is', 'ƒ†transforming', 'ƒ†industries', '.']\n",
      "Token IDs: [8645, 876, 9552, 318, 25449, 11798, 13]\n",
      "Decoded Text: Generative AI is transforming industries.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer (GPT-2 for example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Generative AI is transforming industries.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decode back\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
