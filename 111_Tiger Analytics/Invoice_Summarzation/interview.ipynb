{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca801292",
   "metadata": {},
   "source": [
    "Excellent\\! This is a fantastic and highly relevant project for a data scientist role, especially at a company like Tiger Analytics which focuses on AI & Analytics services. Let's break this down into a compelling story and a rigorous Q\\&A session.\n",
    "\n",
    "First, let's craft the story. When an interviewer asks, \"Tell me about a challenging project you've worked on,\" this is the narrative you'll use.\n",
    "\n",
    "-----\n",
    "\n",
    "### Part 1: The Project Story (The \"Elevator Pitch\")\n",
    "\n",
    "**(Interviewer: \"Can you walk me through your Invoice Summarization project?\")**\n",
    "\n",
    "\"Certainly. In my previous role/project, we identified a significant operational bottleneck within the finance department. The team was spending several hours each day manually processing and analyzing invoices. They had to read through each one to extract key information like total amount, due date, vendor details, or line items, and summarizing multiple invoices for reports was a tedious, error-prone task. If they had a specific question, they'd have to manually search through piles of documents.\n",
    "\n",
    "My objective was to automate this entire process by building an intelligent assistant that could not only summarize invoices instantly but also allow finance professionals to 'chat' with their documents in plain English.\n",
    "\n",
    "To achieve this, I designed and implemented a solution using a technique called Retrieval-Augmented Generation, or RAG. The workflow was straightforward yet powerful.\n",
    "\n",
    "1.  **Ingestion & Processing:** First, a user would upload an invoice, typically a PDF. The system would extract the raw text from it. The real challenge here is that invoices have complex structures, not just plain paragraphs. So, I implemented a smart chunking strategy to break the text into smaller, semantically meaningful pieces while trying to preserve the tabular data and key-value pairs.\n",
    "\n",
    "2.  **Vectorization:** Each of these text chunks was then converted into a numerical representation, or an 'embedding', using a sentence-transformer model from Hugging Face. These embeddings capture the semantic meaning of the text. I stored these vectors in a Chroma vector database, which is incredibly efficient for high-speed similarity searches.\n",
    "\n",
    "3.  **Intelligent Retrieval:** When a user asks a question, like *'What was the total payment due to ACME Corp in July?'*, the user's query is also converted into an embedding. The system then performs a similarity search in Chroma to find the most relevant text chunks from the invoices that are semantically closest to the user's question.\n",
    "\n",
    "4.  **Augmented Generation:** This is the core of the solution. The retrieved, relevant text chunks are then combined with the original question and fed into a carefully crafted prompt for an OpenAI GPT model. By providing this specific context, we're not asking the LLM to answer from its general knowledge; we're instructing it to find the answer *only* within the provided text from the invoice. This drastically reduces hallucinations and ensures factual accuracy.\n",
    "\n",
    "5.  **Delivery:** The entire backend logic was wrapped in a REST API using FastAPI, making it scalable and easy to integrate with any front-end application.\n",
    "\n",
    "The result was a transformative tool for the finance team. We measured a **reduction in the time taken for summarization and ad-hoc querying by approximately 40%**. More importantly, it empowered the team to get instant, accurate insights from their financial documents, freeing them up to focus on more strategic analysis rather than manual data extraction.\"\n",
    "\n",
    "-----\n",
    "\n",
    "### Part 2: Technical Questions & Answers\n",
    "\n",
    "Here is a comprehensive list of questions an interviewer might ask, categorized for clarity.\n",
    "\n",
    "#### A. Architecture and Design Choices\n",
    "\n",
    "**Q1: Why did you choose the RAG (Retrieval-Augmented Generation) architecture? Why not just fine-tune a model?**\n",
    "**A:** We chose RAG for several key reasons. Firstly, **Data Freshness and Scalability**: Fine-tuning requires retraining the model every time new invoices come in, which is computationally expensive and slow. With RAG, we can simply add new invoice embeddings to our Chroma vector database in near real-time. Secondly, **Reduced Hallucinations**: Fine-tuning can still lead to the model generating plausible but incorrect information. RAG grounds the LLM's response in the specific text retrieved from the document, making the answers more factual and verifiable. Lastly, **Cost-Effectiveness**: Fine-tuning large models is expensive. The RAG approach, using a pre-trained LLM and a vector DB, was a much more cost-effective solution to build and maintain.\n",
    "\n",
    "**Q2: You used Chroma as your vector database. Why Chroma? Did you consider other options like FAISS, Pinecone, or Weaviate?**\n",
    "**A:** Yes, I did. We chose Chroma primarily because it's open-source, lightweight, and incredibly easy to set up and run locally, making it perfect for rapid prototyping and development. For our initial scale, its performance was more than sufficient. I considered FAISS, which is a library, not a full-fledged database, and would have required more engineering effort to build the surrounding service. Pinecone and Weaviate are excellent, managed solutions, but they introduce external dependencies and costs. The plan was to start with Chroma and, if the scale demanded it, migrate to a more robust, managed solution like Pinecone later, which is a relatively straightforward process.\n",
    "\n",
    "**Q3: Why LangChain? What specific components of LangChain were most useful?**\n",
    "**A:** LangChain was the backbone of our application, acting as the orchestrator that connected all the moving parts. The most useful components for me were:\n",
    "\n",
    "  * **Document Loaders and Text Splitters:** These modules made it trivial to ingest PDFs and implement sophisticated chunking strategies, like the `RecursiveCharacterTextSplitter`, which respects sentence and paragraph boundaries.\n",
    "  * **VectorStore Integrations:** LangChain has built-in wrappers for various vector databases, including Chroma. This allowed me to easily embed, store, and retrieve documents with just a few lines of code.\n",
    "  * **Chains (specifically `RetrievalQA`):** This is the core component that encapsulates the entire RAG logic. It seamlessly handles taking a user query, retrieving relevant documents from the vector store, stuffing them into a prompt, and getting a response from the LLM. It saved a massive amount of boilerplate code.\n",
    "\n",
    "**Q4: You chose FastAPI for the API. Why not Flask or Django?**\n",
    "**A:** FastAPI was the ideal choice for this project due to its high performance. It's built on top of Starlette and Pydantic, which gives it asynchronous capabilities out-of-the-box. Since calls to the OpenAI API can have some latency, being able to handle requests asynchronously is a huge advantage for scalability. Additionally, FastAPI's automatic generation of interactive API documentation (via Swagger UI) is a massive productivity booster for development and testing.\n",
    "\n",
    "#### B. The RAG Pipeline in Depth\n",
    "\n",
    "**Q5: Let's talk about chunking. How did you decide on the chunk size and overlap? What challenges did you face?**\n",
    "**A:** This was a critical step. The main challenge with invoices is that information is often tabular or in key-value pairs. A naive fixed-size chunking could split a table row or an item-price pair, destroying the context.\n",
    "\n",
    "  * **Strategy:** I used LangChain's `RecursiveCharacterTextSplitter`. It tries to split text based on a hierarchy of separators like double newlines (`\\n\\n`), single newlines (`\\n`), and then spaces. This is generally better at keeping related paragraphs together.\n",
    "  * **Chunk Size:** I experimented with different sizes, starting around 1000 characters. Too small, and you lose context (e.g., the answer to a question requires information from two separate chunks). Too large, and you introduce noise into the LLM prompt, potentially confusing the model and exceeding context window limits. I settled on a size of around **512-768 tokens** as a sweet spot.\n",
    "  * **Overlap:** I introduced a chunk overlap of about **10-15%**. This is crucial to ensure that a piece of information that falls at the very end of one chunk is also present at the beginning of the next, preventing context from being lost at the split boundaries.\n",
    "\n",
    "**Q6: Which embedding model from Hugging Face did you use, and why?**\n",
    "**A:** I used the `sentence-transformers/all-MiniLM-L6-v2` model. It offers an excellent balance between performance and size. It's fast, runs efficiently on a CPU, and produces high-quality 384-dimensional embeddings that are great for semantic search. While larger models might offer slightly better performance, the speed of this model was essential for ensuring a responsive user experience. I also chose a Hugging Face model over OpenAI's `text-embedding-ada-002` to keep the embedding process in-house, reducing API costs and latency.\n",
    "\n",
    "**Q7: How did you actually perform the similarity search? What metric did you use?**\n",
    "**A:** The vector search was handled by Chroma. Under the hood, it typically uses the **Cosine Similarity** metric. Cosine similarity is ideal for this use case because it measures the cosine of the angle between two vectors, focusing on their orientation rather than their magnitude. In the context of text embeddings, this means it's excellent at determining if two pieces of text have a similar *meaning*, regardless of their length. The search would return the 'k' most similar chunks, where 'k' is a hyperparameter I set to 4 or 5 to provide sufficient context to the LLM without overloading it.\n",
    "\n",
    "**Q8: Can you write down the structure of the prompt you passed to the OpenAI model?**\n",
    "**A:** Absolutely. Prompt engineering was key to getting accurate results. The prompt followed a clear structure:\n",
    "\n",
    "```\n",
    "You are a helpful assistant for finance professionals. Your task is to answer questions based ONLY on the provided context from an invoice. If the answer is not found in the context, you must explicitly say \"I cannot find the answer in the provided document.\" Do not use any external knowledge.\n",
    "\n",
    "CONTEXT:\n",
    "---\n",
    "{retrieved_document_chunk_1}\n",
    "---\n",
    "{retrieved_document_chunk_2}\n",
    "---\n",
    "...\n",
    "---\n",
    "\n",
    "QUESTION: {user_question}\n",
    "\n",
    "ANSWER:\n",
    "```\n",
    "\n",
    "This structure is effective because it clearly delineates the context, sets strict rules for the LLM (e.g., \"ONLY on the provided context\"), and tells it how to behave when the information is missing, which is crucial for preventing hallucinations.\n",
    "\n",
    "#### C. Evaluation and Scenario-Based Questions\n",
    "\n",
    "**Q9: The user says the chatbot gave a wrong answer. How would you debug this?**\n",
    "**A:** This is a great question. I would follow a step-by-step debugging process that traces the RAG pipeline:\n",
    "\n",
    "1.  **Check the Source:** First, I'd look at the original invoice PDF to see if the text extraction (OCR) process worked correctly. Was the text garbled or misinterpreted?\n",
    "2.  **Inspect the Retrieval:** Next, I'd examine the exact chunks that were retrieved from Chroma for that specific question. Was the correct information present in these chunks? If not, the issue lies with the embedding/retrieval step. Maybe the chunking strategy was poor, or the query and the text were too semantically different for the embedding model to match.\n",
    "3.  **Analyze the Generation:** If the correct chunks *were* retrieved, I'd look at the exact prompt (context + question) sent to the LLM and its response. This would tell me if the LLM misinterpreted the context or ignored my instructions. In this case, I'd need to refine the prompt.\n",
    "4.  **Isolate the Component:** By following this path, I can isolate the failure to either text extraction, chunking, retrieval, or generation, and then focus my efforts on fixing that specific component.\n",
    "\n",
    "**Q10: Your system works for one invoice. How would you scale it to handle thousands of invoices and allow users to ask questions across all of them?**\n",
    "**A:** Scaling requires a few architectural changes:\n",
    "\n",
    "1.  **Metadata is Key:** When embedding and storing chunks, I would add metadata to each vector. For example: `{'invoice_id': 'INV-123', 'vendor_name': 'ACME Corp', 'date': '2025-10-01'}`.\n",
    "2.  **Pre-filtering:** When a user asks a question, we can first use this metadata to filter the search space. If they ask, \"What did I buy from ACME Corp?\", the system would first filter the Chroma DB to only include vectors where `vendor_name == 'ACME Corp'`. The similarity search is then performed only on this much smaller, relevant subset of vectors. This drastically improves speed and accuracy.\n",
    "3.  **Scalable Infrastructure:** I would move the Chroma DB to a dedicated server instead of running it in memory. For the FastAPI application, I'd use a production-grade server like Gunicorn with multiple worker processes and place it behind a load balancer to handle concurrent user requests.\n",
    "\n",
    "**Q11: What if an invoice contains complex tables? How does your system handle that?**\n",
    "**A:** This is a known challenge for simple text splitting. My initial approach would handle simple tables if they were extracted as formatted text. However, for complex, multi-page tables, a more advanced solution would be needed. I would explore two paths:\n",
    "\n",
    "1.  **Specialized Parsing:** Use tools like `camelot` or `tabula-py` in Python to specifically extract tables from PDFs and convert them into a structured format like CSV or Markdown before the chunking and embedding process. This preserves the row/column relationships.\n",
    "2.  **Multi-modal Models:** The next frontier would be to use a multi-modal model (like GPT-4 with Vision) that can directly \"see\" the invoice layout, including tables and images, eliminating the need for a brittle text extraction and chunking pipeline. This would be a significant but powerful upgrade.\n",
    "\n",
    "**Q12: How did you measure the 40% reduction in summarization time? How would you quantitatively evaluate the *quality* of the chatbot's answers?**\n",
    "**A:**\n",
    "\n",
    "  * **Measuring Time Reduction:** The 40% figure was derived from a user study. We benchmarked the time it took for a group of finance professionals to perform a set of 10 standard tasks (e.g., \"find the total tax amount for these 5 invoices,\" \"summarize all line items from vendor X\") manually. We then measured the time it took them to complete the same tasks using the chatbot. The average reduction in completion time across all users and tasks was \\~40%.\n",
    "  * **Evaluating Quality:** This is more complex. I would use a framework like RAGAS (RAG Assessment), which evaluates RAG pipelines on metrics like:\n",
    "      * **Faithfulness:** Does the answer generated by the LLM factually align with the retrieved context?\n",
    "      * **Answer Relevancy:** How relevant is the answer to the user's question?\n",
    "      * **Context Precision:** Is the retrieved context precise and free of irrelevant information?\n",
    "        To get a baseline, we'd create a \"golden dataset\" of a few dozen question-answer pairs curated by domain experts and evaluate the system's performance against it.\n",
    "\n",
    "Good luck with your interview at Tiger Analytics\\! This is a solid project, and being able to discuss it with this level of depth will definitely impress them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49bde2",
   "metadata": {},
   "source": [
    "\n",
    "1. **Storytelling Answer** (how to narrate the project in interview)\n",
    "2. **Technical Deep Dive (Q&A)**\n",
    "3. **Scenario-Based & Behavioral Questions** (interviewer curveballs)\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Storytelling Answer (Interview Style)\n",
    "\n",
    "> *‚ÄúOne of my most impactful projects was developing an **Invoice Summarization and Chatbot Assistant** that automated finance query handling. The problem statement was that financial analysts spent significant time manually reading invoices, extracting key details, and answering repetitive finance-related questions.\n",
    ">\n",
    "> I designed a **Retrieval-Augmented Generation (RAG) pipeline** using **LangChain + OpenAI GPT-3.5**. The workflow started with **uploading invoices in PDF format**, parsing the text with PyPDF, chunking the text into manageable segments, and storing embeddings in **ChromaDB** using **Hugging Face‚Äôs `all-MiniLM-L6-v2`** embedding model.\n",
    ">\n",
    "> When a user asked a question, we retrieved the most relevant chunks from the vector DB, injected them into a carefully engineered prompt template, and passed it to GPT-3.5. This ensured answers were strictly grounded in invoice data.\n",
    ">\n",
    "> On the backend, I built APIs with **FastAPI** for two key services: uploading invoices and querying them. I also created **Pydantic models** for request/response validation and containerized the application for deployment.\n",
    ">\n",
    "> As a result, summarization and query response time dropped by ~40%, and the system enabled finance teams to get **instant, context-grounded answers** without manual searching. This solution also laid the foundation for scaling into broader document intelligence workflows.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Technical Deep Dive (Q&A)\n",
    "\n",
    "### **Architecture Questions**\n",
    "\n",
    "**Q: Why did you use RAG instead of directly prompting GPT with invoices?**\n",
    "A: Direct prompting suffers from **context length limitations** and **hallucinations**. By chunking and embedding invoices, then retrieving only relevant sections, RAG ensures **scalability, accuracy, and cost efficiency**.\n",
    "\n",
    "**Q: Why Hugging Face `all-MiniLM-L6-v2` for embeddings?**\n",
    "A: It offers a **balanced tradeoff** between **accuracy and performance**. It has ~22M parameters, lightweight enough for fast vectorization while delivering strong semantic similarity on invoice-like texts.\n",
    "\n",
    "**Q: Why ChromaDB over Pinecone/Weaviate?**\n",
    "A: ChromaDB provides a **lightweight, local, persistent vector DB**, suitable for enterprise PoCs and cost-free experimentation. For production scale, we could extend to managed vector DBs like Pinecone.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Questions**\n",
    "\n",
    "**Q: How did you chunk the invoices?**\n",
    "A: I used **RecursiveCharacterTextSplitter** with `chunk_size=700` and `chunk_overlap=100`. This size ensures semantic coherence while staying within embedding token limits.\n",
    "\n",
    "**Q: How does your `ask_question` prompt prevent hallucinations?**\n",
    "A: The template enforces:\n",
    "\n",
    "* *‚ÄúAnswer only from provided context.‚Äù*\n",
    "* *‚ÄúIf not found, say: I cannot find the answer.‚Äù*\n",
    "  This **restricts model creativity** and maintains factual correctness.\n",
    "\n",
    "**Q: How do you handle multiple invoices uploaded?**\n",
    "A: Each ingestion appends new embeddings into the Chroma persistent DB. We also tag metadata (e.g., filename, upload timestamp) to filter and retrieve invoice-specific contexts.\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance & Optimization**\n",
    "\n",
    "**Q: How did you measure the 40% improvement in summarization time?**\n",
    "A: We benchmarked **manual analyst query resolution time vs. automated chatbot**. On average, an analyst took ~5 minutes per query; chatbot responses averaged ~3 minutes end-to-end (including parsing + retrieval + LLM call).\n",
    "\n",
    "**Q: How do you ensure system scalability?**\n",
    "A: Using **FastAPI async endpoints**, caching embeddings, and **batch ingestion**. For scaling, we can deploy vector DB and LLM calls on separate services, possibly containerized via Docker.\n",
    "\n",
    "---\n",
    "\n",
    "### **Error Handling / Edge Cases**\n",
    "\n",
    "**Q: What if the PDF has images or scanned text?**\n",
    "A: PyPDF only extracts digital text. For image-based invoices, we integrate **OCR (Tesseract / AWS Textract)** before ingestion.\n",
    "\n",
    "**Q: How do you prevent garbage-in-garbage-out?**\n",
    "A: We validate invoice file format, size, and enforce text extraction checks. If the document yields insufficient text, we reject ingestion with an error response.\n",
    "\n",
    "**Q: How do you secure sensitive financial data?**\n",
    "A:\n",
    "\n",
    "* Used `.env` for API keys.\n",
    "* Configured access control on FastAPI endpoints.\n",
    "* Data stored locally in Chroma (not in external APIs).\n",
    "* Optionally, encrypt stored embeddings at rest.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Scenario-Based & Behavioral Questions\n",
    "\n",
    "### **Scenario 1: Hallucination**\n",
    "\n",
    "**Q: The model answers with fabricated data. How would you fix it?**\n",
    "A:\n",
    "\n",
    "* Reinforce the **prompt guardrails** (strict \"do not answer outside context\").\n",
    "* Implement **confidence scoring**: if retriever similarity < threshold, return ‚ÄúNot Found‚Äù.\n",
    "* Optionally, fine-tune or use OpenAI‚Äôs **function calling**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario 2: Scaling to Multiple Document Types**\n",
    "\n",
    "**Q: Tomorrow your manager wants this system to handle contracts and purchase orders, not just invoices. What would you change?**\n",
    "A:\n",
    "\n",
    "* Extend schema metadata (document type).\n",
    "* Add domain-specific prompt templates.\n",
    "* Possibly create separate vector DB collections per document type for structured retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario 3: Business Value**\n",
    "\n",
    "**Q: What impact did this project bring to finance teams?**\n",
    "A:\n",
    "\n",
    "* Reduced manual effort by 40%.\n",
    "* Ensured **consistency in answers** (less human error).\n",
    "* Laid foundation for **self-service analytics** where business teams can query invoices without finance analysts‚Äô intervention.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c74e92",
   "metadata": {},
   "source": [
    "\n",
    "# üîπ Extended Q&A for *Invoice Summarization & Chatbot Assistant*\n",
    "\n",
    "---\n",
    "\n",
    "## **A. Technical & Design Questions**\n",
    "\n",
    "**Q1. Why FastAPI instead of Flask or Django?**\n",
    "\n",
    "* **A:** FastAPI provides async I/O support, automatic request validation with Pydantic, and superior performance for API-based ML workloads. Flask is simpler but synchronous, and Django is heavy for this microservice use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How do you decide chunk size and overlap for invoices?**\n",
    "\n",
    "* **A:** I chose `chunk_size=700` and `overlap=100` to balance context preservation and embedding efficiency. Too small chunks risk fragmenting invoice items, while too large chunks risk exceeding token limits during retrieval. Overlap ensures continuity for split entities (e.g., line items across pages).\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. Why did you choose OpenAI GPT-3.5 over open-source LLMs?**\n",
    "\n",
    "* **A:** GPT-3.5 provides high-quality natural language responses and robustness. However, for cost/privacy concerns, we evaluated open-source alternatives (e.g., LLaMA2, Falcon) with Hugging Face Inference endpoints. For production, we‚Äôd benchmark both options on domain-specific data.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What are the limitations of ChromaDB?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Not distributed ‚Üí limited scaling for very large corpora.\n",
    "  * Limited enterprise-grade security (compared to Pinecone/Weaviate).\n",
    "  * No native hybrid search (text + metadata) optimizations at scale.\n",
    "  * For enterprise deployment, migration to managed vector DB may be required.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. How did you validate the system‚Äôs accuracy?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Created a test set of finance queries with ground-truth answers from SMEs.\n",
    "  * Measured **retrieval precision/recall** (whether relevant invoice sections were fetched).\n",
    "  * Measured **LLM accuracy** by comparing generated answers to SME-validated outputs.\n",
    "  * Also monitored **hallucination rate** (% of ‚Äúmade-up‚Äù answers).\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How do you manage multiple users uploading invoices simultaneously?**\n",
    "\n",
    "* **A:** Each upload creates a unique namespace or collection in Chroma, tagged with user/session metadata. Queries are scoped only to that user‚Äôs collection. This prevents data leakage and ensures multi-tenancy support.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. Why use `RecursiveCharacterTextSplitter` instead of a simple tokenizer?**\n",
    "\n",
    "* **A:** It preserves semantic boundaries better. Recursive splitter tries splitting by paragraphs, then sentences, then words‚Äîensuring chunks stay meaningful instead of arbitrary word/token cuts.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How does the RetrievalQA `stuff` chain work in LangChain?**\n",
    "\n",
    "* **A:** It takes retrieved documents, concatenates them into a single context block, and injects them into the LLM prompt. Alternative chain types like `map_reduce` or `refine` are used for longer contexts but are slower. For invoices, `stuff` is sufficient since chunks are manageable.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What‚Äôs the latency profile of this application?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * PDF parsing: 1‚Äì2 sec per invoice (depending on pages).\n",
    "  * Embedding + vector DB ingestion: ~3‚Äì5 sec for medium invoices.\n",
    "  * Query answering: ~2‚Äì3 sec retrieval + 2‚Äì4 sec GPT response.\n",
    "  * End-to-end ~8‚Äì12 sec (vs. 5‚Äì10 min manually).\n",
    "\n",
    "---\n",
    "\n",
    "**Q10. How do you prevent duplicate embeddings on re-upload?**\n",
    "\n",
    "* **A:** Use hashing of text chunks before inserting into Chroma. If hash exists, skip insertion. This avoids DB bloat.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **B. Scenario-Based Questions**\n",
    "\n",
    "**Q11. If an invoice contains multiple currencies, how will your system handle queries about totals?**\n",
    "\n",
    "* **A:** Currently, the system returns verbatim invoice values. To support multi-currency normalization, we can:\n",
    "\n",
    "  * Extract currency metadata during ingestion.\n",
    "  * Use rules or APIs (e.g., exchange rate API) for on-the-fly normalization.\n",
    "\n",
    "---\n",
    "\n",
    "**Q12. Suppose invoices are 200+ pages long. How will you handle context window limitations?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Use hierarchical RAG: first retrieve at section-level, then refine at line-item level.\n",
    "  * Use `map_reduce` summarization chains to compress large sections.\n",
    "  * For very large docs, preprocess with domain-specific extractors (e.g., regex for invoice number, total amount).\n",
    "\n",
    "---\n",
    "\n",
    "**Q13. What if the finance team wants tabular summaries (like line items, totals)?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Integrate **OpenAI function calling / JSON schema outputs** to enforce structured responses.\n",
    "  * For recurring fields, design prompt templates like:\n",
    "\n",
    "    ```json\n",
    "    { \"invoice_number\": \"...\", \"total_amount\": \"...\", \"due_date\": \"...\" }\n",
    "    ```\n",
    "  * This enables downstream integration with BI tools.\n",
    "\n",
    "---\n",
    "\n",
    "**Q14. What would you do if GPT starts hallucinating despite guardrails?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Add retrieval confidence threshold. If cosine similarity < threshold, return ‚ÄúNot Found.‚Äù\n",
    "  * Apply **output validation** via regex or schema enforcement.\n",
    "  * Optionally, replace GPT with **smaller fine-tuned model** specialized on invoices.\n",
    "\n",
    "---\n",
    "\n",
    "**Q15. How will you optimize costs in production?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Use smaller embedding models (MiniLM instead of larger SBERTs).\n",
    "  * Cache embeddings and retrieval results.\n",
    "  * Use GPT-3.5 for most queries, escalate to GPT-4 only for complex queries.\n",
    "  * Explore hybrid infra (open-source LLMs on-prem + OpenAI fallback).\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **C. Business & Behavioral Questions**\n",
    "\n",
    "**Q16. How do you explain this project to a non-technical finance leader?**\n",
    "\n",
    "* **A:** *‚ÄúThink of it as a smart assistant that instantly reads all your invoices and answers your questions ‚Äî like ‚ÄòWhat‚Äôs the total due from Vendor X last quarter?‚Äô Instead of analysts searching PDFs for hours, the system gives an accurate, instant response.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "**Q17. What risks did you anticipate in this project?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * **Data privacy**: invoices contain sensitive info. Mitigation ‚Üí local vector DB, no third-party storage.\n",
    "  * **Hallucinations**: addressed with strict prompts + retrieval grounding.\n",
    "  * **Scalability**: initially used Chroma for PoC, with migration plan to Pinecone.\n",
    "\n",
    "---\n",
    "\n",
    "**Q18. How does this project align with Tiger Analytics‚Äô business focus?**\n",
    "\n",
    "* **A:** Tiger specializes in **AI-driven business transformation**. This project is a classic example: turning unstructured financial documents into **actionable insights**, reducing manual effort, and creating measurable ROI.\n",
    "\n",
    "---\n",
    "\n",
    "**Q19. If asked to deploy this for a global enterprise, what changes would you make?**\n",
    "\n",
    "* **A:**\n",
    "\n",
    "  * Multi-language OCR (Tesseract + multilingual embeddings).\n",
    "  * Enterprise-grade DB (Pinecone, Milvus) with auto-scaling.\n",
    "  * Role-based access control in FastAPI.\n",
    "  * Audit logs for compliance.\n",
    "\n",
    "---\n",
    "\n",
    "**Q20. What was the most challenging technical issue you faced?**\n",
    "\n",
    "* **A:** Extracting meaningful text from PDFs with inconsistent formatting (scanned vs. digital, different vendors). Solution ‚Üí integrated OCR fallback + normalized text preprocessing (regex for invoice numbers, amounts, dates).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c8f8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
