{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc35bc1",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ Interview-Style Q\\&A ‚Äì Payroll Gen AI Query Assistant\n",
    "\n",
    "**Q1. Can you describe the Payroll Gen AI Query Assistant project?**\n",
    "**A:**\n",
    "‚ÄúAt Globant, I designed an LLM-powered chatbot to handle payroll and HR queries in a context-aware manner. The system used Chroma DB for document retrieval, LangChain for building RAG pipelines, and LangSmith for observability. It automated payroll compliance explanations and reduced HR workload by around 38%. The backend was built in Python with FastAPI, making it scalable and easy to integrate into enterprise systems.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How does the architecture of this chatbot look?**\n",
    "**A:**\n",
    "‚ÄúThe flow was:\n",
    "\n",
    "1. User submits a payroll-related query via the chatbot interface.\n",
    "2. Query is embedded using OpenAI embeddings.\n",
    "3. Chroma DB retrieves top-k relevant payroll policies.\n",
    "4. The retrieved chunks are injected into the GPT-4 prompt through LangChain.\n",
    "5. The LLM generates a grounded, context-aware answer.\n",
    "6. The response is returned through the FastAPI endpoint, along with citations from the retrieved docs.\n",
    "\n",
    "LangSmith was integrated to trace each step, monitor latency, and measure hallucination rates.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you handle hallucinations in this system?**\n",
    "**A:**\n",
    "‚ÄúEven with correct retrieval, the LLM sometimes hallucinated. To mitigate this, I implemented layered strategies:\n",
    "\n",
    "* Validated retrieval parameters (chunk size, top-k).\n",
    "* Used strict prompting like *‚ÄòOnly answer from the provided context.‚Äô*\n",
    "* Lowered temperature and tuned top-p to reduce randomness.\n",
    "* Enforced citation-linked answers to keep responses grounded.\n",
    "\n",
    "These measures reduced hallucination rates by about 70%, which was critical for compliance-heavy domains like payroll.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What role did LangSmith play in this project?**\n",
    "**A:**\n",
    "‚ÄúLangSmith was central to observability. It helped trace the entire query lifecycle ‚Äî from embeddings to retrieval to final LLM output. With LangSmith dashboards, I could detect when irrelevant chunks were being retrieved, track token usage, and monitor latency. This observability allowed us to continuously fine-tune prompts and retrieval strategies.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. How did you scale this assistant for multiple concurrent HR queries?**\n",
    "**A:**\n",
    "‚ÄúI built the service on FastAPI with async endpoints to handle concurrent requests. Background tasks like bulk document ingestion were managed using Celery with Redis. The system was containerized with Docker and deployed with multiple Uvicorn workers behind a load balancer. For scalability, we integrated caching for common queries, which cut response times significantly during peak load.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. What business impact did this assistant deliver?**\n",
    "**A:**\n",
    "‚ÄúThe chatbot automated a significant portion of payroll compliance and HR-related query resolution. We measured that it reduced HR query tickets by about 38%, freeing HR teams to focus on higher-value work. It also improved employee experience by providing immediate, reliable answers instead of long wait times.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. If you were to enhance this assistant further, what would you add?**\n",
    "**A:**\n",
    "‚ÄúNext steps would be:\n",
    "\n",
    "* Integrating multimodal support to handle scanned payroll documents or attachments.\n",
    "* Adding role-based access control so managers, employees, and HR each see tailored responses.\n",
    "* Deploying hybrid search (BM25 + embeddings) in Chroma to improve retrieval quality.\n",
    "* Building continuous feedback loops where users can rate answers, feeding into retraining or fine-tuning.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. What challenges did you face in this project, and how did you overcome them?**\n",
    "**A:**\n",
    "‚ÄúOne major challenge was ensuring compliance answers were accurate, since incorrect advice could have legal implications. I solved this by grounding answers strictly in retrieved context and enforcing citation-linked responses. Another challenge was HR data being fragmented across multiple sources, so I built ETL pipelines to consolidate policies into Chroma DB for reliable retrieval.‚Äù\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc6aa2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
