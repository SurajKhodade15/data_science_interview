{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaef685",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ”¹ Interview-Style Q\\&A â€“ Web Data Summarization Project\n",
    "\n",
    "**Q1. Can you describe the Generative AIâ€“Powered Web Data Summarization project?**\n",
    "**A:**\n",
    "â€œAt Globant, I built a data pipeline that scraped energy proposal websites, extracted unstructured text, and applied Generative AI models to produce contextual summaries. The pipeline automated the extraction of key fields like proposal title, duration, budget, deadlines, and highlights. This transformed scattered website data into structured insights that business analysts could use directly, saving significant manual effort.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. What was the architecture of this pipeline?**\n",
    "**A:**\n",
    "â€œThe architecture was straightforward but modular:\n",
    "\n",
    "1. **Web scraping** with BeautifulSoup to collect proposal pages.\n",
    "2. **Text preprocessing** â€” cleaning HTML, removing boilerplate, and normalizing data.\n",
    "3. **Chunking** content for LLM processing.\n",
    "4. **Generative AI summarization** â€” OpenAI GPT took chunks and generated structured outputs with fields like title, budget, and deadline.\n",
    "5. **Storage** â€” structured data was stored in a database for querying and reporting.\n",
    "\n",
    "This end-to-end pipeline turned noisy, unstructured web pages into consistent, structured data.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you ensure reliable extraction from noisy web pages?**\n",
    "**A:**\n",
    "â€œI used BeautifulSoup with custom parsing logic to handle different page layouts. I also implemented regex patterns for common fields like currency and dates. To deal with inconsistent formats, I combined deterministic rules with GPT-based summarization â€” where rules failed, the model filled in gaps. This hybrid approach improved robustness.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. How did you structure the prompts for GPT summarization?**\n",
    "**A:**\n",
    "â€œI designed prompts that explicitly asked the LLM to extract structured JSON fields. For example:\n",
    "*â€˜Given the following proposal text, extract and return JSON with {title, budget, duration, deadline, highlights}.*â€™\n",
    "By enforcing schema-based outputs, we minimized hallucinations and made the summaries machine-readable for downstream analytics.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What challenges did you face, and how did you overcome them?**\n",
    "**A:**\n",
    "â€œOne challenge was inconsistent website formatting. Some sites had well-structured HTML, others were messy. I solved this with a hybrid approach: regex + rule-based parsing for obvious fields, and GPT summarization for ambiguous sections. Another challenge was cost optimization â€” I minimized tokens by chunking content intelligently and preprocessing text before sending to GPT.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. What business impact did this project deliver?**\n",
    "**A:**\n",
    "â€œThis pipeline reduced manual effort in analyzing proposals by automating data extraction and summarization. Analysts no longer had to manually read through entire websites. Instead, they could work with structured summaries, which accelerated decision-making. This not only saved hours of manual effort but also improved coverage of proposals being tracked.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. If you were to extend this solution, what would you add?**\n",
    "**A:**\n",
    "â€œFuture extensions could include:\n",
    "\n",
    "* Integrating with vector databases to enable semantic search across proposals.\n",
    "* Adding OCR for PDF-based proposals.\n",
    "* Deploying as a FastAPI microservice for real-time data extraction.\n",
    "* Using LangChain agents to orchestrate scraping, summarization, and validation workflows automatically.â€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c39cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6524f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_summarizer.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set your API key (export OPENAI_API_KEY in your environment)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def scrape_website(url: str) -> str:\n",
    "    \"\"\"Scrape and clean webpage text.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Remove scripts, styles, and nav elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = \" \".join(soup.get_text().split())\n",
    "    return text\n",
    "\n",
    "def summarize_with_gpt(text: str) -> dict:\n",
    "    \"\"\"Summarize scraped content into structured JSON fields.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following fields from the proposal text and return as JSON:\n",
    "    - Title\n",
    "    - Budget\n",
    "    - Duration\n",
    "    - Deadline\n",
    "    - Key Highlights\n",
    "\n",
    "    Text:\n",
    "    {text[:2000]}  # truncate to first 2000 chars for demo\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://example.com/proposal-page\"  # Replace with an actual proposal site\n",
    "    print(f\"Scraping: {url}\")\n",
    "\n",
    "    raw_text = scrape_website(url)\n",
    "    print(\"\\n--- Raw Extract (truncated) ---\")\n",
    "    print(raw_text[:500], \"...\")\n",
    "\n",
    "    summary = summarize_with_gpt(raw_text)\n",
    "    print(\"\\n--- Structured Summary ---\")\n",
    "    print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
