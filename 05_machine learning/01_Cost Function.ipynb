{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2b6705",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Œ Core Loss Functions in ML\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Mean Squared Error (MSE)**\n",
    "\n",
    "* **Definition:** Average of squared differences between actual and predicted values.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "* **Intuition:** Large errors penalized more (quadratic effect).\n",
    "* **Use Case:** Regression (Linear Regression, baseline models).\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print(\"MSE:\", mse)\n",
    "```\n",
    "\n",
    "âœ… Output: `MSE = 0.375`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Root Mean Squared Error (RMSE)**\n",
    "\n",
    "* **Definition:** Square root of MSE, giving error in same units as target.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "* **Intuition:** Easier to interpret (same scale as y).\n",
    "* **Use Case:** Regression evaluation metrics (house price prediction).\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "```\n",
    "\n",
    "âœ… Output: `RMSE = 0.612`\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mean Absolute Error (MAE)**\n",
    "\n",
    "* **Definition:** Average of absolute differences between actual and predicted values.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "* **Intuition:** Linear penalty â†’ robust to outliers.\n",
    "* **Use Case:** Regression when robustness is needed (finance, demand forecasting).\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "print(\"MAE:\", mae)\n",
    "```\n",
    "\n",
    "âœ… Output: `MAE = 0.5`\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Huber Loss**\n",
    "\n",
    "* **Definition:** Hybrid of MSE & MAE. Quadratic for small errors, linear for large ones.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "L_\\delta(y,\\hat{y}) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2}(y-\\hat{y})^2 & \\text{if } |y-\\hat{y}| \\leq \\delta \\\\\n",
    "\\delta |y-\\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* **Intuition:** Robust to outliers but smoother than MAE.\n",
    "* **Use Case:** Robust regression, computer vision.\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "delta = 1.0\n",
    "errors = y_true - y_pred\n",
    "huber_loss = np.where(np.abs(errors) <= delta,\n",
    "                      0.5 * errors**2,\n",
    "                      delta * np.abs(errors) - 0.5 * delta**2).mean()\n",
    "print(\"Huber Loss:\", huber_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Binary Cross-Entropy (Log Loss)**\n",
    "\n",
    "* **Definition:** Loss for binary classification.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{n}\\sum_{i=1}^n \\big[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\big]\n",
    "$$\n",
    "\n",
    "* **Intuition:** Penalizes confident wrong predictions.\n",
    "* **Use Case:** Logistic Regression, Neural Networks (binary classification).\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "y_true = [1, 0, 1, 1]\n",
    "y_prob = [0.9, 0.1, 0.8, 0.6]\n",
    "\n",
    "print(\"Binary Cross-Entropy:\", log_loss(y_true, y_prob))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Categorical Cross-Entropy**\n",
    "\n",
    "* **Definition:** Extension of log loss for multi-class classification.\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^n \\sum_{c=1}^C y_{i,c}\\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "\n",
    "* **Intuition:** Compares one-hot encoded true labels vs predicted probabilities.\n",
    "* **Use Case:** Softmax classifiers, deep learning models.\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "y_true = [[1,0,0], [0,1,0], [0,0,1]]\n",
    "y_pred = [[0.7,0.2,0.1], [0.1,0.8,0.1], [0.2,0.2,0.6]]\n",
    "\n",
    "print(\"Categorical Cross-Entropy:\", log_loss(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Sparse Categorical Cross-Entropy**\n",
    "\n",
    "* **Definition:** Variant of categorical cross-entropy when labels are integers (not one-hot).\n",
    "* **Intuition:** Saves memory for large class counts (e.g., NLP vocabularies).\n",
    "* **Use Case:** Multi-class classification with large vocab (text, NLP).\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "# true labels as class indices\n",
    "y_true = [0, 1, 2]\n",
    "y_pred = [[0.7,0.2,0.1], [0.1,0.8,0.1], [0.2,0.2,0.6]]\n",
    "\n",
    "print(\"Sparse Categorical Cross-Entropy:\", log_loss(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Hinge Loss (SVM)**\n",
    "\n",
    "* **Definition:** Loss used in Support Vector Machines (SVMs).\n",
    "* **Formula:**\n",
    "\n",
    "$$\n",
    "L = \\max(0, 1 - y_i \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "* **Intuition:** Encourages a margin of at least 1 between classes.\n",
    "* **Use Case:** SVM classification, max-margin classifiers.\n",
    "\n",
    "ðŸ‘‰ **Python Example:**\n",
    "\n",
    "```python\n",
    "y_true = np.array([1, -1, 1])\n",
    "y_pred = np.array([0.8, -0.5, 0.3])  # decision function scores\n",
    "\n",
    "hinge_loss = np.mean(np.maximum(0, 1 - y_true * y_pred))\n",
    "print(\"Hinge Loss:\", hinge_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âš¡ Interview Rapid Fire\n",
    "\n",
    "* **Which loss in regression?** â†’ MSE, MAE, Huber.\n",
    "* **Which loss in binary classification?** â†’ Binary Cross-Entropy.\n",
    "* **Which loss in multi-class classification?** â†’ Categorical / Sparse Categorical Cross-Entropy.\n",
    "* **Which loss in SVMs?** â†’ Hinge Loss.\n",
    "* **Which is robust to outliers?** â†’ MAE, Huber.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc1d3b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
