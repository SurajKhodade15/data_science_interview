{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71342490",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Š Confusion Matrix\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "A **confusion matrix** is a performance measurement tool for classification models.\n",
    "It compares the **actual labels** vs **predicted labels** to evaluate accuracy and errors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Structure (Binary Classification)**\n",
    "\n",
    "```\n",
    "                Predicted\n",
    "               0       1\n",
    "Actual  0     TN      FP\n",
    "        1     FN      TP\n",
    "```\n",
    "\n",
    "* **TP (True Positive):** Predicted 1, actually 1.\n",
    "* **TN (True Negative):** Predicted 0, actually 0.\n",
    "* **FP (False Positive):** Predicted 1, but actually 0. (Type I error)\n",
    "* **FN (False Negative):** Predicted 0, but actually 1. (Type II error)\n",
    "\n",
    "---\n",
    "\n",
    "### **Derived Metrics**\n",
    "\n",
    "All popular classification metrics come from the confusion matrix:\n",
    "\n",
    "1. **Accuracy**\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "2. **Precision**\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "â†’ \"Of predicted positives, how many were correct?\"\n",
    "\n",
    "3. **Recall (Sensitivity, TPR)**\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "â†’ \"Of actual positives, how many were captured?\"\n",
    "\n",
    "4. **Specificity (TNR)**\n",
    "\n",
    "$$\n",
    "Specificity = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "5. **F1-Score**\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Python Example**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_true = [0, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "```\n",
    "\n",
    "âœ… Example Output:\n",
    "\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[3 1]\n",
    " [1 3]]\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.75      0.75      0.75         4\n",
    "           1       0.75      0.75      0.75         4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Interview Questions**\n",
    "\n",
    "* **Q1:** What is the difference between precision and recall in the confusion matrix?\n",
    "  ðŸ‘‰ Precision is about correctness of positive predictions, recall is about capturing all actual positives.\n",
    "\n",
    "* **Q2:** If FP is high, what does it mean?\n",
    "  ðŸ‘‰ The model predicts positives incorrectly (false alarms). Example: predicting healthy patients as sick.\n",
    "\n",
    "* **Q3:** If FN is high, what does it mean?\n",
    "  ðŸ‘‰ The model misses actual positives (dangerous in medical diagnosis).\n",
    "\n",
    "* **Q4:** Which metrics would you prioritize in fraud detection?\n",
    "  ðŸ‘‰ Recall (donâ€™t miss fraud cases), F1-score (balance precision and recall).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b6c3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
