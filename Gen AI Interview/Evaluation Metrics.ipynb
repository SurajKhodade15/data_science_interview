{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d52825a",
   "metadata": {},
   "source": [
    "## Q: How do you evaluate generative text quality?**\n",
    "\n",
    "**A:**\n",
    "Evaluating generative text quality is multi-dimensional ‚Äî we care about **fluency, relevance, coherence, factuality, and style**. No single metric is perfect, so we usually combine **automatic metrics** with **human evaluation**.\n",
    "\n",
    "**1. Automatic Evaluation Metrics**\n",
    "\n",
    "* **BLEU (Bilingual Evaluation Understudy)**\n",
    "\n",
    "  * N-gram overlap between generated text and reference text.\n",
    "  * Good for translation, but limited in capturing meaning.\n",
    "\n",
    "* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "\n",
    "  * Measures overlap of n-grams, recall-focused.\n",
    "  * Popular in summarization tasks.\n",
    "\n",
    "* **METEOR**\n",
    "\n",
    "  * Considers synonyms, stemming, and word order.\n",
    "  * More semantically aware than BLEU/ROUGE.\n",
    "\n",
    "* **BERTScore**\n",
    "\n",
    "  * Uses contextual embeddings (from BERT/Transformer models) to compute semantic similarity.\n",
    "  * Better at capturing meaning than surface-level overlaps.\n",
    "\n",
    "* **Other task-specific metrics**\n",
    "\n",
    "  * Factuality checks (e.g., grounding score in RAG).\n",
    "  * Toxicity/safety metrics.\n",
    "\n",
    "**2. Human Evaluation**\n",
    "\n",
    "* Still considered **gold standard**.\n",
    "* Judges fluency, coherence, factual accuracy, and helpfulness.\n",
    "* Typically rated on Likert scales (e.g., 1‚Äì5) or via pairwise preference.\n",
    "* Expensive and subjective, but captures nuance missed by automatic scores.\n",
    "\n",
    "**3. Hybrid Approaches**\n",
    "\n",
    "* Combine **automatic metrics for scale** and **human eval for depth**.\n",
    "* Example: Use ROUGE/BERTScore for quick validation, then human eval for final deployment readiness.\n",
    "\n",
    "**Enterprise Impact:**\n",
    "For enterprises, automated metrics ensure **scalability and consistency**, while human evaluation ensures **trustworthiness and brand alignment** (critical in finance, healthcare, customer service).\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Quick check for you: if you were evaluating a **summarization model for legal documents**, which metric (BLEU, ROUGE, BERTScore, human eval) would you lean on most, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41728de5",
   "metadata": {},
   "source": [
    "## Q: How do you evaluate embeddings and retrieval quality?**\n",
    "\n",
    "**A:**\n",
    "We evaluate embeddings and retrieval quality using **IR-style ranking metrics**, which measure how well retrieved results align with ground-truth relevance.\n",
    "\n",
    "**1. Precision\\@k**\n",
    "\n",
    "* Fraction of retrieved documents in the top-*k* that are relevant.\n",
    "* Example: Precision\\@5 = 0.8 ‚Üí 4 out of 5 retrieved docs were relevant.\n",
    "* **Best for:** Ensuring high accuracy in the *top answers*.\n",
    "\n",
    "**2. Recall\\@k**\n",
    "\n",
    "* Fraction of all relevant documents that appear in the top-*k*.\n",
    "* Example: Recall\\@10 = 0.6 ‚Üí model retrieved 60% of all possible relevant docs in top 10.\n",
    "* **Best for:** Coverage (important in legal/medical search where missing facts is costly).\n",
    "\n",
    "**3. MRR (Mean Reciprocal Rank)**\n",
    "\n",
    "* Focuses on the position of the **first relevant document**.\n",
    "* Example: If the first relevant doc is at rank 2, reciprocal rank = 1/2.\n",
    "* **Best for:** Scenarios like QA/chatbots, where the first answer must be useful.\n",
    "\n",
    "**4. NDCG (Normalized Discounted Cumulative Gain)**\n",
    "\n",
    "* Weights relevance by position in ranking, giving more credit if highly relevant docs appear earlier.\n",
    "* Normalized for fair comparison across queries.\n",
    "* **Best for:** Multi-level relevance scoring (e.g., ‚Äúhighly relevant,‚Äù ‚Äúpartially relevant‚Äù).\n",
    "\n",
    "**Enterprise Impact:**\n",
    "\n",
    "* **Precision\\@k** ensures users don‚Äôt see junk results.\n",
    "* **Recall\\@k** ensures compliance and completeness (key in finance/legal).\n",
    "* **MRR/NDCG** optimize user experience by surfacing the most useful docs at the top.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1402564",
   "metadata": {},
   "source": [
    "## Q: What is perplexity, and why is it important in LLM evaluation?**\n",
    "\n",
    "**A:**\n",
    "\n",
    "* **Definition:**\n",
    "  Perplexity is a measure of how well a language model predicts a sequence of words. Formally, it‚Äôs the **exponential of the cross-entropy loss**.\n",
    "\n",
    "  $$\n",
    "  \\text{Perplexity} = e^{H(p,q)}\n",
    "  $$\n",
    "\n",
    "  where $H(p,q)$ is the cross-entropy between the true distribution $p$ and the model‚Äôs predicted distribution $q$.\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * Lower perplexity ‚Üí model assigns **higher probability** to the correct sequence ‚Üí better predictive ability.\n",
    "  * A perplexity of *k* means: ‚ÄúOn average, the model is as confused as if it had to choose among *k* equally likely options at each step.‚Äù\n",
    "\n",
    "* **Why it matters for LLM evaluation:**\n",
    "\n",
    "  1. **Model quality:** Indicates how well the LLM understands and predicts natural language.\n",
    "  2. **Comparisons:** Useful for comparing different models or checkpoints during training.\n",
    "  3. **Proxy for fluency:** Lower perplexity usually correlates with more fluent text.\n",
    "\n",
    "* **Limitations:**\n",
    "\n",
    "  * Doesn‚Äôt directly measure factuality, coherence, or task performance.\n",
    "  * A model with low perplexity might still **hallucinate** or generate unsafe text.\n",
    "  * Hence, perplexity is often used alongside task-specific metrics (BLEU, ROUGE, Recall\\@k, human eval).\n",
    "\n",
    "**Enterprise Impact:**\n",
    "Perplexity helps assess **efficiency of training and fine-tuning**, ensuring that enterprise LLMs are not overfitting and remain generalizable. However, in production, business stakeholders also care about **truthfulness, compliance, and user trust**, which require broader evaluation beyond perplexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab71295",
   "metadata": {},
   "source": [
    "## Q: How do you measure toxicity, bias, and fairness in generative outputs?**\n",
    "\n",
    "**A:**\n",
    "\n",
    "We evaluate these using a mix of **automatic detectors, fairness metrics, and human review**.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Toxicity Measurement**\n",
    "\n",
    "* **Automated classifiers:** Use models like **Perspective API**, Detoxify, or fine-tuned BERT to flag offensive, harmful, or unsafe language.\n",
    "* **Metrics:**\n",
    "\n",
    "  * Toxicity score (0‚Äì1 probability).\n",
    "  * % of generated outputs above a threshold.\n",
    "* **Human evaluation:** Spot-check edge cases, since automated classifiers may miss cultural nuances.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Bias Detection**\n",
    "\n",
    "* **Word embedding association tests (WEAT):** Check associations between demographic groups and attributes.\n",
    "* **Bias benchmarks:**\n",
    "\n",
    "  * StereoSet (gender, race, profession biases).\n",
    "  * CrowS-Pairs (causal bias).\n",
    "* **Prompt-based testing:** Generate outputs with identity-specific prompts (e.g., ‚ÄúA doctor is‚Ä¶‚Äù vs ‚ÄúA nurse is‚Ä¶‚Äù) and compare.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Fairness Evaluation**\n",
    "\n",
    "* **Group fairness metrics:**\n",
    "\n",
    "  * **Demographic parity:** Are positive/neutral outputs equally distributed across groups?\n",
    "  * **Equalized odds:** Do error rates differ across groups?\n",
    "* **Representation checks:** Ensure minority voices and perspectives are not underrepresented.\n",
    "* **Human review:** Panels from diverse backgrounds to judge fairness and inclusivity.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Enterprise Practices**\n",
    "\n",
    "* Combine **automatic red-teaming** (large-scale synthetic testing) with **manual red-teaming** (security/ethics experts probing system).\n",
    "* Maintain **auditable logs** of flagged toxic or biased generations.\n",
    "* Integrate **guardrails** (policy filters, safety layers) before outputs reach end-users.\n",
    "\n",
    "---\n",
    "\n",
    "**Enterprise Impact:**\n",
    "Managing toxicity, bias, and fairness ensures **regulatory compliance**, reduces **legal/brand risk**, and builds **user trust** ‚Äî especially critical in sensitive industries like finance, healthcare, and HR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6bf32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
