{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f40ae2",
   "metadata": {},
   "source": [
    "## Q: What is RAG, and why do we use it?**\n",
    "\n",
    "**A:**\n",
    "**Retrieval-Augmented Generation (RAG)** is an architecture that combines a **retrieval system** (like a vector database) with a **generative model** (like an LLM). Instead of relying only on the model‚Äôs internal knowledge, RAG retrieves **relevant, up-to-date, or domain-specific documents** and feeds them into the LLM as context before generating a response.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Retriever:** User query ‚Üí converted into embeddings ‚Üí searched in a vector database (e.g., Pinecone, FAISS, Weaviate, Milvus).\n",
    "2. **Generator:** Retrieved documents are appended to the prompt ‚Üí the LLM generates a grounded, context-aware response.\n",
    "\n",
    "**Why we use RAG:**\n",
    "\n",
    "* **Mitigates hallucination:** Model is grounded in external facts, reducing made-up answers.\n",
    "* **Domain adaptation without full fine-tuning:** Enterprises can add private knowledge (e.g., policies, product docs) without retraining the LLM.\n",
    "* **Keeps knowledge up to date:** RAG enables models to use **fresh data** that wasn‚Äôt present at training time.\n",
    "* **Efficient and cost-effective:** No need for expensive retraining of billions of parameters.\n",
    "\n",
    "üëâ **Enterprise Impact:**\n",
    "RAG is widely used in **enterprise AI assistants and copilots**. For example, a bank can deploy a chatbot that answers customer queries by retrieving from **internal compliance documents**, or a law firm can build a system that summarizes case law. This ensures **accurate, auditable, and trustworthy outputs** aligned with enterprise data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12724ede",
   "metadata": {},
   "source": [
    "## Q: Explain the RAG architecture: Retriever + Generator.**\n",
    "\n",
    "**A:**\n",
    "The **RAG architecture** has two main components:\n",
    "\n",
    "1. **Retriever**\n",
    "\n",
    "   * The retriever‚Äôs job is to fetch **relevant context** from an external knowledge source.\n",
    "   * A user query is first converted into an **embedding vector** using an embedding model (e.g., OpenAI embeddings, sentence transformers).\n",
    "   * This embedding is then used to search a **vector database** (like Pinecone, FAISS, Weaviate, Milvus) that stores embeddings of enterprise documents.\n",
    "   * The retriever returns the **top-k most relevant chunks** of text.\n",
    "\n",
    "2. **Generator**\n",
    "\n",
    "   * The retrieved documents are appended to the **original user query** and provided to a generative model (e.g., GPT, LLaMA, Mistral).\n",
    "   * The LLM uses both the **retrieved context + its pretrained knowledge** to generate a response.\n",
    "   * This ensures the answer is **grounded in real data**, reducing hallucinations.\n",
    "\n",
    "**Flow Summary:**\n",
    "\n",
    "* Input Query ‚Üí Embed ‚Üí Search in Vector DB ‚Üí Retrieve Relevant Chunks ‚Üí Pass Query + Chunks ‚Üí LLM Generates Grounded Response.\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "* Keeps LLM answers **accurate and current** without retraining.\n",
    "* Reduces **hallucinations** by grounding outputs.\n",
    "* Allows easy **domain customization** with enterprise data.\n",
    "\n",
    "üëâ **Enterprise Impact:**\n",
    "In practice, this architecture powers **enterprise knowledge assistants**‚Äîfor example, a legal copilot that retrieves clauses from contracts, or a customer-support bot that answers based on product manuals. It enables organizations to **unlock value from internal data safely and cost-effectively**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da8432",
   "metadata": {},
   "source": [
    "## Q: How do you create embeddings and store them in a vector database (e.g., Pinecone, FAISS, Milvus)?**\n",
    "\n",
    "**A:**\n",
    "The process usually follows an **end-to-end pipeline**:\n",
    "\n",
    "1. **Data Ingestion**\n",
    "\n",
    "   * Collect unstructured data such as PDFs, HTML pages, knowledge base docs, or enterprise reports.\n",
    "   * Use ETL pipelines or document loaders (e.g., LangChain document loaders) to standardize into a clean text format.\n",
    "\n",
    "2. **Pre-processing**\n",
    "\n",
    "   * Clean the text (remove noise, HTML tags, formatting issues).\n",
    "   * Split documents into manageable chunks (e.g., 500‚Äì1000 tokens) with overlap to preserve context.\n",
    "   * Add metadata (source, author, timestamp, tags) for filtering later.\n",
    "\n",
    "3. **Embedding Generation**\n",
    "\n",
    "   * Pass each text chunk through a pre-trained embedding model (e.g., OpenAI‚Äôs `text-embedding-ada-002`, SentenceTransformers, or HuggingFace models).\n",
    "   * The output is a **dense vector** (e.g., 768‚Äì1536 dimensions) that represents semantic meaning.\n",
    "\n",
    "4. **Vector Database Storage**\n",
    "\n",
    "   * Store embeddings and metadata in a **vector database** (Pinecone, FAISS, Milvus, Weaviate).\n",
    "   * The DB indexes embeddings using approximate nearest neighbor (ANN) techniques like HNSW or IVF for efficient similarity search.\n",
    "   * Example (Pinecone): `upsert(vectors=[(id, embedding, metadata)])`.\n",
    "\n",
    "5. **Query Time (RAG usage)**\n",
    "\n",
    "   * A user query is embedded with the same embedding model.\n",
    "   * The vector DB retrieves top-K similar chunks.\n",
    "   * These retrieved chunks are injected into the LLM‚Äôs prompt to ground responses with enterprise knowledge.\n",
    "\n",
    "**Enterprise Impact:**\n",
    "This workflow makes enterprise LLMs **domain-aware** without retraining them. It enables scalable, cost-efficient, and compliant knowledge retrieval for customer support, legal, finance, and research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c936e",
   "metadata": {},
   "source": [
    "## Q: What are chunking strategies for documents in RAG pipelines?**\n",
    "\n",
    "**A:**\n",
    "Chunking is the process of breaking large documents into smaller, semantically meaningful pieces so they can be embedded, stored, and retrieved effectively. The choice of chunking strategy directly affects retrieval quality and LLM performance.\n",
    "\n",
    "**1. Fixed-size Chunking**\n",
    "\n",
    "* Split text into chunks of a fixed number of tokens (e.g., 500‚Äì1000 tokens) with some overlap (e.g., 100 tokens).\n",
    "* **Pros:** Simple, works across domains.\n",
    "* **Cons:** May split across semantic boundaries (e.g., cutting in the middle of a sentence).\n",
    "\n",
    "**2. Semantic/Paragraph-based Chunking**\n",
    "\n",
    "* Split along natural boundaries such as paragraphs, headings, or sections.\n",
    "* Often combined with max-token limits to avoid overly large chunks.\n",
    "* **Pros:** Preserves context better.\n",
    "* **Cons:** Chunk sizes can be inconsistent.\n",
    "\n",
    "**3. Recursive/Semantic Splitters (LangChain-style)**\n",
    "\n",
    "* Start with large sections (e.g., by heading) and recursively split further if they exceed token limits.\n",
    "* Uses semantic awareness (e.g., sentence boundaries).\n",
    "* **Pros:** Balances semantic meaning with size constraints.\n",
    "\n",
    "**4. Sliding Window (Overlapping Chunks)**\n",
    "\n",
    "* Each chunk overlaps with the next (e.g., 500 tokens with 100 overlap).\n",
    "* Ensures context continuity for LLMs.\n",
    "* **Pros:** Prevents loss of meaning at boundaries.\n",
    "* **Cons:** Increases storage and compute cost.\n",
    "\n",
    "**5. Domain-specific Chunking**\n",
    "\n",
    "* Tailored strategies based on domain:\n",
    "\n",
    "  * **Legal docs:** Chunk by clauses or sections.\n",
    "  * **Code repos:** Chunk by functions, classes, or modules.\n",
    "  * **Financial reports:** Chunk by tables, footnotes, and sections.\n",
    "\n",
    "**Enterprise Impact:**\n",
    "\n",
    "* Proper chunking improves **retrieval precision** and reduces hallucinations.\n",
    "* It also lowers **cost and latency**, since the LLM processes only relevant chunks instead of entire documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868dff33",
   "metadata": {},
   "source": [
    "## Q: What are the challenges in RAG: hallucination, retrieval quality, and latency ‚Äî and how do we address them?**\n",
    "\n",
    "**A:**\n",
    "\n",
    "1. **Hallucination (LLM fabricating answers)**\n",
    "\n",
    "   * **Cause:** Even with retrieved docs, LLMs may ‚Äúmake up‚Äù information if context is weak.\n",
    "   * **Mitigations:**\n",
    "\n",
    "     * Prompt engineering (e.g., ‚ÄúAnswer *only* using provided context.‚Äù)\n",
    "     * Attribution (force LLM to cite retrieved sources).\n",
    "     * Use **fact-checking layers** or **consistency checks** across multiple retrieved chunks.\n",
    "     * Fine-tune on domain QA pairs to improve grounding.\n",
    "\n",
    "2. **Retrieval Quality (irrelevant or incomplete chunks retrieved)**\n",
    "\n",
    "   * **Cause:** Poor embeddings, bad chunking, or noisy data.\n",
    "   * **Mitigations:**\n",
    "\n",
    "     * Better chunking strategies (semantic + overlap).\n",
    "     * Hybrid search: combine **vector similarity** with **keyword/BM25 search**.\n",
    "     * Reranking retrieved chunks with cross-encoder models (e.g., ColBERT, Cohere reranker).\n",
    "     * Metadata filters (date, source, category) to narrow scope.\n",
    "\n",
    "3. **Latency (slow responses at query time)**\n",
    "\n",
    "   * **Cause:** Embedding large queries, slow vector DB search, or retrieving too many chunks.\n",
    "   * **Mitigations:**\n",
    "\n",
    "     * Pre-compute embeddings during ingestion, not at runtime.\n",
    "     * Use optimized vector DBs (FAISS HNSW, Pinecone, Milvus).\n",
    "     * Cache frequent queries & embeddings.\n",
    "     * Balance **top-K retrieval** (not too few, not too many).\n",
    "     * Scale infra with GPUs/ANN indexes for large workloads.\n",
    "\n",
    "**Enterprise Impact:**\n",
    "By addressing these challenges, RAG pipelines become **trustworthy, responsive, and cost-efficient**, which is critical for enterprise adoption in domains like healthcare, legal, and finance where accuracy and latency directly affect user trust and compliance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695c769",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
