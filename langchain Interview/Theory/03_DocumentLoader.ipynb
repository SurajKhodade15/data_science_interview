{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6302bad0",
   "metadata": {},
   "source": [
    "\n",
    "## üß© **1. Conceptual Overview**\n",
    "\n",
    "### üîπ What Are Document Loaders?\n",
    "\n",
    "**Document Loaders** in LangChain are **input gateways** that allow you to **ingest data from diverse sources** ‚Äî such as PDFs, text files, HTML pages, databases, APIs, Google Docs, Notion, Slack, etc.\n",
    "\n",
    "Their job is to **extract raw content** and convert it into **LangChain‚Äôs standard `Document` format**, which can then be processed, split, embedded, and retrieved later by the LLM pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Document Loaders Matter\n",
    "\n",
    "In any enterprise-grade GenAI or RAG system:\n",
    "\n",
    "* **Data variety** is the norm ‚Äî not all sources are text files.\n",
    "* LLMs need **structured and cleaned** text, not unformatted raw data.\n",
    "* Consistency in document format enables efficient **chunking, embedding, and indexing**.\n",
    "\n",
    "Thus, loaders are the **first step** in the **data pipeline** of LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± **2. Architectural Role of Document Loaders**\n",
    "\n",
    "LangChain‚Äôs data pipeline can be visualized as:\n",
    "\n",
    "```\n",
    "Data Source\n",
    "   ‚Üì\n",
    "Document Loader (extracts raw data)\n",
    "   ‚Üì\n",
    "Text Splitter (chunks large docs)\n",
    "   ‚Üì\n",
    "Embeddings Model (vectorizes chunks)\n",
    "   ‚Üì\n",
    "VectorStore (stores for retrieval)\n",
    "   ‚Üì\n",
    "Retriever + LLM (query + generation)\n",
    "```\n",
    "\n",
    "The **Document Loader** is the entry node of this architecture.\n",
    "It ensures that all downstream components receive a **uniform document schema**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ **3. The `Document` Object**\n",
    "\n",
    "All loaders output a list of standardized **`Document` objects** with two attributes:\n",
    "\n",
    "```python\n",
    "{\n",
    "  page_content: str,      # The actual text\n",
    "  metadata: dict          # Source information (filename, URL, author, etc.)\n",
    "}\n",
    "```\n",
    "\n",
    "This ensures interoperability across the LangChain ecosystem.\n",
    "\n",
    "üìò *Example Document:*\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"LangChain is a framework for LLM-based apps...\",\n",
    "    metadata={\"source\": \"intro_to_langchain.pdf\", \"page\": 2}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **4. Built-in Document Loaders**\n",
    "\n",
    "LangChain provides **over 100+ built-in loaders**, covering nearly all enterprise data sources.\n",
    "\n",
    "Here‚Äôs a structured classification:\n",
    "\n",
    "| **Category**          | **Examples**                     | **Module**                                                                  |\n",
    "| --------------------- | -------------------------------- | --------------------------------------------------------------------------- |\n",
    "| **Text Files**        | `.txt`, `.md`, `.csv`            | `TextLoader`, `CSVLoader`                                                   |\n",
    "| **PDF Files**         | PDFs with text or scanned images | `PyPDFLoader`, `PDFMinerLoader`, `PDFPlumberLoader`                         |\n",
    "| **Office Docs**       | Word, Excel, PowerPoint          | `Docx2txtLoader`, `UnstructuredExcelLoader`, `UnstructuredPowerPointLoader` |\n",
    "| **Web Data**          | URLs, sitemaps, APIs             | `UnstructuredURLLoader`, `WebBaseLoader`, `SitemapLoader`                   |\n",
    "| **Email & Chat**      | Outlook, Gmail, Slack            | `OutlookLoader`, `SlackDirectoryLoader`                                     |\n",
    "| **Databases**         | SQL, MongoDB                     | `SQLDatabaseLoader`, `MongoDBLoader`                                        |\n",
    "| **Cloud Sources**     | Google Drive, Notion, Confluence | `GoogleDriveLoader`, `NotionDBLoader`, `ConfluenceLoader`                   |\n",
    "| **Code Repositories** | GitHub, local codebases          | `GitLoader`, `NotebookLoader`                                               |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **5. Core Loader Example**\n",
    "\n",
    "### **üìò Example: Loading a PDF**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/company_policy.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[0].page_content[:200])\n",
    "print(documents[0].metadata)\n",
    "```\n",
    "\n",
    "Each page becomes a separate `Document` object with metadata like page number and source.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìò Example: Loading Text and CSV Files**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader, CSVLoader\n",
    "\n",
    "# Load a plain text file\n",
    "text_docs = TextLoader(\"data/overview.txt\").load()\n",
    "\n",
    "# Load a CSV file\n",
    "csv_docs = CSVLoader(\"data/customers.csv\").load()\n",
    "```\n",
    "\n",
    "Each row of a CSV becomes an individual document with column metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìò Example: Loading from a Website**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://langchain.com\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])\n",
    "```\n",
    "\n",
    "The `WebBaseLoader` fetches and cleans webpage text (HTML ‚Üí readable text).\n",
    "\n",
    "---\n",
    "\n",
    "### **üìò Example: Loading from a Notion Database**\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import NotionDBLoader\n",
    "\n",
    "loader = NotionDBLoader(\n",
    "    integration_token=\"your_notion_api_token\",\n",
    "    database_id=\"your_database_id\"\n",
    ")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "Ideal for **corporate knowledge management ingestion**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **6. Custom Document Loaders**\n",
    "\n",
    "In enterprise environments, data often resides in **custom APIs or internal systems**.\n",
    "LangChain allows you to **create your own loader** by subclassing `BaseLoader`.\n",
    "\n",
    "### **Custom Loader Example**\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import BaseLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "class APIDataLoader(BaseLoader):\n",
    "    def __init__(self, endpoint):\n",
    "        self.endpoint = endpoint\n",
    "\n",
    "    def load(self):\n",
    "        # Simulate API call\n",
    "        data = [{\"title\": \"LangChain\", \"desc\": \"Framework for LLMs\"}]\n",
    "        return [Document(page_content=item[\"desc\"], metadata={\"title\": item[\"title\"]}) for item in data]\n",
    "\n",
    "loader = APIDataLoader(\"https://api.example.com/data\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "This approach allows full integration with internal microservices, REST APIs, or proprietary databases.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **7. Integration with Other Components**\n",
    "\n",
    "Once loaded, the data flows through **Text Splitters** and **Embeddings** before being indexed.\n",
    "\n",
    "Example End-to-End Pipeline:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Load\n",
    "loader = PyPDFLoader(\"data/handbook.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Embed\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Step 4: Query\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **8. Popular Enterprise Loaders**\n",
    "\n",
    "| **Source**   | **Loader**                       | **Notes**                                |\n",
    "| ------------ | -------------------------------- | ---------------------------------------- |\n",
    "| PDF          | `PyPDFLoader`                    | Fast, accurate text extraction           |\n",
    "| HTML/Web     | `WebBaseLoader`                  | Cleans tags & preserves readable content |\n",
    "| Word         | `UnstructuredWordDocumentLoader` | Retains section hierarchy                |\n",
    "| Slack        | `SlackDirectoryLoader`           | Integrates enterprise chat archives      |\n",
    "| Google Drive | `GoogleDriveLoader`              | OAuth-based access                       |\n",
    "| Confluence   | `ConfluenceLoader`               | Atlassian enterprise-ready               |\n",
    "| JSON         | `JSONLoader`                     | Customizable schema mapping              |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **9. Best Practices**\n",
    "\n",
    "1. **Normalize metadata** ‚Äì Ensure consistent fields (e.g., ‚Äúsource‚Äù, ‚Äútype‚Äù, ‚Äúauthor‚Äù).\n",
    "2. **Chunk after loading** ‚Äì Load full files, then split logically.\n",
    "3. **De-duplicate content** ‚Äì Avoid redundant embeddings.\n",
    "4. **Log load sources** ‚Äì Helps trace responses in RAG pipelines.\n",
    "5. **Monitor loader latency** ‚Äì Especially for API-based or large PDFs.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **10. Common Challenges**\n",
    "\n",
    "| **Issue**             | **Root Cause**                 | **Mitigation**                                 |\n",
    "| --------------------- | ------------------------------ | ---------------------------------------------- |\n",
    "| Missing text in PDFs  | Scanned images, not text-based | Use OCR loader (e.g., `UnstructuredPDFLoader`) |\n",
    "| API rate limits       | External source throttling     | Implement retry + caching                      |\n",
    "| Encoding issues       | Non-UTF8 formats               | Convert to UTF-8 before load                   |\n",
    "| Inconsistent metadata | Different loader formats       | Apply standard metadata mapping                |\n",
    "\n",
    "---\n",
    "\n",
    "## üíº **11. Interview Q&A**\n",
    "\n",
    "### **Beginner**\n",
    "\n",
    "**Q1. What is a Document Loader in LangChain?**\n",
    "It‚Äôs a component used to ingest and standardize data from various sources into LangChain‚Äôs `Document` format.\n",
    "\n",
    "**Q2. What are the key attributes of a `Document` object?**\n",
    "`page_content` and `metadata`.\n",
    "\n",
    "**Q3. How does LangChain handle different file formats?**\n",
    "Through specialized loaders like `TextLoader`, `PyPDFLoader`, `CSVLoader`, and `UnstructuredLoader`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q4. How do Document Loaders fit into a RAG pipeline?**\n",
    "They are the first step ‚Äî extracting content before splitting, embedding, and storing in a vector database.\n",
    "\n",
    "**Q5. What‚Äôs the difference between `PyPDFLoader` and `UnstructuredPDFLoader`?**\n",
    "`PyPDFLoader` extracts digital text; `UnstructuredPDFLoader` uses OCR for scanned or complex layouts.\n",
    "\n",
    "**Q6. How would you handle loading from an internal company API?**\n",
    "By building a **custom loader** extending `BaseLoader` and returning standardized `Document` objects.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q7. How can you optimize document loading for large-scale ingestion (e.g., 10K PDFs)?**\n",
    "\n",
    "* Use asynchronous I/O (`aiofiles`, `asyncio`)\n",
    "* Batch embeddings\n",
    "* Parallelize loading using `ThreadPoolExecutor`\n",
    "* Persist intermediate chunks\n",
    "\n",
    "**Q8. How does metadata support traceability in RAG systems?**\n",
    "Metadata links answers back to sources, enabling transparency and auditability ‚Äî essential for enterprise compliance.\n",
    "\n",
    "**Q9. Describe a failure scenario in document ingestion and its mitigation.**\n",
    "A corrupt PDF causes the pipeline to fail ‚Äî implement exception handling + fallback loaders (OCR-based).\n",
    "\n",
    "**Q10. What‚Äôs your approach to document deduplication before embedding?**\n",
    "Hash `page_content`, maintain a hash index, and skip duplicates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560eff8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
