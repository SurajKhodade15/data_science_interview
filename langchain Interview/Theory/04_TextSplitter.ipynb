{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831f44c8",
   "metadata": {},
   "source": [
    "\n",
    "# üß© 1. Conceptual Overview\n",
    "\n",
    "### üîπ What Is a Text Splitter?\n",
    "\n",
    "A **Text Splitter** divides large documents (loaded via Document Loaders) into smaller, manageable **chunks of text** that can be efficiently embedded, stored, and retrieved later.\n",
    "\n",
    "LLMs have token limits (e.g., GPT-4 Turbo: ~128K tokens). Without chunking, you risk:\n",
    "\n",
    "* Exceeding model limits\n",
    "* Losing semantic boundaries\n",
    "* Degraded retrieval accuracy\n",
    "\n",
    "Splitters solve this by applying **intelligent segmentation** ‚Äî usually at sentence, paragraph, or token level.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Splitting Matters\n",
    "\n",
    "Splitting determines:\n",
    "\n",
    "* How much **context** is stored in each vector.\n",
    "* How **relevant** retrieved content will be.\n",
    "* The **embedding quality** and **retrieval precision**.\n",
    "\n",
    "A good splitter balances **semantic coherence** with **token efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß± 2. Architectural Role\n",
    "\n",
    "LangChain‚Äôs ingestion pipeline:\n",
    "\n",
    "```\n",
    "Raw Data ‚Üí Document Loader ‚Üí Text Splitter ‚Üí Embeddings ‚Üí VectorStore ‚Üí Retriever ‚Üí LLM\n",
    "```\n",
    "\n",
    "The **Text Splitter** ensures that:\n",
    "\n",
    "* Each chunk fits within model/token constraints.\n",
    "* Metadata is propagated and preserved.\n",
    "* Context overlap avoids boundary loss.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 3. Core Splitter Classes\n",
    "\n",
    "LangChain offers multiple **splitter strategies**, each suited for different content types.\n",
    "\n",
    "| **Splitter**                            | **Purpose / Logic**                                | **Best For**             |\n",
    "| --------------------------------------- | -------------------------------------------------- | ------------------------ |\n",
    "| `CharacterTextSplitter`                 | Splits by character count                          | Plain text               |\n",
    "| `RecursiveCharacterTextSplitter`        | Hierarchical split: paragraphs ‚Üí sentences ‚Üí words | Structured documents     |\n",
    "| `TokenTextSplitter`                     | Splits by token count                              | LLM-token optimized      |\n",
    "| `MarkdownHeaderTextSplitter`            | Splits markdown by headers                         | Notebooks, documentation |\n",
    "| `SentenceTransformersTokenTextSplitter` | Token-aware, using model tokenizer                 | Fine-grained RAG         |\n",
    "| `Language` Splitters                    | Language-specific chunking (Python, JS, SQL)       | Code data                |\n",
    "| `HTMLHeaderTextSplitter`                | Based on HTML tags                                 | Web documents            |\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 4. Key Parameters\n",
    "\n",
    "| **Parameter**     | **Description**                                                   |\n",
    "| ----------------- | ----------------------------------------------------------------- |\n",
    "| `chunk_size`      | Maximum size of a chunk (in characters or tokens).                |\n",
    "| `chunk_overlap`   | Number of overlapping units between chunks (to preserve context). |\n",
    "| `separators`      | Custom delimiters (`\\n`, `.`, `;`) controlling boundary logic.    |\n",
    "| `length_function` | Defines how ‚Äúlength‚Äù is calculated (characters, tokens).          |\n",
    "\n",
    "---\n",
    "\n",
    "# üìò 5. Practical Example ‚Äî Recursive Splitter\n",
    "\n",
    "### Example: Splitting a long PDF document\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "loader = PyPDFLoader(\"data/policy.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Original Docs: {len(docs)} | Split Chunks: {len(chunks)}\")\n",
    "print(chunks[0].page_content[:300])\n",
    "```\n",
    "\n",
    "üîπ *Why Recursive?*\n",
    "It tries to split text at natural breakpoints (paragraph ‚Üí line ‚Üí sentence ‚Üí word), ensuring coherence.\n",
    "\n",
    "---\n",
    "\n",
    "# üìò 6. Example ‚Äî Token-Based Splitting\n",
    "\n",
    "Useful when targeting **specific token budgets** aligned with embedding or generation models.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "tokens = splitter.split_text(\"Your long technical document here...\")\n",
    "print(len(tokens))\n",
    "```\n",
    "\n",
    "This ensures consistent chunking aligned with **LLM tokenization rules** (using `tiktoken` or tokenizer-specific methods).\n",
    "\n",
    "---\n",
    "\n",
    "# üìò 7. Example ‚Äî Markdown & Structured Documents\n",
    "\n",
    "Ideal for engineering documentation, README files, or Jupyter notebooks.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    ")\n",
    "docs = splitter.split_text(open(\"docs/readme.md\").read())\n",
    "print(len(docs))\n",
    "```\n",
    "\n",
    "Preserves section hierarchy in metadata:\n",
    "\n",
    "```python\n",
    "{'Header 1': 'Introduction', 'Header 2': 'Setup', 'Header 3': 'Usage'}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß© 8. Example ‚Äî Code Splitter\n",
    "\n",
    "LangChain supports **language-aware splitters** for codebases:\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=400, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "code = open(\"scripts/model_training.py\").read()\n",
    "chunks = splitter.create_documents([code])\n",
    "print(chunks[0].page_content)\n",
    "```\n",
    "\n",
    "This ensures logical splits at class/function boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "# üîÑ 9. Chunking Strategy Design (Best Practices)\n",
    "\n",
    "| **Objective**              | **Recommended Approach**                                    |\n",
    "| -------------------------- | ----------------------------------------------------------- |\n",
    "| **General knowledge base** | RecursiveCharacterTextSplitter, 800‚Äì1000 chars, 150 overlap |\n",
    "| **Technical docs**         | MarkdownHeaderTextSplitter or TokenTextSplitter             |\n",
    "| **Legal/Policy documents** | Recursive splitter with paragraph separators                |\n",
    "| **Code repositories**      | Language-aware splitter                                     |\n",
    "| **Chat history**           | Character splitter, smaller chunks (400‚Äì600 chars)          |\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öñÔ∏è 10. Choosing the Right Chunk Size\n",
    "\n",
    "### üîπ Considerations:\n",
    "\n",
    "* Embedding model‚Äôs **token capacity** (e.g., `text-embedding-3-large` ‚Üí ~8K tokens)\n",
    "* Context requirement per query\n",
    "* Vector DB efficiency\n",
    "\n",
    "| Model                              | Recommended Chunk Size | Overlap |\n",
    "| ---------------------------------- | ---------------------- | ------- |\n",
    "| `OpenAI text-embedding-3-small`    | 600‚Äì800                | 100     |\n",
    "| `text-embedding-3-large`           | 1000‚Äì1500              | 200     |\n",
    "| `sentence-transformers`            | 300‚Äì500                | 50‚Äì100  |\n",
    "| `gpt-4-turbo` (generation context) | ‚â§1500                  | 200     |\n",
    "\n",
    "---\n",
    "\n",
    "# üßÆ 11. Metadata Propagation\n",
    "\n",
    "All splitters preserve the metadata of the original document, ensuring traceability.\n",
    "\n",
    "üìò Example:\n",
    "\n",
    "```python\n",
    "print(chunks[0].metadata)\n",
    "# Output: {'source': 'data/policy.pdf', 'page': 2}\n",
    "```\n",
    "\n",
    "You can also append or enrich metadata at the chunk level for advanced retrieval analytics.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 12. Common Pitfalls\n",
    "\n",
    "| **Issue**                         | **Cause**                          | **Mitigation**                  |\n",
    "| --------------------------------- | ---------------------------------- | ------------------------------- |\n",
    "| Chunks too large ‚Üí Token overflow | Chunk size > embedding/model limit | Reduce `chunk_size`             |\n",
    "| Context loss between chunks       | No overlap                         | Use `chunk_overlap=100‚Äì200`     |\n",
    "| Poor semantic alignment           | Simple character splitting         | Use Recursive or Token splitter |\n",
    "| High memory use                   | Large doc ingestion                | Process in batches              |\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 13. Integration with VectorStores\n",
    "\n",
    "Example full pipeline:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load & split\n",
    "loader = TextLoader(\"data/ai_overview.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embed & store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "```\n",
    "\n",
    "Now, each chunk is semantically indexed for high-precision retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "# üíº 14. Interview Questions & Answers\n",
    "\n",
    "### **Beginner**\n",
    "\n",
    "**Q1. What is the purpose of a Text Splitter in LangChain?**\n",
    "To break large documents into smaller, coherent text chunks for efficient embedding and retrieval.\n",
    "\n",
    "**Q2. Why is chunk overlap important?**\n",
    "To maintain context continuity across adjacent chunks, ensuring smoother comprehension during retrieval.\n",
    "\n",
    "**Q3. What are the main types of text splitters?**\n",
    "Character-based, Recursive, Token-based, Markdown-based, and Language-aware splitters.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q4. Difference between `CharacterTextSplitter` and `RecursiveCharacterTextSplitter`?**\n",
    "\n",
    "* `CharacterTextSplitter` uses fixed-size segmentation.\n",
    "* `RecursiveCharacterTextSplitter` respects logical structure ‚Äî paragraphs, sentences, words.\n",
    "\n",
    "**Q5. How would you choose chunk size for OpenAI embeddings?**\n",
    "Typically 800‚Äì1200 characters with 100‚Äì200 overlap to balance token efficiency and semantic completeness.\n",
    "\n",
    "**Q6. What metadata is preserved during splitting?**\n",
    "File name, page numbers, headers, or source path from the original loader.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q7. How would you handle multilingual documents?**\n",
    "Use a multilingual-aware splitter or sentence boundary detection models (`spacy`, `langdetect`) before chunking.\n",
    "\n",
    "**Q8. What‚Äôs the trade-off between small and large chunks?**\n",
    "\n",
    "* Small chunks ‚Üí High retrieval precision but lower coherence.\n",
    "* Large chunks ‚Üí Better context but risk token inefficiency.\n",
    "\n",
    "**Q9. How can splitting be optimized for RAG latency?**\n",
    "\n",
    "* Preprocess & cache chunks\n",
    "* Parallelize splitting\n",
    "* Optimize chunk size for embedding vector dimensionality\n",
    "\n",
    "**Q10. How would you split a 100MB document for RAG?**\n",
    "Load in streaming batches ‚Üí split incrementally ‚Üí persist chunks asynchronously to a vector store.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 15. Real-World Implementation Pattern\n",
    "\n",
    "| **Stage**         | **Component**                 | **Description**           |\n",
    "| ----------------- | ----------------------------- | ------------------------- |\n",
    "| 1Ô∏è‚É£ Data Load     | Document Loaders              | Extract raw content       |\n",
    "| 2Ô∏è‚É£ Data Prep     | Text Splitters                | Segment into chunks       |\n",
    "| 3Ô∏è‚É£ Vectorization | Embeddings                    | Generate semantic vectors |\n",
    "| 4Ô∏è‚É£ Storage       | VectorStore (FAISS, Pinecone) | Store vectors + metadata  |\n",
    "| 5Ô∏è‚É£ Retrieval     | Retriever + Chain             | Query + generate response |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3493b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
