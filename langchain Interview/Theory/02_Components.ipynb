{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb84784",
   "metadata": {},
   "source": [
    "\n",
    "## üß± **1. Overview: The Modular Architecture of LangChain**\n",
    "\n",
    "LangChain is organized around **modular components** that work independently yet integrate seamlessly into an LLM-powered pipeline.\n",
    "\n",
    "The **key principle** behind this modular design:\n",
    "\n",
    "> *Each component solves a specific problem in the lifecycle of LLM application development ‚Äî from data ingestion to reasoning and action.*\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Core Components**\n",
    "\n",
    "LangChain has **six foundational components**, often summarized as:\n",
    "\n",
    "> **Model ‚Üí Prompt ‚Üí Chain ‚Üí Memory ‚Üí Agent ‚Üí Tool**\n",
    "\n",
    "Each serves a distinct role in the orchestration pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **1. Models**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Models are the **brains** ‚Äî they generate or embed text.\n",
    "LangChain abstracts interaction with multiple model providers under a common interface.\n",
    "\n",
    "### üß† Types:\n",
    "\n",
    "* **LLMs** ‚Äî Standard text generators (e.g., GPT-4, Claude, Gemini, Mistral).\n",
    "* **ChatModels** ‚Äî Dialogue-optimized models using role-based messaging.\n",
    "* **Embeddings** ‚Äî Convert text to numerical vectors for similarity search.\n",
    "\n",
    "### üß∞ Common Classes:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.3)\n",
    "response = model.invoke(\"Explain LangChain architecture\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Prompts**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Prompts define **how** inputs are formatted and presented to the model.\n",
    "LangChain formalizes this via reusable templates that handle input variables dynamically.\n",
    "\n",
    "### üß† Key Classes:\n",
    "\n",
    "* **PromptTemplate** ‚Äì Text templates for LLMs.\n",
    "* **ChatPromptTemplate** ‚Äì Multi-role templates (system, user, assistant).\n",
    "* **FewShotPromptTemplate** ‚Äì Adds in-context examples.\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the importance of {topic} in AI.\"\n",
    ")\n",
    "prompt = template.format(topic=\"LangChain\")\n",
    "```\n",
    "\n",
    "Prompts can be layered with **Memory**, **Chains**, and **Retrievers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **3. Memory**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Memory stores **contextual state** ‚Äî making interactions with LLMs *stateful*.\n",
    "Without memory, each LLM call is stateless and unaware of prior exchanges.\n",
    "\n",
    "### üß† Common Memory Classes:\n",
    "\n",
    "| Type                           | Description                                    |\n",
    "| ------------------------------ | ---------------------------------------------- |\n",
    "| **ConversationBufferMemory**   | Stores full conversation history.              |\n",
    "| **ConversationSummaryMemory**  | Summarizes past messages for efficiency.       |\n",
    "| **VectorStoreRetrieverMemory** | Retrieves relevant past info using embeddings. |\n",
    "| **EntityMemory**               | Tracks specific entities across conversations. |\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Hello, how can I help?\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **4. Chains**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Chains are **sequences of steps** that connect multiple components ‚Äî LLMs, prompts, memory, and tools ‚Äî into a cohesive logic pipeline.\n",
    "\n",
    "### üß† Types of Chains:\n",
    "\n",
    "| Chain                     | Description                              |\n",
    "| ------------------------- | ---------------------------------------- |\n",
    "| **LLMChain**              | Single LLM + Prompt interaction          |\n",
    "| **SequentialChain**       | Multi-step pipeline (output ‚Üí input)     |\n",
    "| **SimpleSequentialChain** | Simplified chaining                      |\n",
    "| **Custom Chains**         | User-defined with complex logic          |\n",
    "| **RouterChain**           | Route to different subchains dynamically |\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=template)\n",
    "response = chain.run(\"LangChain components\")\n",
    "```\n",
    "\n",
    "Chains can include **memory**, **retrievers**, or **parsers** for output structuring.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **5. Agents**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Agents provide **reasoning + autonomy**.\n",
    "They allow the LLM to **decide dynamically** which tool or action to execute based on user intent.\n",
    "\n",
    "### üß† Key Elements:\n",
    "\n",
    "* **Agent** ‚Äì The decision-making controller.\n",
    "* **Tools** ‚Äì Functions or APIs an agent can call.\n",
    "* **AgentExecutor** ‚Äì Executes reasoning-action loops.\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=model)\n",
    "agent = initialize_agent(tools, model, agent_type=\"zero-shot-react-description\")\n",
    "agent.run(\"What is the square root of 256?\")\n",
    "```\n",
    "\n",
    "Agents use the **ReAct framework** (Reason + Act) ‚Äî model decides ‚Üí executes ‚Üí reflects ‚Üí outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **6. Tools**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Tools are **external capabilities** the LLM can invoke to perform real actions ‚Äî search the web, run code, query a DB, or retrieve data.\n",
    "\n",
    "### üß† Examples:\n",
    "\n",
    "| Tool              | Function                  |\n",
    "| ----------------- | ------------------------- |\n",
    "| `SerpAPI`         | Internet search           |\n",
    "| `Python REPL`     | Execute Python code       |\n",
    "| `SQLDatabaseTool` | Query relational DB       |\n",
    "| `RequestsGetTool` | Make API calls            |\n",
    "| Custom Tool       | Any user-defined function |\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.tools import Tool\n",
    "import datetime\n",
    "\n",
    "def get_date(_): \n",
    "    return f\"Today‚Äôs date is {datetime.date.today()}\"\n",
    "\n",
    "tool = Tool(name=\"DateTool\", func=get_date, description=\"Returns today's date\")\n",
    "```\n",
    "\n",
    "Tools expand an LLM‚Äôs **capability space** beyond text generation ‚Äî forming the backbone of agentic systems.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **7. Retrieval and VectorStores**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "For RAG (Retrieval-Augmented Generation), LangChain uses **retrievers** and **vector stores** to fetch external context.\n",
    "\n",
    "### üß† Workflow:\n",
    "\n",
    "Documents ‚Üí Embeddings ‚Üí VectorStore (FAISS, Pinecone, Chroma) ‚Üí Retriever ‚Üí Prompt injection ‚Üí LLM.\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=\"./db\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **8. Output Parsers**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Output parsers **structure the raw LLM response** ‚Äî converting unstructured text into JSON, lists, or domain objects.\n",
    "\n",
    "### üß† Common Parsers:\n",
    "\n",
    "| Parser                     | Usage                              |\n",
    "| -------------------------- | ---------------------------------- |\n",
    "| **RegexParser**            | Extracts specific patterns         |\n",
    "| **StructuredOutputParser** | Uses schema for structured output  |\n",
    "| **PydanticOutputParser**   | Converts text into Pydantic models |\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import RegexParser\n",
    "parser = RegexParser(regex=r\"(\\d+)\", output_keys=[\"number\"])\n",
    "parser.parse(\"The result is 42\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **9. LangChain Expression Language (LCEL)**\n",
    "\n",
    "### üí° Purpose:\n",
    "\n",
    "Introduced to simplify chain composition using a **declarative pipeline syntax** ‚Äî improving speed, readability, and parallelism.\n",
    "\n",
    "### ‚öôÔ∏è Example:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain {topic} in one line.\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"topic\": \"LangChain\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **10. Deployment & Monitoring**\n",
    "\n",
    "| Component     | Purpose                              |\n",
    "| ------------- | ------------------------------------ |\n",
    "| **LangServe** | Deploy chains/agents as REST APIs    |\n",
    "| **LangSmith** | Monitor, trace, and debug executions |\n",
    "\n",
    "---\n",
    "\n",
    "## üíº **11. Interview Questions & Answers**\n",
    "\n",
    "### **Beginner**\n",
    "\n",
    "**Q1. What are the main components of LangChain?**\n",
    "LLMs, Prompts, Chains, Memory, Agents, Tools, Retrievers, and Output Parsers.\n",
    "\n",
    "**Q2. What is the role of a Chain?**\n",
    "Chains connect components into a logical flow ‚Äî transforming inputs to outputs through one or multiple LLM calls.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q3. How do Agents differ from Chains?**\n",
    "Agents make **dynamic decisions** about which tool or sub-chain to invoke, while Chains are **static** and pre-defined.\n",
    "\n",
    "**Q4. How does Memory improve user experience?**\n",
    "Memory enables context retention across interactions, allowing continuity and personalization.\n",
    "\n",
    "**Q5. Explain how LangChain integrates external data sources.**\n",
    "Via **Retrievers** and **VectorStores** that embed and search documents semantically.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q6. Describe how LCEL improves over traditional Chains.**\n",
    "LCEL provides a declarative, composable syntax that‚Äôs faster, parallelizable, and easier to debug.\n",
    "\n",
    "**Q7. How do Tools extend the capability of an LLM?**\n",
    "They enable the model to perform actions ‚Äî such as searching, computation, or API interaction ‚Äî transforming it from passive reasoning to active execution.\n",
    "\n",
    "**Q8. In a production-grade pipeline, how would you integrate LangServe and LangSmith?**\n",
    "LangServe exposes the chain/agent as an API; LangSmith monitors performance, logs traces, and identifies prompt bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario**\n",
    "\n",
    "**Q9. You‚Äôre designing an enterprise chatbot using LangChain. How would you architect it?**\n",
    "\n",
    "* ChatModel for dialogue\n",
    "* ChatPromptTemplate for structure\n",
    "* ConversationBufferMemory for context\n",
    "* Retriever (FAISS/Pinecone) for internal docs\n",
    "* LLMChain or Agent for orchestration\n",
    "* LangServe for API deployment\n",
    "* LangSmith for observability\n",
    "\n",
    "**Q10. What‚Äôs the difference between `PromptTemplate` and `ChatPromptTemplate`?**\n",
    "`PromptTemplate` ‚Üí raw text format for LLMs\n",
    "`ChatPromptTemplate` ‚Üí role-based structure for chat-oriented models\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d65f64",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
