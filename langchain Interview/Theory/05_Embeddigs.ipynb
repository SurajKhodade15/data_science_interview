{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9218cf35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß© 1. Conceptual Overview\n",
    "\n",
    "### üîπ What Are Embeddings?\n",
    "\n",
    "An **embedding** is a **numerical representation (vector)** of text that captures **semantic meaning** ‚Äî words, sentences, or documents with similar meanings are mapped **closer** in vector space.\n",
    "\n",
    "For example:\n",
    "\n",
    "* ‚ÄúAI engineer‚Äù and ‚Äúmachine learning developer‚Äù ‚Üí close in vector space\n",
    "* ‚ÄúDog‚Äù and ‚Äúbanana‚Äù ‚Üí far apart\n",
    "\n",
    "### üîπ In LangChain\n",
    "\n",
    "Embeddings transform textual chunks into vectors so that they can be:\n",
    "\n",
    "* Stored in **Vector Databases** (like FAISS, Pinecone, Chroma).\n",
    "* Retrieved using **similarity search** during RAG queries.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 2. Architectural Role in LangChain\n",
    "\n",
    "LangChain pipeline architecture:\n",
    "\n",
    "```\n",
    "Raw Data\n",
    "   ‚Üì\n",
    "Document Loader\n",
    "   ‚Üì\n",
    "Text Splitter\n",
    "   ‚Üì\n",
    "Embedding Model  ‚Üê (We are here)\n",
    "   ‚Üì\n",
    "Vector Store\n",
    "   ‚Üì\n",
    "Retriever\n",
    "   ‚Üì\n",
    "LLM\n",
    "```\n",
    "\n",
    "Embeddings enable **semantic retrieval** ‚Äî unlike keyword search, which is purely lexical.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 3. Core LangChain Embedding Interfaces\n",
    "\n",
    "LangChain provides a unified interface across multiple embedding model providers.\n",
    "\n",
    "All embedding models adhere to:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class Embeddings:\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        ...\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "Two methods:\n",
    "\n",
    "* `embed_documents()` ‚Üí batch embeddings for chunks\n",
    "* `embed_query()` ‚Üí embedding for a query string\n",
    "\n",
    "---\n",
    "\n",
    "# üß∞ 4. Popular Embedding Providers in LangChain\n",
    "\n",
    "| **Provider**              | **Class**               | **Model**                                          | **Key Strength**                 |\n",
    "| ------------------------- | ----------------------- | -------------------------------------------------- | -------------------------------- |\n",
    "| **OpenAI**                | `OpenAIEmbeddings`      | `text-embedding-3-small`, `text-embedding-3-large` | Industry standard, high accuracy |\n",
    "| **Hugging Face**          | `HuggingFaceEmbeddings` | e.g., `sentence-transformers/all-MiniLM-L6-v2`     | Offline, open-source             |\n",
    "| **Google Vertex AI**      | `VertexAIEmbeddings`    | Vertex text models                                 | Cloud-native                     |\n",
    "| **Cohere**                | `CohereEmbeddings`      | `embed-english-v3.0`                               | Fast, multilingual               |\n",
    "| **Ollama / Local Models** | `OllamaEmbeddings`      | e.g., `nomic-embed-text`                           | Local inference                  |\n",
    "| **Azure OpenAI**          | `AzureOpenAIEmbeddings` | OpenAI models via Azure                            | Enterprise-grade security        |\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 5. Example: OpenAI Embeddings\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "text = \"LangChain enables LLMs to interact with external data sources.\"\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "print(len(vector))  # ‚Üí 3072 dimensions\n",
    "```\n",
    "\n",
    "Each vector is a list of floating-point numbers representing the text in multidimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 6. Example: Hugging Face Local Embeddings\n",
    "\n",
    "Ideal for **offline**, **private**, or **low-latency** deployments.\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectors = embeddings.embed_documents([\"LangChain is powerful.\", \"RAG pipelines are efficient.\"])\n",
    "print(len(vectors[0]))  # 384 dimensions\n",
    "```\n",
    "\n",
    "This approach is cost-efficient and avoids API calls.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 7. Example: Using Embeddings with FAISS Vector Store\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Load and split\n",
    "loader = TextLoader(\"data/ai_overview.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Step 2: Embed\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 3: Store in FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Step 4: Query\n",
    "query = \"What is LangChain used for?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "for r in results:\n",
    "    print(r.page_content[:150])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üßÆ 8. Understanding Vector Dimensions\n",
    "\n",
    "Each model generates a vector of specific dimensionality:\n",
    "\n",
    "| **Model**                | **Dimensions** |\n",
    "| ------------------------ | -------------- |\n",
    "| `text-embedding-3-small` | 1536           |\n",
    "| `text-embedding-3-large` | 3072           |\n",
    "| `all-MiniLM-L6-v2`       | 384            |\n",
    "| `nomic-embed-text`       | 768            |\n",
    "| `Cohere v3`              | 1024           |\n",
    "\n",
    "üîπ **Higher dimensions = richer semantic detail**, but also **larger storage + slower retrieval**.\n",
    "Optimize dimensionality based on **data complexity** and **query diversity**.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© 9. Embedding Similarity Metrics\n",
    "\n",
    "Vector stores use distance metrics to measure closeness between vectors:\n",
    "\n",
    "| **Metric**             | **Description**                | **Usage**          |\n",
    "| ---------------------- | ------------------------------ | ------------------ |\n",
    "| **Cosine similarity**  | Measures angle between vectors | Most common        |\n",
    "| **Euclidean distance** | Measures linear distance       | Dense embeddings   |\n",
    "| **Dot product**        | Magnitude-sensitive similarity | Normalized vectors |\n",
    "\n",
    "LangChain abstracts this; vector DBs (FAISS, Pinecone, Chroma) handle it internally.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 10. Query Flow in a RAG System\n",
    "\n",
    "```\n",
    "User Query ‚Üí Embed Query\n",
    "      ‚Üì\n",
    "Similarity Search in VectorStore\n",
    "      ‚Üì\n",
    "Top K Chunks Retrieved\n",
    "      ‚Üì\n",
    "LLM Prompt + Context ‚Üí Answer\n",
    "```\n",
    "\n",
    "Embedding is used twice:\n",
    "\n",
    "1. During ingestion (to store document vectors)\n",
    "2. During query time (to find similar chunks)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è 11. Example: Comparing Two Sentences\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "v1 = emb.embed_query(\"AI models generate human-like text.\")\n",
    "v2 = emb.embed_query(\"Large Language Models create natural text.\")\n",
    "\n",
    "similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "print(f\"Cosine similarity: {similarity:.3f}\")\n",
    "```\n",
    "\n",
    "‚Üí High similarity (~0.9) indicates semantic closeness.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† 12. Optimization Strategies\n",
    "\n",
    "| **Objective**              | **Optimization**                                             |\n",
    "| -------------------------- | ------------------------------------------------------------ |\n",
    "| Reduce cost                | Use local models (HuggingFaceEmbeddings)                     |\n",
    "| Improve retrieval accuracy | Use large embedding model (`text-embedding-3-large`)         |\n",
    "| Reduce latency             | Cache embeddings or pre-compute vectors                      |\n",
    "| Lower storage overhead     | Dimensionality reduction (PCA)                               |\n",
    "| Domain-specific semantics  | Fine-tune embedding models (Sentence-BERT or domain corpora) |\n",
    "\n",
    "---\n",
    "\n",
    "# üîí 13. Enterprise Best Practices\n",
    "\n",
    "1. **Persist Embeddings** ‚Äî store vectors once, reuse them (don‚Äôt re-embed every time).\n",
    "2. **Metadata tagging** ‚Äî embed alongside metadata (source, author, timestamp).\n",
    "3. **Version control** ‚Äî embedding models change; re-embed if model updates.\n",
    "4. **Hybrid search** ‚Äî combine **vector + keyword** retrieval for precision.\n",
    "5. **Privacy** ‚Äî for sensitive data, prefer **on-prem embeddings** over API-based ones.\n",
    "\n",
    "---\n",
    "\n",
    "# üíº 14. Interview Questions & Answers\n",
    "\n",
    "### **Beginner**\n",
    "\n",
    "**Q1. What is an embedding?**\n",
    "A vector representation of text capturing semantic meaning for similarity-based retrieval.\n",
    "\n",
    "**Q2. Why are embeddings important in LangChain?**\n",
    "They enable semantic search, context retrieval, and RAG capabilities.\n",
    "\n",
    "**Q3. What are the two main embedding methods in LangChain?**\n",
    "`embed_documents()` for data ingestion and `embed_query()` for retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q4. What‚Äôs the difference between OpenAI and Hugging Face embeddings?**\n",
    "OpenAI uses cloud APIs (high accuracy, higher cost), Hugging Face is local (cheaper, customizable).\n",
    "\n",
    "**Q5. How does cosine similarity help retrieval?**\n",
    "It measures semantic closeness between query and document vectors.\n",
    "\n",
    "**Q6. What is dimensionality, and why does it matter?**\n",
    "It‚Äôs the number of numerical values per embedding vector ‚Äî higher dimensions = better semantic granularity but higher storage cost.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q7. How can embeddings be optimized for latency in large-scale RAG?**\n",
    "Use FAISS GPU indexing, batch embedding, caching, and vector quantization.\n",
    "\n",
    "**Q8. How do you handle multi-lingual documents?**\n",
    "Use multilingual embeddings such as `sentence-transformers/distiluse-base-multilingual-cased-v2`.\n",
    "\n",
    "**Q9. What are hybrid retrieval methods?**\n",
    "Combining semantic vector search with keyword/metadata filters to improve relevance.\n",
    "\n",
    "**Q10. When would you re-embed your data corpus?**\n",
    "\n",
    "* When switching embedding models\n",
    "* After major corpus updates\n",
    "* When semantic drift affects retrieval performance\n",
    "\n",
    "---\n",
    "\n",
    "# üß© 15. Real-World Architecture Snapshot\n",
    "\n",
    "| **Stage**     | **Component**    | **Responsibility**                  |\n",
    "| ------------- | ---------------- | ----------------------------------- |\n",
    "| 1Ô∏è‚É£ Loader    | Document Loader  | Ingests raw content                 |\n",
    "| 2Ô∏è‚É£ Splitter  | Text Splitter    | Chunks text                         |\n",
    "| 3Ô∏è‚É£ Embedder  | Embedding Model  | Converts to vector                  |\n",
    "| 4Ô∏è‚É£ Storage   | Vector Store     | Persists vectors                    |\n",
    "| 5Ô∏è‚É£ Retriever | Vector Retriever | Fetches semantically similar chunks |\n",
    "| 6Ô∏è‚É£ LLM Chain | Model            | Generates context-aware response    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd3bf6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
