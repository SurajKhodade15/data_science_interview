{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6036395b",
   "metadata": {},
   "source": [
    "## Executive summary\n",
    "\n",
    "* A **vector store** persists embedding vectors + metadata and exposes semantic search (similarity / hybrid / filtered searches). LangChain provides a unified VectorStore interface so you can swap implementations with minimal code change. ([LangChain Docs][1])\n",
    "* Choice of vector DB depends on **scale (GB vs TB)**, **latency requirements**, **filtering needs**, **cost model**, and **operational constraints (on-prem vs managed cloud)**. Key options: **FAISS / Annoy** (local, simple), **Chroma** (local/embedded DB), **Pinecone / Qdrant / Milvus / Weaviate / Redis** (managed/scale/feature-rich), **Elasticsearch / Supabase** (hybrid or convenience). ([LangChain][2])\n",
    "\n",
    "---\n",
    "\n",
    "# 1 — Vector stores (quick map & when to pick them)\n",
    "\n",
    "| Vector Store                               |                                                                                           Fit / Strengths | Notes                                                                                       |\n",
    "| ------------------------------------------ | --------------------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------- |\n",
    "| **FAISS**                                  | High-performance nearest-neighbour for local / GPU indexing; excellent for prototyping and scale with GPU | Very fast, many index types; used for in-memory and persisted index files. ([LangChain][2]) |\n",
    "| **Annoy**                                  |                                                      Small, disk-backed, fast approximate NN; lightweight | Good for read-heavy workloads, limited filtering features.                                  |\n",
    "| **Chroma**                                 |                                            Local-first, easy to use, built for embeddings and prototyping | Good developer UX and quick persistence. ([LangChain][3])                                   |\n",
    "| **Pinecone**                               |                                   Managed, production-ready, auto-scaling vector DB with metadata filters | Low ops, strong SLA; good for teams that prefer managed service. ([LangChain Docs][4])      |\n",
    "| **Qdrant**                                 |                  Open-source, full-featured vector DB with payload (metadata) filtering and hybrid search | Strong hybrid/dense+sparse retrieval capabilities. ([LangChain Docs][5])                    |\n",
    "| **Milvus**                                 |                                    Enterprise-grade, highly scalable, supports partitioning, GPU indexing | Good for massive corpora, supports advanced index types. ([LangChain Docs][6])              |\n",
    "| **Weaviate**                               |                               Schema-driven, semantic search + knowledge graph features, built-in modules | Good for semantic search with ontology/graph integrations. ([Weaviate Documentation][7])    |\n",
    "| **Redis (RedisVectorStore / Redis Stack)** |                                   Ultra-low latency in-memory vector search + rich filters via RediSearch | Ideal for realtime and high QPS use cases. ([LangChain][8])                                 |\n",
    "| **Elasticsearch**                          |                                                             Hybrid: lexical + vector search in one engine | Good when you need combined keyword + vector queries and existing ES infra.                 |\n",
    "| **Supabase / Postgres (pgvector)**         |                   Simpler infra if you already use Postgres; good for small-to-medium scale and filtering | Leverages relational features and SQL-based filters.                                        |\n",
    "\n",
    "(Decision criteria: latency, query load, metadata filtering, hybrid search, cost preferences, operation model.)\n",
    "\n",
    "---\n",
    "\n",
    "# 2 — LangChain integration pattern (canonical code)\n",
    "\n",
    "LangChain exposes a consistent API (`VectorStore.from_documents`, `similarity_search`, `as_retriever`) — the ingestion & query pattern is the same across stores.\n",
    "\n",
    "Generic pipeline (Python):\n",
    "\n",
    "```python\n",
    "# 1) prepare docs -> splitting -> embeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader(\"data/doc.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # or Ollama/HF\n",
    "\n",
    "# 2) persist into chosen VectorStore (example: FAISS)\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# 3) query\n",
    "results = vectorstore.similarity_search(\"What is LangChain?\", k=4)\n",
    "for d in results:\n",
    "    print(d.page_content[:200])\n",
    "```\n",
    "\n",
    "LangChain docs: unified vectorstore interface and API reference. ([LangChain Docs][1])\n",
    "\n",
    "Below are store-specific quickstarts and code notes.\n",
    "\n",
    "---\n",
    "\n",
    "# 3 — Store-specific quick reference & LangChain code\n",
    "\n",
    "### FAISS (local / GPU)\n",
    "\n",
    "* Use when you want a fast, local index (good for prototyping and GPU-accelerated production). Supports many index types (Flat, IVF, HNSW, PQ). LangChain offers both sync and async helpers. ([LangChain][2])\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "# after chunks & embeddings\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "# save/load\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "```\n",
    "\n",
    "**Notes**: GPU FAISS requires `faiss-gpu` and CUDA; CPU uses `faiss-cpu`. Consider index type trade-offs (Flat vs IVF vs HNSW).\n",
    "\n",
    "---\n",
    "\n",
    "### Pinecone (managed)\n",
    "\n",
    "* Great managed option with metadata filtering, namespaces, and scaling. Use vendor SDK + LangChain connector. Common in production for teams who want zero ops. ([LangChain Docs][4])\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone, os\n",
    "\n",
    "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=\"us-east1-gcp\")\n",
    "index_name = \"my-index\"\n",
    "vectorstore = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Chroma (local/embeddable)\n",
    "\n",
    "* Developer-friendly, easy to persist locally or run as a service. LangChain has a Chroma integration. Good for quick RAG systems and experimentation. ([LangChain][3])\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
    "vectorstore.persist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Qdrant\n",
    "\n",
    "* Open-source, supports payload-based filtering, hybrid search, and is performant at scale. LangChain has Qdrant integration and tutorials. ([LangChain Docs][5])\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "vectorstore = Qdrant.from_documents(chunks, embeddings, client=client, collection_name=\"docs\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Milvus\n",
    "\n",
    "* Enterprise-scale vector DB with partitioning, GPU acceleration, and many index types. LangChain docs + Milvus guides exist. Use for TB-scale datasets. ([LangChain Docs][6])\n",
    "\n",
    "```python\n",
    "from langchain_milvus import Milvus\n",
    "vectorstore = Milvus(embedding_function=embeddings, connection_args={\"uri\": \"tcp://127.0.0.1:19530\"}, index_params={\"index_type\": \"IVF_FLAT\"})\n",
    "vectorstore.add_documents(chunks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Weaviate\n",
    "\n",
    "* Schema-based, module support (e.g., Q&A modules), vector + class schema; good when you want semantic graph/ML pipelines. ([Weaviate Documentation][7])\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "vectorstore = Weaviate.from_documents(chunks, embeddings, client=client, index_name=\"Docs\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Redis (Redis Stack / RediSearch)\n",
    "\n",
    "* Extremely low-latency, supports vector similarity and rich metadata filtering. Good for real-time low-latency use cases and ephemeral caches. ([LangChain][8])\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores.redis import Redis\n",
    "vectorstore = Redis.from_documents(chunks, embeddings, redis_url=\"redis://localhost:6379\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Elasticsearch (dense vectors)\n",
    "\n",
    "* Use when you want combined full-text and vector search in the same cluster (useful for hybrid search and existing ES infra).\n",
    "\n",
    "### Supabase / Postgres (pgvector)\n",
    "\n",
    "* If you already use Postgres/Supabase, this is convenient for small/medium data, leverages SQL filters and ACID features.\n",
    "\n",
    "---\n",
    "\n",
    "# 4 — Key technical concepts & tuning knobs\n",
    "\n",
    "### Index types & approximate NN\n",
    "\n",
    "* **Flat (brute force)**: exact search, simple, memory-heavy.\n",
    "* **IVF (inverted file)**: cluster-based, reduces search cost with approximate recall.\n",
    "* **HNSW**: graph-based approximate NN, excellent tradeoff of accuracy & latency.\n",
    "* **PQ / OPQ**: quantization for memory reduction.\n",
    "\n",
    "(FAISS, Milvus, Qdrant, Pinecone expose these index types differently; choose based on recall & latency targets.) ([LangChain][2])\n",
    "\n",
    "### Similarity metrics\n",
    "\n",
    "* **Cosine**, **Euclidean (L2)**, **Dot-product**. Pick according to embedding normalization and DB support.\n",
    "\n",
    "### Hybrid search (sparse + dense)\n",
    "\n",
    "* Combine vector similarity with lexical scoring (BM25) or metadata filters — Qdrant, Milvus, Elasticsearch, Weaviate support hybrid approaches. ([LangChain Docs][5])\n",
    "\n",
    "### Filtering & metadata\n",
    "\n",
    "* Use metadata in vector stores for precise retrieval (e.g., `{\"source\":\"policy.pdf\", \"department\":\"legal\"}`). Many stores support server-side filters that dramatically reduce candidate vectors before similarity ranking.\n",
    "\n",
    "### Sharding, replication & partitioning\n",
    "\n",
    "* For scale: choose DBs that support partitions/namespaces (Milvus, Pinecone, Qdrant). Milvus supports partitions and advanced sharding. ([LangChain Docs][6])\n",
    "\n",
    "### GPU acceleration\n",
    "\n",
    "* FAISS (faiss-gpu), Milvus (GPU nodes), and some managed services use GPUs for indexing and fast ANN. Use when throughput/latency demands it.\n",
    "\n",
    "---\n",
    "\n",
    "# 5 — Operational & cost considerations\n",
    "\n",
    "* **Managed vs self-hosted**: Pinecone, Qdrant Cloud, Weaviate Cloud — low ops but recurring cost. Self-hosted (FAISS, Chroma, Milvus) requires infra and ops expertise. ([LangChain Docs][4])\n",
    "* **Storage & persistence**: FAISS indexes can be saved to disk; vector DBs persist to networked storage. Consider snapshot & backup strategies.\n",
    "* **Throughput & concurrency**: Redis offers high QPS; Pinecone provides SLA-backed throughput. Use async clients for parallel ingestion.\n",
    "* **Monitoring & observability**: Track recall/latency, vector store health, embedding pipeline failures, and reindexing jobs.\n",
    "* **Re-embedding strategy**: version your embedding model; re-embed corpus when switching models.\n",
    "\n",
    "---\n",
    "\n",
    "# 6 — Best practices (battle-tested)\n",
    "\n",
    "1. **Normalize metadata** (source, page, chunk_id, created_at).\n",
    "2. **Chunk & overlap** sensibly before embedding (e.g., 800± chars, 100–200 overlap).\n",
    "3. **Use batching** when embedding to minimize API calls and better throughput.\n",
    "4. **Persist embeddings** and avoid re-embedding frequently; use lazy re-indexing for updates.\n",
    "5. **Measure retrieval quality** (precision@k, recall@k, MRR) — don't rely only on anecdotal checks.\n",
    "6. **Hybrid search** — combine vector similarity with exact filters to reduce false positives.\n",
    "7. **Monitor query latency and errors**; set SLAs depending on user expectations.\n",
    "8. **Plan reindex windows** for large corpus updates (use incremental adds where possible).\n",
    "9. **Secure vector stores** (auth, TLS), especially if embeddings are sensitive.\n",
    "10. **Cache hot queries** and top-K results for low-latency UX.\n",
    "\n",
    "---\n",
    "\n",
    "# 7 — Example: hybrid retrieval with LangChain retriever\n",
    "\n",
    "```python\n",
    "# Use vectorstore retriever with metadata filter\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "docs = retriever.get_relevant_documents(\"Policy for data retention\")\n",
    "# or with metadata filter (if supported by store)\n",
    "results = vectorstore.similarity_search(\"example\", k=5, filter={\"department\": \"legal\"})\n",
    "```\n",
    "\n",
    "Stores that support server-side filtering (Pinecone, Qdrant, Redis, Milvus) make filtered similarity queries efficient. ([LangChain Docs][4])\n",
    "\n",
    "---\n",
    "\n",
    "# 8 — When to pick each DB (decision checklist)\n",
    "\n",
    "* **Prototype / local small corpora**: Chroma or FAISS. ([LangChain][3])\n",
    "* **Production, managed, minimal ops**: Pinecone or Qdrant Cloud. ([LangChain Docs][4])\n",
    "* **High QPS / low latency**: Redis (Redis Stack) or optimized FAISS/GPU. ([LangChain][8])\n",
    "* **Massive TB-scale**: Milvus or enterprise Pinecone with GPU nodes. ([LangChain Docs][6])\n",
    "* **Hybrid keyword + vector**: Elasticsearch or Weaviate. ([Weaviate Documentation][7])\n",
    "\n",
    "---\n",
    "\n",
    "# 9 — Interview Q&A (top questions + succinct model answers)\n",
    "\n",
    "Q1: **What is a vector store?**\n",
    "A: A persistence layer for embedding vectors and metadata that supports nearest-neighbour semantic search; LangChain exposes a unified API to interact with different implementations. ([LangChain Docs][1])\n",
    "\n",
    "Q2: **FAISS vs Pinecone — how decide?**\n",
    "A: FAISS = self-hosted, high control, GPU acceleration; Pinecone = managed, autoscaling, lower ops. Choose FAISS for custom indexing and Pinecone if you want managed infra and SLA. ([LangChain][2])\n",
    "\n",
    "Q3: **What is HNSW and why use it?**\n",
    "A: HNSW (Hierarchical Navigable Small World) is a graph-based ANN index offering fast query times and high recall for large-scale similarity search.\n",
    "\n",
    "Q4: **How do you handle metadata filtering?**\n",
    "A: Use DBs that support payload filtering (Qdrant, Pinecone, Redis, Milvus) and pass filter expressions at query time to restrict candidate vectors before ranking. ([LangChain Docs][5])\n",
    "\n",
    "Q5: **How to scale embeddings ingestion for 1M docs?**\n",
    "A: Batch embeddings, parallelize encoding across workers/GPUs, use efficient index types (IVF/HNSW), and insert into a scalable vector DB (Milvus/Pinecone/Qdrant) with bulk APIs.\n",
    "\n",
    "Q6: **What is hybrid retrieval?**\n",
    "A: Combining vector similarity (semantic) with lexical or sparse retrieval (e.g., BM25) or metadata filters to improve precision and trustworthiness. ([LangChain Docs][6])\n",
    "\n",
    "Q7: **How to evaluate a vector store in production?**\n",
    "A: Track recall@k, precision@k, latency P95, throughput, cost per query, and error rates; A/B test embeddings + retriever configurations.\n",
    "\n",
    "Q8: **When to re-embed your corpus?**\n",
    "A: When you change embedding model, change tokenization semantics, see retrieval drift, or when major corpus updates happen.\n",
    "\n",
    "---\n",
    "\n",
    "# 10 — References & further reading (official docs)\n",
    "\n",
    "* LangChain vectorstores overview & API. ([LangChain Docs][1])\n",
    "* FAISS (LangChain FAISS async & docs). ([LangChain][2])\n",
    "* Chroma provider docs. ([LangChain][3])\n",
    "* Pinecone + LangChain integration guide. ([LangChain Docs][4])\n",
    "* Qdrant + LangChain docs & tutorials. ([LangChain Docs][5])\n",
    "* Milvus LangChain integration & RAG guide. ([LangChain Docs][6])\n",
    "* Redis vector search with LangChain. ([LangChain][8])\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cbddf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
