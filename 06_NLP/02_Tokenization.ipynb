{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f06656e",
   "metadata": {},
   "source": [
    "\n",
    "# **Tokenization and Its Types**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Theory**\n",
    "\n",
    "### **What is Tokenization?**\n",
    "\n",
    "* **Definition**: Tokenization is the process of splitting raw text into smaller meaningful units called **tokens** (words, subwords, sentences, or characters).\n",
    "* Tokens are the **basic building blocks** for any NLP pipeline.\n",
    "* Example:\n",
    "  Input: *“I love NLP!”*\n",
    "  Tokens: `[“I”, “love”, “NLP”, “!”]`\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Tokenization**\n",
    "\n",
    "1. **Sentence Tokenization**\n",
    "\n",
    "   * Splitting text into sentences.\n",
    "   * Example:\n",
    "     Text: *“I love NLP. It is amazing.”*\n",
    "     Tokens: `[“I love NLP.”, “It is amazing.”]`\n",
    "\n",
    "2. **Word Tokenization**\n",
    "\n",
    "   * Splitting sentences into words.\n",
    "   * Example:\n",
    "     Text: *“I love NLP.”*\n",
    "     Tokens: `[“I”, “love”, “NLP”, “.”]`\n",
    "\n",
    "3. **Character Tokenization**\n",
    "\n",
    "   * Splitting text into individual characters.\n",
    "   * Example:\n",
    "     Text: *“NLP”*\n",
    "     Tokens: `[“N”, “L”, “P”]`\n",
    "\n",
    "4. **Subword Tokenization (Modern NLP)**\n",
    "\n",
    "   * Splitting words into smaller units (useful for rare or unknown words).\n",
    "   * Example:\n",
    "     Word: *“unhappiness”* → `[“un”, “happy”, “ness”]`\n",
    "   * Used in **BPE (Byte Pair Encoding)**, **WordPiece** (BERT), **SentencePiece** (T5).\n",
    "\n",
    "5. **Whitespace Tokenization**\n",
    "\n",
    "   * Splitting text by spaces.\n",
    "   * Simple, but fails for punctuation and contractions.\n",
    "   * Example: *“I’m happy.”* → `[“I’m”, “happy.”]`\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Examples in NLTK & SpaCy**\n",
    "\n",
    "### **NLTK Example**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"Tokenization is the first step in NLP. It's essential for text processing.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "print(\"Sentence Tokens:\", sent_tokenize(text))\n",
    "\n",
    "# Word Tokenization\n",
    "print(\"Word Tokens:\", word_tokenize(text))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Sentence Tokens: ['Tokenization is the first step in NLP.', \"It's essential for text processing.\"]\n",
    "Word Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', \"'s\", 'essential', 'for', 'text', 'processing', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **SpaCy Example**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Tokenization is the first step in NLP. It's essential for text processing.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Sentence Tokenization\n",
    "print(\"Sentence Tokens:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "# Word Tokenization with POS\n",
    "print(\"\\nWord Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Sentence Tokens:\n",
    "Tokenization is the first step in NLP.\n",
    "It’s essential for text processing.\n",
    "\n",
    "Word Tokens:\n",
    "Tokenization NOUN\n",
    "is AUX\n",
    "the DET\n",
    "first ADJ\n",
    "step NOUN\n",
    "in ADP\n",
    "NLP PROPN\n",
    ". PUNCT\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Interview-Style Q&A**\n",
    "\n",
    "### **Basic Level**\n",
    "\n",
    "**Q1. What is tokenization in NLP?**\n",
    "*A: Tokenization is the process of splitting raw text into smaller meaningful units called tokens (words, sentences, subwords, or characters). It is the foundation for most NLP tasks.*\n",
    "\n",
    "**Q2. What are the main types of tokenization?**\n",
    "*A: Sentence tokenization, word tokenization, character tokenization, subword tokenization, and whitespace tokenization.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate Level**\n",
    "\n",
    "**Q3. Why is subword tokenization important in modern NLP models?**\n",
    "*A: Subword tokenization helps handle rare or unknown words by breaking them into smaller units, reducing vocabulary size and improving generalization in models like BERT and GPT.*\n",
    "\n",
    "**Q4. Difference between NLTK and SpaCy in tokenization?**\n",
    "*A: NLTK provides rule-based tokenizers (like Punkt) and is flexible for teaching/research. SpaCy uses a production-ready tokenizer optimized for speed and accuracy, with built-in support for linguistic annotations.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Level**\n",
    "\n",
    "**Q5. How do tokenizers in Transformer models differ from traditional tokenizers?**\n",
    "*A: Traditional tokenizers split text into words/sentences, while Transformer tokenizers (BPE, WordPiece, SentencePiece) split text into subwords, making them more robust to rare words and morphologically rich languages.*\n",
    "\n",
    "**Q6. What challenges exist in tokenization?**\n",
    "*A: Handling contractions (e.g., “don’t”), multi-word entities (e.g., “New York”), languages without spaces (e.g., Chinese), and domain-specific terms (e.g., medical jargon). Modern approaches like subword tokenization address many of these issues.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e85edb",
   "metadata": {},
   "source": [
    "\n",
    "# **Comparison of Tokenization Techniques**\n",
    "\n",
    "| **Technique**                                            | **Description**                                      | **Use Cases**                                                                             | **Pros**                                                                | **Cons**                                                                     |\n",
    "| -------------------------------------------------------- | ---------------------------------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Whitespace Tokenization**                              | Splits text based on spaces only.                    | Quick text splitting, simple preprocessing.                                               | Simple and fast. No external libraries needed.                          | Fails on punctuation and contractions (*“I’m” → “I’m”*), language-dependent. |\n",
    "| **Sentence Tokenization**                                | Splits text into sentences using rules or models.    | Document summarization, sentiment analysis at sentence level.                             | Maintains context; good for sentence-level NLP.                         | Rule-based models fail with abbreviations (*“Dr. Smith”*).                   |\n",
    "| **Word Tokenization**                                    | Splits sentences into words/tokens.                  | Text classification, POS tagging, sentiment analysis.                                     | Foundation for most NLP tasks; widely supported in NLTK & SpaCy.        | Struggles with contractions (*“don’t” → [“do”, “n’t”]*).                     |\n",
    "| **Character Tokenization**                               | Splits text into individual characters.              | Languages without clear word boundaries (Chinese, Japanese, Korean); spelling correction. | Useful for morphologically rich languages; captures fine details.       | Very sparse representation; loses semantic meaning.                          |\n",
    "| **Subword Tokenization** (BPE, WordPiece, SentencePiece) | Splits words into smaller meaningful units.          | Transformers (BERT, GPT, T5), machine translation, speech recognition.                    | Handles rare/unknown words; reduces vocabulary size; language-agnostic. | More complex; requires training a tokenizer model.                           |\n",
    "| **Rule-based Tokenization**                              | Uses regex/patterns to split text.                   | Domain-specific NLP (legal, medical, financial text).                                     | Customizable for industry-specific use cases.                           | Fragile; rules may fail for unseen cases.                                    |\n",
    "| **Statistical / ML-based Tokenization**                  | Uses statistical models to predict token boundaries. | Complex languages (Chinese, Thai, Hindi).                                                 | More accurate in complex scripts; adapts better than rules.             | Requires large training data; slower than simple methods.                    |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Insights for Interviews**:\n",
    "\n",
    "* Mention that **traditional NLP** often used *word-level tokenization*, but **modern deep learning models** rely on *subword tokenization* for efficiency and robustness.\n",
    "* Show awareness of **language diversity** — e.g., whitespace tokenization won’t work in **Chinese** or **Japanese**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c502008",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
