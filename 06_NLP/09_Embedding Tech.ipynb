{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a6717a",
   "metadata": {},
   "source": [
    "\n",
    "# **Embedding Techniques in NLP**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Theory**\n",
    "\n",
    "### **What is Embedding?**\n",
    "\n",
    "* **Definition**: Embeddings (or vector representations) convert **text (words, sentences, documents)** into **numerical vectors** that can be processed by ML/DL models.\n",
    "* Goal: Capture **semantic meaning** of text.\n",
    "\n",
    "---\n",
    "\n",
    "### **A. One-Hot Encoding**\n",
    "\n",
    "* Each word is represented by a **binary vector**.\n",
    "* Only one element is `1`, rest are `0`.\n",
    "* Example:\n",
    "  Vocabulary = `[apple, banana, orange]`\n",
    "\n",
    "  * apple ‚Üí `[1, 0, 0]`\n",
    "  * banana ‚Üí `[0, 1, 0]`\n",
    "  * orange ‚Üí `[0, 0, 1]`\n",
    "\n",
    "**Pros**: Simple, easy to implement.\n",
    "**Cons**:\n",
    "\n",
    "* Sparse vectors (mostly zeros).\n",
    "* No semantic similarity (apple ‚â† banana).\n",
    "* Vocabulary explosion with large corpora.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Bag of Words (BoW)**\n",
    "\n",
    "* Represent text as a **vector of word counts** (or frequencies).\n",
    "* Ignores word order and grammar.\n",
    "* Example:\n",
    "  Corpus = [‚ÄúI love NLP‚Äù, ‚ÄúI love AI‚Äù]\n",
    "  Vocabulary = `[I, love, NLP, AI]`\n",
    "\n",
    "  * ‚ÄúI love NLP‚Äù ‚Üí `[1, 1, 1, 0]`\n",
    "  * ‚ÄúI love AI‚Äù ‚Üí `[1, 1, 0, 1]`\n",
    "\n",
    "**Pros**: Simple, interpretable.\n",
    "**Cons**:\n",
    "\n",
    "* Ignores context & word order.\n",
    "* Large feature space.\n",
    "* Common words dominate representation.\n",
    "\n",
    "---\n",
    "\n",
    "### **C. TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)**\n",
    "\n",
    "* Improves BoW by **downweighting common words** and **highlighting important words**.\n",
    "\n",
    "* Formula:\n",
    "\n",
    "  * **TF (Term Frequency)** = Count of word in doc / Total words in doc\n",
    "  * **IDF (Inverse Document Frequency)** = log(Total docs / Docs containing word)\n",
    "  * **TF-IDF = TF √ó IDF**\n",
    "\n",
    "* Example:\n",
    "\n",
    "  * Word ‚Äúthe‚Äù appears in **1000 documents** ‚Üí low weight.\n",
    "  * Word ‚ÄúNLP‚Äù appears in **10 documents only** ‚Üí higher weight.\n",
    "\n",
    "**Pros**: Better than BoW, highlights informative words.\n",
    "**Cons**: Still ignores context/semantics, vocabulary is large.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Examples**\n",
    "\n",
    "---\n",
    "\n",
    "### **A. One-Hot Encoding**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "corpus = [[\"apple\"], [\"banana\"], [\"orange\"]]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot = encoder.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", encoder.categories_)\n",
    "print(\"One-hot vectors:\\n\", one_hot)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Vocabulary: [array(['apple', 'banana', 'orange'], dtype=object)]\n",
    "One-hot vectors:\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Bag of Words (CountVectorizer)**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"I love NLP\", \"I love AI\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Vectors:\\n\", X.toarray())\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Vocabulary: ['ai', 'love', 'nlp']\n",
    "BoW Vectors:\n",
    "[[0 1 1]\n",
    " [1 1 0]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **C. TF-IDF**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"I love NLP\", \"I love AI\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectors:\\n\", X.toarray())\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Vocabulary: ['ai', 'love', 'nlp']\n",
    "TF-IDF Vectors:\n",
    "[[0.         0.70710678 0.70710678]\n",
    " [0.70710678 0.70710678 0.        ]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Comparison Table**\n",
    "\n",
    "| Technique        | Representation        | Pros                                   | Cons                                     | Use Cases                                 |\n",
    "| ---------------- | --------------------- | -------------------------------------- | ---------------------------------------- | ----------------------------------------- |\n",
    "| **One-Hot**      | Binary vector         | Simple, interpretable                  | Sparse, no semantic meaning              | Very small datasets, toy problems         |\n",
    "| **Bag of Words** | Word counts/frequency | Easy, works with traditional ML models | Ignores context, high dimensionality     | Text classification, spam detection       |\n",
    "| **TF-IDF**       | Weighted frequency    | Highlights important terms             | Still ignores context, large vocab space | Information retrieval, keyword extraction |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Interview-Style Q&A**\n",
    "\n",
    "### **Basic**\n",
    "\n",
    "**Q1. What is one-hot encoding in NLP?**\n",
    "*A: It represents each word as a binary vector with only one active element (1), and the rest zeros.*\n",
    "\n",
    "**Q2. What is the limitation of one-hot encoding?**\n",
    "*A: It leads to sparse, high-dimensional vectors and cannot capture semantic similarity between words.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q3. How does Bag of Words represent text?**\n",
    "*A: BoW represents text as a vector of word counts or frequencies, ignoring grammar and word order.*\n",
    "\n",
    "**Q4. What problem does TF-IDF solve compared to BoW?**\n",
    "*A: TF-IDF reduces the weight of common words (like \"the\", \"is\") and highlights rare but informative words, improving feature representation.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q5. What are the limitations of BoW and TF-IDF compared to word embeddings like Word2Vec or BERT?**\n",
    "*A: BoW and TF-IDF ignore semantic meaning and word order, while embeddings like Word2Vec or BERT capture context, semantics, and relationships between words.*\n",
    "\n",
    "**Q6. If given a very large text corpus, how would you choose between BoW and TF-IDF?**\n",
    "*A: Use TF-IDF because it reduces noise from frequent words, making features more informative. BoW is simpler and may be used for smaller datasets or baseline models.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ccd96",
   "metadata": {},
   "source": [
    "\n",
    "# **Embedding / Vectorization Techniques in NLP**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Classical Techniques** (we already covered)\n",
    "\n",
    "* One-Hot Encoding\n",
    "* Bag of Words (BoW)\n",
    "* TF-IDF\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Advanced Techniques**\n",
    "\n",
    "### **A. Word2Vec (Google, 2013)**\n",
    "\n",
    "* Learns **dense, low-dimensional vectors** for words.\n",
    "* Trained on large corpora using a shallow neural network.\n",
    "* Two training architectures:\n",
    "\n",
    "  * **CBOW (Continuous Bag of Words):** Predicts a word from context.\n",
    "  * **Skip-gram:** Predicts context words from a word.\n",
    "* Captures **semantic relationships** ‚Üí *king ‚Äì man + woman ‚âà queen*.\n",
    "\n",
    "üëâ **Use Cases**: Semantic search, text similarity, document clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. GloVe (Global Vectors, Stanford, 2014)**\n",
    "\n",
    "* Uses **co-occurrence matrix** + matrix factorization.\n",
    "* Captures **global statistical information** (not just local context like Word2Vec).\n",
    "* Pre-trained embeddings available (Wikipedia, Common Crawl).\n",
    "\n",
    "üëâ **Use Cases**: Sentiment analysis, semantic similarity, transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **C. FastText (Facebook, 2016)**\n",
    "\n",
    "* Extension of Word2Vec.\n",
    "* Represents words as **bag of character n-grams**.\n",
    "* Can generate embeddings for **out-of-vocabulary (OOV)** words ‚Üí handles morphology well.\n",
    "\n",
    "üëâ **Use Cases**: Multilingual NLP, handling misspellings, domain-specific vocab.\n",
    "\n",
    "---\n",
    "\n",
    "### **D. ELMo (Embeddings from Language Models, 2018)**\n",
    "\n",
    "* First **contextual embeddings**.\n",
    "* Word meaning depends on **context** ‚Üí *‚Äúbank‚Äù (river vs. finance)*.\n",
    "* Uses **bi-directional LSTMs**.\n",
    "\n",
    "üëâ **Use Cases**: Named Entity Recognition (NER), POS tagging, sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **E. BERT (Bidirectional Encoder Representations from Transformers, 2018)**\n",
    "\n",
    "* **Transformer-based embeddings**.\n",
    "* Contextual, bidirectional, subword-level.\n",
    "* Pre-trained with **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.\n",
    "* State-of-the-art in many NLP tasks.\n",
    "\n",
    "üëâ **Use Cases**: QA systems, sentence classification, entity recognition, semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "### **F. Sentence Transformers (SBERT, 2019)**\n",
    "\n",
    "* Extension of BERT for **sentence-level embeddings**.\n",
    "* Produces embeddings that can be compared with **cosine similarity**.\n",
    "* Used widely in **semantic search, clustering, and RAG systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### **G. Doc2Vec (Paragraph Vectors, 2014)**\n",
    "\n",
    "* Extension of Word2Vec for **document embeddings**.\n",
    "* Learns fixed-length vector representations for variable-length text (paragraphs, documents).\n",
    "\n",
    "üëâ **Use Cases**: Document classification, clustering, topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **H. Transformer-based Large Embeddings (OpenAI, Cohere, etc.)**\n",
    "\n",
    "* Modern APIs (e.g., **OpenAI‚Äôs text-embedding-3-large**) produce **universal embeddings**.\n",
    "* Capture deep context and generalize across tasks.\n",
    "\n",
    "üëâ **Use Cases**: Enterprise search, recommendation systems, knowledge graphs.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Comparison Table**\n",
    "\n",
    "| Technique       | Dimension | Context-Aware | Handles OOV | Pros                          | Cons                   | Use Cases                          |\n",
    "| --------------- | --------- | ------------- | ----------- | ----------------------------- | ---------------------- | ---------------------------------- |\n",
    "| **One-Hot**     | High      | ‚ùå             | ‚ùå           | Simple                        | Sparse, no meaning     | Toy models                         |\n",
    "| **BoW**         | High      | ‚ùå             | ‚ùå           | Easy, interpretable           | Ignores order/context  | Baseline classification            |\n",
    "| **TF-IDF**      | High      | ‚ùå             | ‚ùå           | Weights important words       | Still no context       | Info retrieval, keyword extraction |\n",
    "| **Word2Vec**    | 100‚Äì300   | ‚ùå             | ‚ùå           | Captures semantic similarity  | Same word, same vector | Semantic similarity                |\n",
    "| **GloVe**       | 100‚Äì300   | ‚ùå             | ‚ùå           | Captures global co-occurrence | Static embeddings      | General NLP, similarity            |\n",
    "| **FastText**    | 100‚Äì300   | ‚ùå             | ‚úÖ           | Handles rare/OOV words        | Static embeddings      | Multilingual NLP                   |\n",
    "| **Doc2Vec**     | 100‚Äì300   | ‚ùå             | ‚úÖ           | Whole document representation | Less popular now       | Document classification            |\n",
    "| **ELMo**        | 1024      | ‚úÖ             | ‚úÖ           | Contextual embeddings         | Heavy, slower          | Sequence labeling                  |\n",
    "| **BERT**        | 768‚Äì1024  | ‚úÖ             | ‚úÖ           | State-of-the-art contextual   | Large compute cost     | QA, NER, classification            |\n",
    "| **SBERT**       | 768       | ‚úÖ             | ‚úÖ           | Sentence-level meaning        | Requires fine-tuning   | Semantic search                    |\n",
    "| **OpenAI Emb.** | 1536‚Äì4096 | ‚úÖ             | ‚úÖ           | General-purpose embeddings    | API cost               | Enterprise search, RAG             |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Interview-Style Q&A**\n",
    "\n",
    "### **Basic**\n",
    "\n",
    "**Q1. What is the difference between BoW and Word2Vec?**\n",
    "*A: BoW is sparse, counts words, ignores context. Word2Vec learns dense embeddings where semantically similar words have similar vectors.*\n",
    "\n",
    "**Q2. Why is TF-IDF better than BoW?**\n",
    "*A: TF-IDF reduces the influence of frequent but uninformative words like ‚Äúthe‚Äù or ‚Äúis‚Äù, giving higher importance to rare but informative words.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate**\n",
    "\n",
    "**Q3. What advantage does FastText have over Word2Vec?**\n",
    "*A: FastText represents words as character n-grams, so it can generate embeddings for out-of-vocabulary words and handle morphology better.*\n",
    "\n",
    "**Q4. How do contextual embeddings like BERT differ from static embeddings like Word2Vec?**\n",
    "*A: Static embeddings assign the same vector to a word regardless of context, while contextual embeddings change meaning depending on sentence context.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced**\n",
    "\n",
    "**Q5. Why are embeddings like BERT or SBERT preferred in modern NLP pipelines?**\n",
    "*A: They capture deep semantic and syntactic context, generalize across tasks, and provide strong performance on downstream applications like semantic search, QA, and summarization.*\n",
    "\n",
    "**Q6. How would you choose between TF-IDF and modern embeddings in a real project?**\n",
    "*A: For small datasets and interpretable tasks, TF-IDF may suffice. For semantic understanding, multilingual support, or advanced tasks (chatbots, RAG, search), modern embeddings like BERT/SBERT or OpenAI embeddings are better.*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64c1d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
