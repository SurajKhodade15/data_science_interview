{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370e5f54",
   "metadata": {},
   "source": [
    "\n",
    "# **Sentence Segmentation**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Theory**\n",
    "\n",
    "### **What is Sentence Segmentation?**\n",
    "\n",
    "* **Definition**: Sentence segmentation is the process of splitting a text into **individual sentences**.\n",
    "* It’s also called **sentence boundary detection**.\n",
    "* Example:\n",
    "  Text: *“I love NLP. It is fascinating. Let’s learn SpaCy and NLTK.”*\n",
    "  Sentences:\n",
    "\n",
    "  ```\n",
    "  [\"I love NLP.\", \"It is fascinating.\", \"Let's learn SpaCy and NLTK.\"]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Sentence Segmentation important?**\n",
    "\n",
    "* Prepares text for **downstream tasks** like:\n",
    "\n",
    "  * **Tokenization** → Word-level processing\n",
    "  * **POS tagging**\n",
    "  * **NER**\n",
    "  * **Sentiment analysis** (sentence-level sentiment)\n",
    "  * **Text summarization** (sentence extraction)\n",
    "* Helps **maintain context** when splitting large documents.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges**\n",
    "\n",
    "* Abbreviations: *“Dr. Smith is here.”* → Avoid splitting after “Dr.”\n",
    "* Decimal numbers: *“Price is $5.50. Amazing deal!”*\n",
    "* Ellipses or punctuation marks: *“Wait… what happened?”*\n",
    "* Quotations and dialogues: *“Hello,” he said. “How are you?”*\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Examples**\n",
    "\n",
    "### **Sentence Segmentation with NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Dr. Smith loves NLP. He works at OpenAI. Let's learn SpaCy and NLTK!\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Sentences:\n",
    "Dr. Smith loves NLP.\n",
    "He works at OpenAI.\n",
    "Let's learn SpaCy and NLTK!\n",
    "```\n",
    "\n",
    "* **NLTK’s `punkt` tokenizer** handles common abbreviations automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sentence Segmentation with SpaCy**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Dr. Smith loves NLP. He works at OpenAI. Let's learn SpaCy and NLTK!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Sentences:\n",
    "Dr. Smith loves NLP.\n",
    "He works at OpenAI.\n",
    "Let's learn SpaCy and NLTK!\n",
    "```\n",
    "\n",
    "* SpaCy’s sentence segmentation is **rule-based and model-enhanced**, often using **dependency parsing** to identify sentence boundaries.\n",
    "* Can be **customized** with `sentencizer` for faster segmentation without parsing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom Sentencizer in SpaCy**\n",
    "\n",
    "```python\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "# Add the sentencizer component\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"Dr. Smith loves NLP. He works at OpenAI. Let's learn SpaCy and NLTK!\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "```\n",
    "\n",
    "✅ This is faster if you don’t need full dependency parsing.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Interview-Style Q&A**\n",
    "\n",
    "### **Basic Level**\n",
    "\n",
    "**Q1. What is sentence segmentation in NLP?**\n",
    "*A: Sentence segmentation is the process of splitting a text into individual sentences to prepare it for downstream NLP tasks.*\n",
    "\n",
    "**Q2. Why is it important?**\n",
    "*A: It maintains context, enables sentence-level processing, and prepares text for tokenization, POS tagging, NER, summarization, and sentiment analysis.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate Level**\n",
    "\n",
    "**Q3. How does NLTK handle sentence segmentation?**\n",
    "*A: NLTK uses the Punkt tokenizer, which is unsupervised and trained on large corpora. It handles abbreviations and punctuation to detect sentence boundaries.*\n",
    "\n",
    "**Q4. How does SpaCy handle sentence segmentation?**\n",
    "*A: SpaCy uses a combination of rule-based heuristics, dependency parsing, and machine learning models to detect sentence boundaries. It can also be customized with the `sentencizer` component for faster segmentation.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Level**\n",
    "\n",
    "**Q5. What are common challenges in sentence segmentation?**\n",
    "*A: Abbreviations (e.g., Dr., Mr.), decimal numbers, ellipses, quotations, and domain-specific punctuations can confuse boundary detection.*\n",
    "\n",
    "**Q6. When would you customize SpaCy’s sentencizer?**\n",
    "*A: When processing large texts where dependency parsing is unnecessary, to improve speed, or when domain-specific sentence boundaries need to be defined.*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b520c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
