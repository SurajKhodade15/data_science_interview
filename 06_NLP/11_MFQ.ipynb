{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e49f8bf",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ **Foundations**\n",
    "\n",
    "**Q1. What is tokenization and why is subword tokenization preferred today?**\n",
    "**A:** Tokenization means breaking text into smaller units‚Äîlike words, characters, or subwords‚Äîso a model can process them. Subword tokenization (like BPE or WordPiece) strikes a balance: it keeps vocabulary size manageable while still handling rare or unseen words gracefully. It also captures meaningful parts of words (like prefixes/suffixes), which helps models generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. Stemming vs Lemmatization‚Äîwhen to use which?**\n",
    "**A:** Stemming is a quick heuristic that chops off word endings (so ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù), but it might produce invalid words. Lemmatization, on the other hand, uses linguistic knowledge to get the true base form (‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù). If speed is key‚Äîsay, in large search systems‚Äîstemming is fine. For tasks needing linguistic accuracy like chatbots or information extraction, lemmatization is better.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. Stopwords‚Äîshould we remove them?**\n",
    "**A:** For older models like Bag-of-Words or TF-IDF, yes‚Äîit reduces noise and dimensionality. But for transformer models like BERT, no‚Äîsince stopwords can affect meaning (for example, ‚Äúnot happy‚Äù ‚â† ‚Äúhappy‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. BoW vs TF-IDF vs Embeddings**\n",
    "**A:**\n",
    "\n",
    "* **BoW:** Simple word counts, no notion of importance.\n",
    "* **TF-IDF:** Highlights rare but meaningful words by downweighting common ones.\n",
    "* **Embeddings:** Capture deeper semantic meaning, allowing words like *‚Äúking‚Äù* and *‚Äúqueen‚Äù* to be close in vector space. Contextual embeddings (like BERT) even adjust based on sentence meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Classic Modeling**\n",
    "\n",
    "**Q5. Why use n-grams? What‚Äôs the catch?**\n",
    "**A:** N-grams capture short word sequences like ‚ÄúNew York‚Äù or ‚Äúmachine learning.‚Äù The drawback? The vocabulary explodes as n increases, and unseen sequences get zero probability. That‚Äôs why we use smoothing and later switched to neural or subword models.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. What is smoothing in language modeling?**\n",
    "**A:** Smoothing redistributes probability to unseen n-grams so they aren‚Äôt zero. Techniques like Kneser-Ney or Good-Turing help models generalize and reduce perplexity.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. When are CRFs preferred?**\n",
    "**A:** In sequence labeling tasks like NER or POS tagging, where labels depend on each other (e.g., ‚ÄúNew‚Äù ‚Üí ‚ÄúYork‚Äù ‚Üí ‚ÄúCity‚Äù). CRFs model label dependencies globally, unlike independent classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Embeddings**\n",
    "\n",
    "**Q8. Compare Word2Vec, GloVe, FastText, and contextual embeddings.**\n",
    "**A:**\n",
    "\n",
    "* **Word2Vec:** Learns from local context windows.\n",
    "* **GloVe:** Uses global co-occurrence statistics.\n",
    "* **FastText:** Breaks words into character n-grams, great for rare or morphologically rich languages.\n",
    "* **Contextual (ELMo/BERT):** Word meaning depends on context‚Äîso ‚Äúbank‚Äù in ‚Äúriver bank‚Äù vs ‚Äúcredit bank‚Äù differs.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. How do we handle Out-of-Vocabulary (OOV) words?**\n",
    "**A:** Subword tokenization (like BPE or WordPiece) or models like FastText that build word vectors from character n-grams solve OOV problems effectively. Avoiding a single `<UNK>` token is key.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Transformers and Attention**\n",
    "\n",
    "**Q10. Explain self-attention and its complexity.**\n",
    "**A:** Self-attention lets each token look at every other token to understand relationships. It‚Äôs powerful but computationally heavy‚ÄîO(n¬≤) with respect to sequence length‚Äîso researchers use optimized versions like sparse or linear attention.\n",
    "\n",
    "---\n",
    "\n",
    "**Q11. Why did Transformers replace RNNs/LSTMs?**\n",
    "**A:** Transformers process all tokens simultaneously (better parallelism), capture long-range dependencies, and scale beautifully with large datasets and compute. That‚Äôs why models like BERT and GPT became dominant.\n",
    "\n",
    "---\n",
    "\n",
    "**Q12. Encoder vs Decoder vs Encoder-Decoder?**\n",
    "**A:**\n",
    "\n",
    "* **Encoder:** Understands input (BERT).\n",
    "* **Decoder:** Generates text (GPT).\n",
    "* **Encoder-Decoder:** Translates or summarizes by mapping input to output (T5, BART).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Pretraining and Adaptation**\n",
    "\n",
    "**Q13. Common pretraining objectives?**\n",
    "**A:**\n",
    "\n",
    "* **BERT:** Masked Language Modeling (MLM)\n",
    "* **GPT:** Next-token prediction (causal LM)\n",
    "* **T5/BART:** Denoising seq-to-seq tasks\n",
    "* **Contrastive:** Used for retrieval or alignment (e.g., CLIP)\n",
    "\n",
    "---\n",
    "\n",
    "**Q14. Full fine-tuning vs adapters vs prompt-tuning?**\n",
    "**A:**\n",
    "\n",
    "* **Full fine-tuning:** Update all model parameters (best performance, high cost).\n",
    "* **Adapters/prefix-tuning:** Train small added layers, much cheaper.\n",
    "* Use **PEFT** methods when you need multiple domain models or cost-efficient deployment.\n",
    "\n",
    "---\n",
    "\n",
    "**Q15. In-context learning vs fine-tuning?**\n",
    "**A:** In ICL, we guide the model with examples in the prompt‚Äîno retraining. Fine-tuning updates model weights. ICL is quick and flexible; fine-tuning is stable for long-term, domain-specific use.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Tokenization Details**\n",
    "\n",
    "**Q16. WordPiece vs BPE vs UnigramLM**\n",
    "**A:**\n",
    "\n",
    "* **BPE:** Greedy merges of frequent pairs.\n",
    "* **WordPiece:** Maximizes likelihood for better balance.\n",
    "* **UnigramLM:** Probabilistic and prunes less useful tokens.\n",
    "  Choice affects sequence length and rare word handling.\n",
    "\n",
    "---\n",
    "\n",
    "**Q17. Normalization and special tokens?**\n",
    "**A:** Normalize case/Unicode, handle tokens like `[CLS]`, `[SEP]`, `[PAD]`, `[MASK]`. Padding must be masked during attention to avoid biasing results.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Sequence Labeling and Parsing**\n",
    "\n",
    "**Q18. How to build a strong NER today?**\n",
    "**A:** Use a pretrained transformer encoder (like BERT) with a token classification head or CRF decoder. Add domain-specific dictionaries and span models for nested entities. Track recall through domain evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q19. POS/Dependency parsing best practices?**\n",
    "**A:** Use contextual encoders with biaffine or graph-based decoders. For multilingual tasks, shared subword vocabularies and adapters work well.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Classification and Retrieval**\n",
    "\n",
    "**Q20. Robust text classification architecture?**\n",
    "**A:** Encoder (like RoBERTa) ‚Üí pooling layer (CLS or mean) ‚Üí linear classifier. Handle imbalance with focal loss or weighted sampling. Evaluate per-class F1 for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "**Q21. What is RAG and why use it?**\n",
    "**A:** Retrieval-Augmented Generation (RAG) enhances LLMs with real documents to ground answers in facts. It reduces hallucinations and keeps outputs up-to-date by combining retrievers (BM25 or dual encoders) with generators.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Evaluation**\n",
    "\n",
    "**Q22. Perplexity, BLEU, ROUGE, METEOR, BERTScore‚Äîwhen to use?**\n",
    "**A:**\n",
    "\n",
    "* **Perplexity:** Language modeling quality.\n",
    "* **BLEU:** Precision for machine translation.\n",
    "* **ROUGE:** Recall for summarization.\n",
    "* **METEOR:** Includes synonyms/stemming.\n",
    "* **BERTScore:** Contextual similarity, often closer to human judgment.\n",
    "\n",
    "---\n",
    "\n",
    "**Q23. Macro vs Micro F1?**\n",
    "**A:**\n",
    "\n",
    "* **Macro F1:** Equal weight to all classes‚Äîgood for imbalance analysis.\n",
    "* **Micro F1:** Aggregates over all predictions‚Äîgood for overall accuracy.\n",
    "  Always report both.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Data Quality and Imbalance**\n",
    "\n",
    "**Q24. How to handle noisy labels?**\n",
    "**A:** Use robust losses (like label smoothing), confidence filtering, and small clean validation sets. Analyze errors by data slices and retrain iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "**Q25. Handling class imbalance?**\n",
    "**A:** Techniques include oversampling, class weighting, focal loss, or threshold tuning. Evaluate using PR-AUC or per-class F1 rather than overall accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Production and Safety**\n",
    "\n",
    "**Q26. How to reduce hallucinations in LLM apps?**\n",
    "**A:** Add retrieval grounding, constrained decoding, validation checks, and feedback loops. Use instruction-tuned models and safe fallback responses.\n",
    "\n",
    "---\n",
    "\n",
    "**Q27. Defenses against prompt injection/jailbreaks?**\n",
    "**A:** Sanitize inputs, restrict accessible tools, isolate retrieval contexts, and perform adversarial testing. Never execute model outputs directly.\n",
    "\n",
    "---\n",
    "\n",
    "**Q28. What should be monitored post-deployment?**\n",
    "**A:** Track quality, drift (embedding stats), latency, cost, and safety (toxicity/PII). Implement feedback loops and versioned rollbacks.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Ethics and Privacy**\n",
    "\n",
    "**Q29. How to detect and mitigate bias?**\n",
    "**A:** Measure subgroup metrics, test counterfactuals, balance training data, and use adversarial training or calibration. Publish model cards to ensure transparency.\n",
    "\n",
    "---\n",
    "\n",
    "**Q30. Privacy-preserving NLP?**\n",
    "**A:** Use PII redaction, differential privacy, federated learning, and strict data retention policies. Keep sensitive information isolated from inference logs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Efficiency and Scaling**\n",
    "\n",
    "**Q31. How to speed up Transformer inference?**\n",
    "**A:** Quantize (INT8/INT4), prune redundant weights, use knowledge distillation, caching (for decoding), and frameworks like ONNX/TensorRT.\n",
    "\n",
    "---\n",
    "\n",
    "**Q32. When to distill a model?**\n",
    "**A:** When latency or cost is critical. A smaller ‚Äústudent‚Äù mimics a larger ‚Äúteacher‚Äù model‚Äôs behavior‚Äîretain performance while cutting inference time.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Multilingual NLP**\n",
    "\n",
    "**Q33. How does zero-shot cross-lingual transfer work?**\n",
    "**A:** Multilingual models (like mBERT) use shared subword vocabularies and aligned embedding spaces, so they can generalize knowledge to unseen languages.\n",
    "\n",
    "---\n",
    "\n",
    "**Q34. Tokenization challenges for Indic/CJK scripts?**\n",
    "**A:** These languages lack clear word boundaries. Careful segmentation and Unicode normalization are crucial to avoid token fragmentation and loss of meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **LLM Practicalities**\n",
    "\n",
    "**Q35. Why does context window size matter?**\n",
    "**A:** It limits how much text the model can ‚Äúsee‚Äù at once. Larger windows reduce truncation issues but cost more in compute. Retrieval or summarization can help manage length efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Q36. Guardrails and tool-use orchestration?**\n",
    "**A:** Use structured prompts, function calling with schema validation, and deterministic routing for critical outputs. Always enforce post-processing checks.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Design / Case Scenarios**\n",
    "\n",
    "**Q37. Design a production NER for noisy support tickets.**\n",
    "**A:**\n",
    "\n",
    "1. Define clear annotation guidelines.\n",
    "2. Fine-tune a domain BERT + CRF.\n",
    "3. Add custom dictionaries.\n",
    "4. Handle imbalance via focal loss.\n",
    "5. Deploy with confidence thresholds + human review.\n",
    "6. Continuously monitor drift and re-train.\n",
    "\n",
    "---\n",
    "\n",
    "**Q38. Build a multilingual sentiment model on a budget.**\n",
    "**A:** Use XLM-R with PEFT adapters, augment data via translation, and distill to a smaller model. Calibrate per language and monitor drift regularly.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Coding-style Probes**\n",
    "\n",
    "**Q39. Compute TF-IDF and top features per class.**\n",
    "**A:** Use `TfidfVectorizer` + `LogisticRegression`. Then check model coefficients for top positive/negative n-grams per class. Ensure train/test split avoids leakage.\n",
    "\n",
    "---\n",
    "\n",
    "**Q40. Sketch a simple BPE tokenizer.**\n",
    "**A:**\n",
    "\n",
    "1. Start with a character-level vocab.\n",
    "2. Merge most frequent pairs iteratively.\n",
    "3. Encode by longest-match subwords.\n",
    "4. Keep special tokens and handle unknowns gracefully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7445a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d70b35d0",
   "metadata": {},
   "source": [
    "\n",
    "## üöë **Project 1: US Healthcare Insurance Claim Fraud Analysis**\n",
    "\n",
    "**Q1. What problem were you solving and why NLP?**\n",
    "We aimed to detect fraudulent healthcare claims, where a significant portion of data lies in unstructured text ‚Äî such as procedure notes and diagnosis descriptions. NLP helped extract linguistic and contextual patterns that aren‚Äôt visible in numeric features, thereby improving fraud detection accuracy and reducing manual investigator workload.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. What was your end-to-end pipeline?**\n",
    "Data ingestion (Pandas) ‚Üí text cleaning (normalizing ICD/CPT codes, lowercasing, punctuation removal) ‚Üí feature engineering (TF-IDF n-grams, medical term flags, code co-occurrence stats) ‚Üí model training (XGBoost/CatBoost) ‚Üí PR-AUC-based threshold tuning ‚Üí Streamlit dashboard for investigator insights.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you address class imbalance?**\n",
    "Fraud cases were under 1%. I used **class weighting** and **PR-AUC optimization** to balance recall and precision. Additionally, thresholds were chosen based on business capacity ‚Äî fixing precision at ~90% to manage investigator bandwidth.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. Why XGBoost/CatBoost instead of deep models?**\n",
    "Given sparse tabular + textual features and limited labeled data, gradient boosting models provided a strong balance of accuracy, interpretability, and speed. Deep models like CNN/RNN didn‚Äôt show meaningful uplift but required higher compute and tuning effort.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. Which features contributed the most?**\n",
    "Key signals included rare CPT/ICD code combinations, phrase patterns in provider notes, and claim description rarity. SHAP analysis identified anomalous term clusters and provider-specific behavior as top drivers.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How did you ensure generalization and avoid data leakage?**\n",
    "Splitting was done by provider and time period to prevent leakage across claims. We also excluded post-adjudication fields and used temporal validation to simulate real-world deployment.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. How did you evaluate success with business teams?**\n",
    "We reported **cost savings** = (fraud amount recovered ‚Äì investigation cost ‚Äì false positive load).\n",
    "Operational KPIs included *precision at review capacity* and *hours saved*. A Streamlit dashboard visualized prioritized claims and explainability layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How did you implement explainability?**\n",
    "We integrated SHAP at both global and claim levels, displaying top indicative phrases and anomalous code combinations. The dashboard showed textual highlights and links to similar confirmed fraud cases for investigator trust.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What were the major risks and how did you mitigate them?**\n",
    "\n",
    "* **Data drift:** monthly term drift monitoring and calibration.\n",
    "* **Fairness:** provider-slice performance analysis.\n",
    "* **False accusations:** conservative thresholds + mandatory human review.\n",
    "\n",
    "---\n",
    "\n",
    "**Q10. What would you add with more data or budget?**\n",
    "Integrate **UMLS/SNOMED** for concept normalization, fine-tune **clinical transformer encoders** via PEFT, and incorporate **graph-based relationships** between providers, claims, and procedures.\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ **Project 2: Employee Feedback Sentiment Analysis**\n",
    "\n",
    "**Q1. What was the goal and constraints?**\n",
    "To extract actionable insights from employee exit feedback, highlighting sentiment trends. Constraints included small labeled data, strong privacy requirements, and the need for interpretable dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. Baseline vs improved approach?**\n",
    "Baseline: **TF-IDF + Logistic Regression**.\n",
    "Enhanced version: added **bigrams for negation**, **domain lexicons**, **class-weighting**, and tuned thresholds using **macro-F1** for imbalanced labels.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you handle sarcasm or negation?**\n",
    "Handled negations via bigram features (‚Äúnot satisfied‚Äù), and for ambiguous sarcasm, flagged low-confidence samples for human review instead of overfitting on rare patterns.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What privacy measures were implemented?**\n",
    "Used **spaCy + regex** for PII redaction (names, emails, IDs), aggregated outputs at department level, and applied role-based access control and retention limits.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. Why choose Logistic Regression?**\n",
    "Given limited labeled data, LR was ideal for interpretability and fast iteration. The model‚Äôs coefficients also gave direct insight into drivers of sentiment, which was critical for HR adoption.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How was evaluation and calibration handled?**\n",
    "Used **stratified cross-validation**, tracked **macro/per-class F1**, and applied **Platt scaling** for calibrated probability outputs to support automated alerts.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. What were common model errors?**\n",
    "Domain idioms (e.g., ‚Äúbench time‚Äù) and mixed sentiments within feedback. We mitigated these using domain dictionaries, sentence-level polarity aggregation, and targeted augmentation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. What was the business impact?**\n",
    "Delivered **quarterly sentiment dashboards** by department, identified top linguistic drivers of dissatisfaction, and flagged potential retention risks‚Äîsupporting data-driven HR actions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What would you do next with more time or data?**\n",
    "Fine-tune lightweight **transformer adapters**, add **topic modeling** to uncover themes, and introduce **active learning** loops using HR feedback for incremental labeling.\n",
    "\n",
    "---\n",
    "\n",
    "## üëî **Project 3: Employee‚ÄìProject Alignment Engine (Recommender)**\n",
    "\n",
    "**Q1. What did the system do and why cosine similarity?**\n",
    "It matched employees to projects based on skill alignment. We used **TF-IDF + cosine similarity** since it handles sparse, high-dimensional text well and provides a transparent, scalable baseline.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How were profiles and queries represented?**\n",
    "Employee profiles included concatenated skills, roles, certifications, and recent project text. Projects were similarly vectorized. Synonym normalization and weighted boosts for certifications and recency were applied.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you measure success?**\n",
    "We ran an A/B study ‚Äî **time-to-staff** reduced by 27%, and **match quality** (manager-rated relevance 0‚Äì3) improved by 21%. Offline, we tracked top-k hit rates and ranking correlation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. What ranking adjustments mattered most?**\n",
    "Boosting for certifications, synonym expansion, and decaying scores for outdated experience improved ranking quality. We also enforced diversity to avoid repeatedly surfacing the same candidates.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. How did you validate and iterate?**\n",
    "We gathered qualitative feedback from hiring managers, analyzed misranked cases, tuned similarity weights, and updated domain dictionaries iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. Why not use neural embeddings?**\n",
    "Given latency and limited training data, TF-IDF was sufficient. Neural embeddings (e.g., Sentence-BERT) are in the roadmap to bridge semantic gaps once data volume justifies fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7. How did you handle cold start and data quality?**\n",
    "For new employees, relied on certifications and job titles. Standardized skill taxonomies and ran automated checks on parsed resumes to prevent noise.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How was deployment handled?**\n",
    "Implemented batch vectorization with cached embeddings, incremental updates for new profiles, and exposed via a lightweight API integrated with Streamlit dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Cross-Cutting Interview Probes**\n",
    "\n",
    "**Q1. How did you ensure fairness and avoid bias?**\n",
    "Tracked model performance across provider/department/location slices, conducted disparate impact analysis, and set human review thresholds for sensitive cases.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How did you monitor these systems post-deployment?**\n",
    "Monitored input drift (term distributions), output drift (prediction rates), calibration stability, and business KPIs. Automated alerts triggered retraining when drift exceeded thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How did you communicate results to non-technical stakeholders?**\n",
    "Used **precision‚Äìrecall trade-offs**, **cost‚Äìbenefit visuals**, and **interpretable examples**. Dashboards emphasized business impact‚Äîhours saved, fraud captured, or sentiment shifts.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. How did you ensure data security and privacy?**\n",
    "Implemented **PII redaction**, **role-based access**, **audit trails**, and **aggregate reporting** across all projects, aligning with organizational data governance policies.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. Biggest learning from these projects?**\n",
    "That **alignment between technical metrics and operational goals** (like review capacity or staffing speed) matters more than model accuracy. And continuous error-slice analysis yields the highest ROI over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b502d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
