{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5a332f",
   "metadata": {},
   "source": [
    "Perfect üëç Thanks for the exact structure.\n",
    "Following the same format, here‚Äôs the topic **Loss and Cost Functions** in Jupyter-friendly Markdown + Code format:\n",
    "\n",
    "---\n",
    "\n",
    "## Loss and Cost Functions\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "\n",
    "* **Loss function**: Measures how far a single prediction is from the actual target.\n",
    "* **Cost function**: Aggregates the loss across all training samples (e.g., average loss).\n",
    "* Loss functions guide optimization by telling the model **how wrong** it is.\n",
    "* Choosing the right loss depends on the **task** (regression, binary classification, multi-class classification, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "\n",
    "* **Loss vs Cost**:\n",
    "\n",
    "  * *Loss* ‚Üí error for one observation.\n",
    "  * *Cost* ‚Üí average error over the dataset.\n",
    "* Loss functions are task-dependent:\n",
    "\n",
    "  * Regression ‚Üí MSE, MAE\n",
    "  * Classification ‚Üí Cross-Entropy, Hinge\n",
    "* Differentiability is important for gradient-based optimization.\n",
    "* Robustness to outliers varies: MAE is more robust than MSE.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases / When to Use\n",
    "\n",
    "| Loss Function             | Formula                                    | Typical Use Case                                      |   |                                      |\n",
    "| ------------------------- | ------------------------------------------ | ----------------------------------------------------- | - | ------------------------------------ |\n",
    "| Mean Squared Error (MSE)  | $\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2$      | Regression; penalizes large errors heavily.           |   |                                      |\n",
    "| Mean Absolute Error (MAE) | ( \\frac{1}{n}\\sum                          | y\\_i - \\hat{y}\\_i                                     | ) | Regression; more robust to outliers. |\n",
    "| Huber Loss                | Combines MSE + MAE                         | Regression with outliers; smooth gradient.            |   |                                      |\n",
    "| Binary Cross-Entropy      | $-[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ | Binary classification.                                |   |                                      |\n",
    "| Categorical Cross-Entropy | $-\\sum y_i \\log(\\hat{y}_i)$                | Multi-class classification.                           |   |                                      |\n",
    "| Hinge Loss                | $\\max(0, 1 - y \\cdot \\hat{y})$             | Support Vector Machines, margin-based classification. |   |                                      |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Formulas\n",
    "\n",
    "* **MSE:** $L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "* **MAE:** $L = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "* **Huber Loss:**\n",
    "\n",
    "  $$\n",
    "  L_\\delta(y,\\hat{y}) = \n",
    "  \\begin{cases} \n",
    "  \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "  \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "* **Binary Cross-Entropy:**\n",
    "\n",
    "  $$\n",
    "  L = -\\frac{1}{n} \\sum \\big[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i) \\big]\n",
    "  $$\n",
    "* **Categorical Cross-Entropy:**\n",
    "\n",
    "  $$\n",
    "  L = -\\sum_{i=1}^n \\sum_{c=1}^C y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "  $$\n",
    "* **Hinge Loss:**\n",
    "\n",
    "  $$\n",
    "  L = \\sum \\max(0, 1 - y_i \\cdot \\hat{y}_i)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q\\&A\n",
    "\n",
    "| Question                                       | Answer                                                                                |\n",
    "| ---------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| What is the difference between Loss and Cost?  | Loss ‚Üí error for a single sample, Cost ‚Üí average loss over dataset.                   |\n",
    "| Why is MSE widely used in regression?          | Smooth, differentiable, and penalizes larger errors more strongly.                    |\n",
    "| When is MAE better than MSE?                   | When the dataset has outliers; MAE is more robust.                                    |\n",
    "| What is the advantage of Huber Loss?           | Combines robustness of MAE with smoothness of MSE.                                    |\n",
    "| Why do we use Cross-Entropy in classification? | Measures probability difference between predicted distribution and true distribution. |\n",
    "| Why not use MSE for classification?            | Outputs are probabilities; MSE is not ideal for probability distributions.            |\n",
    "| What is Hinge Loss used for?                   | Margin-based classifiers like Support Vector Machines.                                |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Comparing Loss Functions\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# True value\n",
    "y_true = 3.0\n",
    "\n",
    "# Predictions\n",
    "y_pred = np.linspace(-1, 7, 200)\n",
    "\n",
    "# MSE\n",
    "mse = (y_true - y_pred) ** 2\n",
    "\n",
    "# MAE\n",
    "mae = np.abs(y_true - y_pred)\n",
    "\n",
    "# Huber Loss (Œ¥ = 1.0)\n",
    "delta = 1.0\n",
    "huber = np.where(np.abs(y_true - y_pred) <= delta,\n",
    "                 0.5 * (y_true - y_pred) ** 2,\n",
    "                 delta * np.abs(y_true - y_pred) - 0.5 * delta**2)\n",
    "\n",
    "plt.plot(y_pred, mse, label=\"MSE\", color=\"blue\")\n",
    "plt.plot(y_pred, mae, label=\"MAE\", color=\"red\")\n",
    "plt.plot(y_pred, huber, label=\"Huber Loss\", color=\"green\")\n",
    "\n",
    "plt.title(\"Comparison of Regression Loss Functions\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Cross-Entropy Example\n",
    "y_true_bin = 1\n",
    "y_pred_probs = np.linspace(0.01, 0.99, 100)\n",
    "bce = -(y_true_bin*np.log(y_pred_probs) + (1-y_true_bin)*np.log(1-y_pred_probs))\n",
    "\n",
    "plt.plot(y_pred_probs, bce, label=\"Binary Cross-Entropy\", color=\"purple\")\n",
    "plt.title(\"Binary Cross-Entropy Loss\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c52b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47e007ba",
   "metadata": {},
   "source": [
    "Got it üëç. From now, I‚Äôll provide everything in **Jupyter Notebook markup format** (`Markdown + Python code cells`) so you can directly paste into your notebook without editing.\n",
    "\n",
    "Let‚Äôs properly rewrite **Loss and Cost Functions** in that format (with TensorFlow/Keras examples only).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Loss and Cost Functions in Deep Learning\n",
    "\n",
    "### üß† Theoretical Intuition\n",
    "\n",
    "* **Loss Function**: A function that measures how far the predicted output is from the actual output for a **single training example**.\n",
    "* **Cost Function**: The average of loss values across the **entire training dataset**.\n",
    "* In deep learning, minimizing the loss/cost function helps the model learn better weights.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Commonly Used Loss Functions in Deep Learning\n",
    "\n",
    "| **Loss Function**                    | **Mathematical Intuition**                                                  | **Use Case**                                |   |                          |\n",
    "| ------------------------------------ | --------------------------------------------------------------------------- | ------------------------------------------- | - | ------------------------ |\n",
    "| **Mean Squared Error (MSE)**         | $\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$                               | Regression problems                         |   |                          |\n",
    "| **Mean Absolute Error (MAE)**        | (\\frac{1}{n}\\sum\\_{i=1}^n                                                   | y\\_i - \\hat{y}\\_i                           | ) | Regression with outliers |\n",
    "| **Binary Cross-Entropy**             | $-\\frac{1}{n}\\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$ | Binary classification                       |   |                          |\n",
    "| **Categorical Cross-Entropy**        | $-\\sum y_i \\log(\\hat{y}_i)$                                                 | Multi-class classification (one-hot labels) |   |                          |\n",
    "| **Sparse Categorical Cross-Entropy** | Similar to categorical but uses **integer labels** instead of one-hot       | Multi-class classification (large classes)  |   |                          |\n",
    "| **Huber Loss**                       | Quadratic for small errors, linear for large errors                         | Robust regression with outliers             |   |                          |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Example in TensorFlow/Keras\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, CategoricalCrossentropy\n",
    "\n",
    "# Example data\n",
    "y_true_reg = tf.constant([3.0, -0.5, 2.0, 7.0])   # True values (Regression)\n",
    "y_pred_reg = tf.constant([2.5, 0.0, 2.1, 7.8])    # Predicted values (Regression)\n",
    "\n",
    "y_true_cls = tf.constant([[1, 0, 0], [0, 1, 0]])  # True values (Classification - one hot)\n",
    "y_pred_cls = tf.constant([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1]])  # Predicted probabilities\n",
    "\n",
    "# Regression Loss (MSE)\n",
    "mse = MeanSquaredError()\n",
    "print(\"MSE Loss:\", mse(y_true_reg, y_pred_reg).numpy())\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "bce = BinaryCrossentropy()\n",
    "print(\"Binary Cross-Entropy Loss:\", bce([1, 0, 1], [0.9, 0.1, 0.8]).numpy())\n",
    "\n",
    "# Categorical Cross-Entropy Loss\n",
    "cce = CategoricalCrossentropy()\n",
    "print(\"Categorical Cross-Entropy Loss:\", cce(y_true_cls, y_pred_cls).numpy())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Interview Questions and Answers\n",
    "\n",
    "| **Question**                                                                                           | **Answer**                                                                                       |\n",
    "| ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |\n",
    "| What is the difference between **loss** and **cost** function?                                         | Loss = error for a single sample. Cost = average error across dataset.                           |\n",
    "| Why do we prefer **Cross-Entropy** over **MSE** in classification?                                     | Cross-Entropy penalizes wrong confident predictions more heavily, leading to faster convergence. |\n",
    "| What is the difference between **Categorical Cross-Entropy** and **Sparse Categorical Cross-Entropy**? | Categorical requires **one-hot encoded labels**, Sparse works with **integer labels**.           |\n",
    "| When to use **Huber Loss**?                                                                            | When you want a balance between MSE (sensitive to outliers) and MAE (robust to outliers).        |\n",
    "| Can we use MSE for classification?                                                                     | Not recommended, because it leads to slow convergence and poor probability calibration.          |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f218847",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
