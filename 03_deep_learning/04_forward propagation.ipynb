{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb045ddb",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "- **Forward propagation** is the process of passing input data through a neural network to obtain an output.  \n",
    "- Each layer computes **weighted sum + bias**, applies **activation function**, and passes the result to the next layer.  \n",
    "- It’s the first step before computing loss and performing backpropagation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "- Happens **layer by layer**, from input to output.  \n",
    "- Computes **activations** for all neurons.  \n",
    "- Used to **predict outputs** for given inputs.  \n",
    "- **Does not update weights**; purely a computation step.  \n",
    "- Helps visualize **how input transforms through network layers**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "- Any prediction task using ANN / MLP  \n",
    "- Debugging network outputs layer by layer  \n",
    "- Teaching / understanding neural network behavior  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Intuition\n",
    "For layer \\(l\\):  \n",
    "\n",
    "\\[\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "\\]  \n",
    "\n",
    "\\[\n",
    "a^{(l)} = \\sigma(z^{(l)})\n",
    "\\]  \n",
    "\n",
    "Where:  \n",
    "- \\(a^{(0)} = X\\) (input layer)  \n",
    "- \\(\\sigma\\) = activation function (ReLU, Sigmoid, etc.)  \n",
    "\n",
    "Output layer gives final prediction \\(y_{\\text{pred}} = a^{(L)}\\)  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q&A\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| What is forward propagation? | Passing input through network layer by layer to compute output. |\n",
    "| Does forward propagation update weights? | No, it only computes activations and outputs. |\n",
    "| Why is forward propagation important? | It calculates the predicted output and is necessary for computing loss. |\n",
    "| What is the formula for forward propagation? | z = W·a_prev + b; a = activation(z) |\n",
    "| Can forward propagation be used for debugging? | Yes, by checking intermediate activations for each layer. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Forward Propagation Step-by-Step\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Input Data\n",
    "X = torch.tensor([[0.5, 1.0],\n",
    "                  [1.5, -0.5],\n",
    "                  [-1.0, 2.0]], dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Define network parameters manually\n",
    "# Input layer: 2 neurons, Hidden layer: 3 neurons, Output: 1 neuron\n",
    "W1 = torch.tensor([[0.1, 0.2, -0.1],\n",
    "                   [0.4, -0.3, 0.2]], dtype=torch.float32)\n",
    "b1 = torch.tensor([0.0, 0.1, -0.1], dtype=torch.float32)\n",
    "\n",
    "W2 = torch.tensor([[0.2],\n",
    "                   [-0.5],\n",
    "                   [0.3]], dtype=torch.float32)\n",
    "b2 = torch.tensor([0.05], dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ Forward Propagation\n",
    "\n",
    "# Layer 1: Hidden layer\n",
    "z1 = torch.matmul(X, W1) + b1    # Linear combination\n",
    "a1 = torch.relu(z1)              # ReLU activation\n",
    "print(\"Hidden layer output (a1):\\n\", a1)\n",
    "\n",
    "# Layer 2: Output layer\n",
    "z2 = torch.matmul(a1, W2) + b2   # Linear combination\n",
    "y_pred = z2                       # No activation for regression\n",
    "print(\"Output layer prediction (y_pred):\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5096c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
