{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1dd2e28",
   "metadata": {},
   "source": [
    "## Activation Functions and When to Use Which\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "- Activation functions introduce **non-linearity** into neural networks.  \n",
    "- Without them, multiple layers collapse into a single linear transformation.  \n",
    "- Each neuron decides **whether to activate** based on its input.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "- **Sigmoid**: Output between 0 and 1, saturates at extremes.  \n",
    "- **Tanh**: Output between -1 and 1, zero-centered.  \n",
    "- **ReLU**: Output = max(0, x), avoids vanishing gradient for positive inputs.  \n",
    "- **Leaky ReLU**: Allows small negative values to prevent dead neurons.  \n",
    "- **Softmax**: Converts vector into probability distribution for multi-class classification.  \n",
    "- **Choosing activation** depends on **layer type** (hidden/output), **problem type**, and **training stability**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases / When to Use\n",
    "\n",
    "| Activation Function | Typical Use Case / Notes |\n",
    "|--------------------|-------------------------|\n",
    "| Sigmoid | Output layer for **binary classification** (probability 0–1); avoid in deep hidden layers due to vanishing gradient. |\n",
    "| Tanh | Hidden layers when zero-centered output is desired; small networks; avoids bias shift. |\n",
    "| ReLU | Most hidden layers in **deep networks**; fast computation; mitigates vanishing gradient for positive inputs. |\n",
    "| Leaky ReLU | Hidden layers to prevent “dead ReLU” problem where neurons output 0 constantly. |\n",
    "| Softmax | Output layer for **multi-class classification**; provides probability distribution across classes. |\n",
    "| Linear (Identity) | Output layer for **regression tasks**; no non-linearity needed. |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Formulas\n",
    "- **Sigmoid:** \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)  \n",
    "- **Tanh:** \\( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)  \n",
    "- **ReLU:** \\( f(x) = \\max(0, x) \\)  \n",
    "- **Leaky ReLU:** \\( f(x) = x \\text{ if } x > 0 \\text{ else } 0.01x \\)  \n",
    "- **Softmax:** \\( \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\)  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q&A\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| Why do we need activation functions? | To introduce non-linearity, enabling networks to model complex patterns. |\n",
    "| When is Sigmoid used? | Binary classification output layer. |\n",
    "| Why avoid Sigmoid in hidden layers? | Can cause vanishing gradient, slowing training in deep networks. |\n",
    "| When is Tanh preferred over Sigmoid? | When zero-centered output is desired; reduces bias shift. |\n",
    "| Why ReLU is popular in hidden layers? | Simple, computationally fast, mitigates vanishing gradient for positive inputs. |\n",
    "| What problem does Leaky ReLU solve? | Prevents dead neurons that never activate. |\n",
    "| When is Softmax used? | Multi-class classification output layer; gives probability distribution. |\n",
    "| Which activation is used in regression output? | Linear (identity) activation. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Plot Activation Functions Step-by-Step\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input values\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, sigmoid, label='Sigmoid', color='blue')\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "plt.plot(x, tanh, label='Tanh', color='red')\n",
    "\n",
    "# ReLU\n",
    "relu = np.maximum(0, x)\n",
    "plt.plot(x, relu, label='ReLU', color='green')\n",
    "\n",
    "# Leaky ReLU\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
    "plt.plot(x, leaky_relu, label='Leaky ReLU', color='purple')\n",
    "\n",
    "plt.title(\"Activation Functions\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Softmax example\n",
    "softmax_input = np.array([2.0, 1.0, 0.1])\n",
    "exp_vals = np.exp(softmax_input)\n",
    "softmax_output = exp_vals / np.sum(exp_vals)\n",
    "\n",
    "print(\"Softmax Input:\", softmax_input)\n",
    "print(\"Softmax Probabilities:\", softmax_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53259386",
   "metadata": {},
   "source": [
    "## Activation Functions Reference Table\n",
    "\n",
    "| Activation Function | Pros | Cons | Ideal Usage / Notes |\n",
    "|--------------------|------|------|-------------------|\n",
    "| **Sigmoid** | Smooth gradient, outputs 0–1 (probabilities) | Vanishing gradient for large positive/negative inputs; not zero-centered | Output layer for **binary classification**; avoid in deep hidden layers |\n",
    "| **Tanh** | Zero-centered, smooth gradient | Vanishing gradient for large inputs | Hidden layers in small networks; when zero-centered output is needed |\n",
    "| **ReLU** | Computationally efficient, mitigates vanishing gradient for positives, sparse activation | Dead neurons if input < 0 (never activates) | Hidden layers in **deep networks**; most commonly used |\n",
    "| **Leaky ReLU** | Fixes dead neuron problem by allowing small negative slope | Slightly more computation than ReLU | Hidden layers when some neurons might die with ReLU |\n",
    "| **Parametric ReLU (PReLU)** | Learns negative slope during training | Extra parameters increase complexity | Deep hidden layers where flexibility is needed |\n",
    "| **ELU (Exponential Linear Unit)** | Smooth output for negatives, reduces bias shift | Slightly slower computation | Hidden layers in deep networks; helps faster learning |\n",
    "| **Softmax** | Converts outputs to probability distribution; differentiable | Can saturate if input differences are large | Output layer for **multi-class classification** |\n",
    "| **Linear / Identity** | Simple; no non-linearity | Cannot model complex patterns | Output layer for **regression tasks** |\n",
    "| **Swish** | Smooth, non-monotonic, better gradient flow | Slightly slower than ReLU | Deep networks; sometimes improves accuracy over ReLU |\n",
    "| **GELU (Gaussian Error Linear Unit)** | Smooth, non-linear, used in Transformers | More computation than ReLU | Hidden layers in **Transformer-based networks** like BERT |\n",
    "\n",
    "---\n",
    "\n",
    "### Tips for Choosing Activation Functions\n",
    "- Hidden layers: usually **ReLU or variants** (Leaky ReLU, ELU, GELU)  \n",
    "- Output layer depends on task:  \n",
    "  - Binary classification → **Sigmoid**  \n",
    "  - Multi-class classification → **Softmax**  \n",
    "  - Regression → **Linear**  \n",
    "- Avoid Sigmoid/Tanh in very deep networks to reduce vanishing gradient problem  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d810a0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
