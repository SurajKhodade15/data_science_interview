{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6327468",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (ANN) / Multi-Layer Perceptron (MLP)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "- An **ANN** is a network of interconnected neurons (nodes), inspired by the human brain.  \n",
    "- **Multi-Layer Perceptron (MLP)** is an ANN with **one or more hidden layers** between input and output.  \n",
    "- Each neuron performs a **weighted sum of inputs**, adds a **bias**, and applies an **activation function**.  \n",
    "- MLPs can solve **non-linear problems** that a single Perceptron cannot.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "- **Layers in ANN / MLP**:  \n",
    "  1. **Input Layer**: Receives features from data  \n",
    "  2. **Hidden Layer(s)**: Perform computations & learn features  \n",
    "  3. **Output Layer**: Produces prediction / classification  \n",
    "- **Activation Functions** introduce non-linearity (ReLU, Sigmoid, Tanh, etc.)  \n",
    "- **Feedforward**: Inputs → hidden layers → output  \n",
    "- **Backpropagation**: Algorithm to update weights using gradient descent  \n",
    "- **MLPs are universal function approximators** — can model any continuous function given enough neurons and data  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "- Image recognition  \n",
    "- Speech recognition  \n",
    "- NLP tasks: sentiment analysis, text classification  \n",
    "- Fraud detection in finance  \n",
    "- Any structured/tabular prediction task  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Intuition\n",
    "- **Forward pass for a layer**:\n",
    "\n",
    "\\[\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "a^{(l)} = \\sigma(z^{(l)})\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\(l\\) = layer index  \n",
    "- \\(W^{(l)}\\), \\(b^{(l)}\\) = weights & biases  \n",
    "- \\(a^{(l-1)}\\) = activation from previous layer  \n",
    "- \\(\\sigma\\) = activation function  \n",
    "\n",
    "- **Output layer** produces final prediction  \n",
    "- **Backpropagation** uses **chain rule** to compute gradients and update weights:  \n",
    "\n",
    "\\[\n",
    "W^{(l)} := W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "\\]  \n",
    "\n",
    "where \\(L\\) is the loss function, \\(\\eta\\) is learning rate  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q&A\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| What is an Artificial Neural Network (ANN)? | A network of interconnected neurons that process data similar to the human brain. |\n",
    "| What is a Multi-Layer Perceptron (MLP)? | An ANN with one or more hidden layers capable of solving non-linear problems. |\n",
    "| Why do we need hidden layers? | Hidden layers allow the network to learn complex, non-linear patterns in data. |\n",
    "| What is the role of activation functions in MLP? | Introduce non-linearity, enabling the network to approximate complex functions. |\n",
    "| What is backpropagation? | Algorithm to compute gradients of loss w.r.t weights and update them using optimization. |\n",
    "| What is feedforward in an MLP? | The process of passing input through all layers to compute output predictions. |\n",
    "| Can MLP solve XOR problem? | Yes, unlike a single Perceptron, an MLP with hidden layer can solve XOR. |\n",
    "| What is the universal approximation theorem? | MLPs with at least one hidden layer can approximate any continuous function given enough neurons. |\n",
    "| What are common activation functions used? | Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax (output layer for classification). |\n",
    "| How are weights initialized in MLP? | Usually small random values (Xavier/He initialization) to avoid symmetry issues. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Simple MLP in PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset: y = x1 + x2\n",
    "X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0],[1],[1],[2]], dtype=torch.float32)\n",
    "\n",
    "# Define MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 5)   # input -> hidden\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)   # hidden -> output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize\n",
    "model = SimpleMLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    for xi in X:\n",
    "        print(f\"Input: {xi.numpy()}, Predicted: {model(xi).item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1050eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3698bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions after training:\n",
      "Input: [0. 0.], Predicted: 1.00, True: 0.0\n",
      "Input: [0. 1.], Predicted: 1.00, True: 1.0\n",
      "Input: [1. 0.], Predicted: 1.00, True: 1.0\n",
      "Input: [1. 1.], Predicted: 1.00, True: 2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Prepare dataset (simple sum example)\n",
    "# Inputs: x1, x2\n",
    "X = torch.tensor([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]], dtype=torch.float32)\n",
    "\n",
    "# Outputs: y = x1 + x2\n",
    "y = torch.tensor([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [2]], dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Define network structure\n",
    "# Input layer: 2 neurons\n",
    "# Hidden layer: 5 neurons\n",
    "# Output layer: 1 neuron\n",
    "\n",
    "hidden_neurons = 5\n",
    "\n",
    "# Initialize weights and biases manually for interpretability\n",
    "W1 = torch.randn((2, hidden_neurons), requires_grad=True)\n",
    "b1 = torch.randn((hidden_neurons,), requires_grad=True)\n",
    "W2 = torch.randn((hidden_neurons, 1), requires_grad=True)\n",
    "b2 = torch.randn((1,), requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ Training loop (500 epochs)\n",
    "for epoch in range(500):\n",
    "    \n",
    "    # Forward pass\n",
    "    hidden_input = torch.matmul(X, W1) + b1       # Linear for hidden layer\n",
    "    hidden_output = torch.relu(hidden_input)      # ReLU activation\n",
    "    \n",
    "    output_input = torch.matmul(hidden_output, W2) + b2   # Linear for output layer\n",
    "    y_pred = output_input                            # No activation (regression)\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = ((y - y_pred)**2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights manually (SGD)\n",
    "    with torch.no_grad():\n",
    "        W1 -= lr * W1.grad\n",
    "        b1 -= lr * b1.grad\n",
    "        W2 -= lr * W2.grad\n",
    "        b2 -= lr * b2.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    W1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    W2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "\n",
    "# ----------------------------\n",
    "# 4️⃣ Predictions\n",
    "print(\"Predictions after training:\")\n",
    "for i, xi in enumerate(X):\n",
    "    # Forward pass for each input\n",
    "    h_input = torch.matmul(xi, W1) + b1\n",
    "    h_output = torch.relu(h_input)\n",
    "    out = torch.matmul(h_output, W2) + b2\n",
    "    print(f\"Input: {xi.numpy()}, Predicted: {out.item():.2f}, True: {y[i].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a0504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
