{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191adb38",
   "metadata": {},
   "source": [
    "## Backward Propagation (Backpropagation)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Theoretical Intuition\n",
    "- **Backpropagation** is the process of calculating **gradients of the loss function** with respect to each weight in the network.  \n",
    "- Used to **update weights** during training to minimize the loss.  \n",
    "- Works by **propagating the error backward** from output to input layers.  \n",
    "- Relies on the **chain rule** from calculus.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Pointers\n",
    "- Step after **forward propagation** and computing **loss**  \n",
    "- Updates all weights and biases **layer by layer**  \n",
    "- Fundamental to training **deep neural networks**  \n",
    "- Uses **learning rate** to scale weight updates  \n",
    "- Combines with **gradient descent** or its variants  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "- Training any **ANN / MLP**  \n",
    "- Solving regression or classification problems  \n",
    "- Optimizing network performance on datasets  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Intuition\n",
    "For a simple 2-layer network:  \n",
    "\n",
    "1. Compute loss: \\( L = \\frac{1}{2}(y - \\hat{y})^2 \\)  \n",
    "2. Output layer gradient:  \n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W_2} = a_1^T \\cdot (\\hat{y} - y)\n",
    "\\]  \n",
    "3. Hidden layer gradient:  \n",
    "\\[\n",
    "\\delta_1 = ( \\hat{y} - y) \\cdot W_2^T \\cdot \\sigma'(z_1)\n",
    "\\]  \n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\delta_1\n",
    "\\]  \n",
    "\n",
    "- Update rule (SGD): \\( W := W - \\eta \\cdot \\frac{\\partial L}{\\partial W} \\)  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q&A\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| What is backpropagation? | Algorithm to compute gradients of the loss function w.r.t weights for training. |\n",
    "| Why do we use backpropagation? | To update weights and minimize loss using gradient descent. |\n",
    "| What is the chain rule in backpropagation? | It calculates the derivative of composite functions layer by layer. |\n",
    "| Does backpropagation work for deep networks? | Yes, it is essential for training deep neural networks. |\n",
    "| What are the main steps in backpropagation? | 1. Forward pass 2. Compute loss 3. Compute gradients 4. Update weights |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Demo: Step-by-Step Backpropagation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Input and output\n",
    "X = torch.tensor([[0.5, 1.0],\n",
    "                  [1.5, -0.5]], dtype=torch.float32)\n",
    "y = torch.tensor([[1.0],\n",
    "                  [0.0]], dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Initialize weights and biases\n",
    "W1 = torch.randn((2, 3), requires_grad=True)   # Input -> Hidden\n",
    "b1 = torch.randn((3,), requires_grad=True)\n",
    "W2 = torch.randn((3, 1), requires_grad=True)   # Hidden -> Output\n",
    "b2 = torch.randn((1,), requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ Forward Propagation\n",
    "z1 = torch.matmul(X, W1) + b1\n",
    "a1 = torch.relu(z1)             # Hidden layer activation\n",
    "\n",
    "z2 = torch.matmul(a1, W2) + b2\n",
    "y_pred = z2                     # Output layer (regression)\n",
    "\n",
    "# Compute loss (MSE)\n",
    "loss = ((y - y_pred)**2).mean()\n",
    "print(\"Loss before backpropagation:\", loss.item())\n",
    "\n",
    "# ----------------------------\n",
    "# 4️⃣ Backward Propagation\n",
    "loss.backward()   # Compute gradients automatically\n",
    "\n",
    "# Print gradients\n",
    "print(\"\\nGradients:\")\n",
    "print(\"dL/dW2:\\n\", W2.grad)\n",
    "print(\"dL/db2:\\n\", b2.grad)\n",
    "print(\"dL/dW1:\\n\", W1.grad)\n",
    "print(\"dL/db1:\\n\", b1.grad)\n",
    "\n",
    "# ----------------------------\n",
    "# 5️⃣ Update weights manually (SGD)\n",
    "with torch.no_grad():\n",
    "    W1 -= lr * W1.grad\n",
    "    b1 -= lr * b1.grad\n",
    "    W2 -= lr * W2.grad\n",
    "    b2 -= lr * b2.grad\n",
    "\n",
    "    # Zero gradients\n",
    "    W1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    W2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "\n",
    "# ----------------------------\n",
    "# 6️⃣ Forward pass after update\n",
    "a1_new = torch.relu(torch.matmul(X, W1) + b1)\n",
    "y_pred_new = torch.matmul(a1_new, W2) + b2\n",
    "loss_new = ((y - y_pred_new)**2).mean()\n",
    "print(\"\\nLoss after one update:\", loss_new.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec2136",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
