{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aeeed5",
   "metadata": {},
   "source": [
    "# üìò Retrieval-Augmented Generation (RAG) ‚Äî End-to-End Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Executive Summary**\n",
    "- **Definition:** RAG is a GenAI technique where an LLM is augmented with external knowledge retrieved from a database, documents, or APIs.  \n",
    "- **Purpose:** Reduce hallucinations, improve factual accuracy, and allow models to handle larger knowledge bases than their context window.  \n",
    "- **Where it fits:** Used in chatbots, domain-specific QA systems, document summarization, and multi-step reasoning pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Conceptual Theory (Deep Dive)**\n",
    "\n",
    "| Concept | Definition | Key Intuition | Trade-offs |\n",
    "|---------|-----------|---------------|------------|\n",
    "| **Retrieval** | Fetching relevant knowledge chunks based on query | ‚ÄúGive the LLM the right context to reason‚Äù | +Accurate outputs / ‚ÄìLatency from DB calls |\n",
    "| **Augmentation** | Combine retrieved docs with user prompt | ‚ÄúFeed context to LLM before generation‚Äù | +Grounded responses / ‚ÄìToken usage increases |\n",
    "| **Generation** | LLM produces output using augmented context | ‚ÄúModel predicts answer using both memory and retrieval‚Äù | +Factuality / ‚ÄìDepends on retrieval quality |\n",
    "| **Vector DBs** | Stores embeddings for similarity search | Efficient semantic search | +Scalable / ‚ÄìStorage & maintenance cost |\n",
    "| **Chunking** | Split large documents for retrieval | Fit into LLM context window | +Enables long documents / ‚ÄìToo small chunks lose coherence |\n",
    "\n",
    "**Core Workflow**\n",
    "1. User query received.  \n",
    "2. Embed the query into a vector space.  \n",
    "3. Retrieve top-k relevant documents or text chunks.  \n",
    "4. Augment LLM input with retrieved knowledge.  \n",
    "5. Generate output.  \n",
    "6. Optional: Store conversation/output in memory for context.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Practical Usage Patterns**\n",
    "\n",
    "| Use Case | Components Involved | Notes |\n",
    "|---------|------------------|------|\n",
    "| Enterprise QA | LLM + Vector DB + Retriever | Ground answers on internal docs |\n",
    "| Chatbot | LLM + Memory + RAG | Keep conversation context while retrieving knowledge |\n",
    "| Document Summarization | LLM + Retriever + Chains | Retrieve relevant sections before summarizing |\n",
    "| Customer Support | LLM + RAG + Agents | Combine retrieval with automated tool execution |\n",
    "| Domain-specific research | Domain-specific embeddings + LLM | Reduces hallucinations in specialized domains |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Interview Questions & Answers**\n",
    "\n",
    "| Question | Answer |\n",
    "|---------|--------|\n",
    "| What is RAG? | Retrieval-Augmented Generation is a technique where external knowledge is retrieved and fed to an LLM to improve factual accuracy. |\n",
    "| Why use RAG instead of plain LLMs? | LLMs have limited context and may hallucinate; RAG allows grounded answers using real knowledge. |\n",
    "| What are vector databases in RAG? | Databases like FAISS, Pinecone, or Chroma store embeddings for semantic search and efficient retrieval. |\n",
    "| How do you choose the number of retrieved documents? | Balance relevance and cost: top-k usually 3‚Äì10 documents; depends on task complexity. |\n",
    "| What is chunking? | Splitting large documents into smaller segments so they fit LLM‚Äôs context window for retrieval. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Python Example ‚Äî RAG Pipeline**\n",
    "\n",
    "```python\n",
    "# pip install langchain openai faiss-cpu\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1) Load documents\n",
    "loader = PyPDFLoader(\"sample_doc.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "# 2) Generate embeddings and create vector DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 3) Create retriever\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# 4) Initialize LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# 5) Build RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# 6) Run query\n",
    "query = \"Explain the key insights from the PDF about LangChain.\"\n",
    "result = rag_chain.run(query)\n",
    "print(result)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Best Practices**\n",
    "\n",
    "* **Chunking:** 500‚Äì1000 tokens per chunk for LLMs with limited context.\n",
    "* **Top-k Retrieval:** Adjust number of retrieved docs to balance relevance vs token usage.\n",
    "* **Embedding Choice:** Choose embedding model based on domain and semantic fidelity.\n",
    "* **Caching:** Cache frequently retrieved queries to reduce latency and cost.\n",
    "* **Memory Integration:** Combine RAG with conversation memory for multi-turn chat applications.\n",
    "* **Evaluation:** Periodically verify retrieval relevance and output quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Quick Summary**\n",
    "\n",
    "* RAG = **Retrieve ‚Üí Augment ‚Üí Generate**\n",
    "* Reduces hallucinations and enables LLMs to access large knowledge bases.\n",
    "* Key Components: **Embeddings, Vector DB, Retriever, LLM, Memory**\n",
    "* Ideal for **chatbots, enterprise QA, summarization, and domain-specific reasoning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990e8c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
