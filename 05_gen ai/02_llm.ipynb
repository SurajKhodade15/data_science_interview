{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72da0d51",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs) — End-to-End Notes\n",
    "\n",
    "## 1) Executive Summary\n",
    "- **What it is:** Large Language Models (LLMs) are deep learning models trained on massive text corpora to understand and generate human-like natural language.\n",
    "- **Why it matters:** They enable conversational AI, code generation, summarization, translation, and act as the foundation for GenAI applications.\n",
    "- **Where it fits:** Core reasoning/generation layer in GenAI stacks, powering copilots, RAG systems, and agent frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Conceptual Theory (Deep Dive)\n",
    "| Concept | Definition | Key Intuition | Math/Mechanics | Trade-offs |\n",
    "|---|---|---|---|---|\n",
    "| Transformer | Neural architecture with attention | “Look at all words at once” | Self-attention: Softmax(QKᵀ/√d)V | +Scales well / –Expensive |\n",
    "| Pretraining | Train on unlabeled data | Learn world knowledge | Minimize cross-entropy loss (next token prediction) | +Rich representations / –Huge cost |\n",
    "| Fine-tuning | Adapt pretrained LLM | Domain/task-specific | Gradient descent on smaller datasets | +Customization / –Risk of forgetting |\n",
    "| Prompting | Instructions to steer LLM | \"Talking to the model\" | Zero-shot, few-shot, chain-of-thought | +Flexible / –Prompt fragility |\n",
    "| Alignment | Align model with human values | Make outputs safe/useful | RLHF, DPO, Constitutional AI | +Safety / –Complex training |\n",
    "\n",
    "**Core Workflow**\n",
    "1. **Pretraining:** Learn general world + language knowledge.\n",
    "2. **Fine-tuning:** Adapt to tasks (summarization, Q&A, coding).\n",
    "3. **Prompting/Instruction tuning:** Guide model behavior.\n",
    "4. **Inference/Serving:** Deploy via API, integrate with tools.\n",
    "5. **Evaluation:** Check for quality, safety, efficiency.\n",
    "\n",
    "**Common Pitfalls & Anti-Patterns**\n",
    "- Hallucination → *Mitigation:* Retrieval-Augmented Generation (RAG).\n",
    "- Token overuse → *Mitigation:* Chunking + embeddings.\n",
    "- Bias in training data → *Mitigation:* Bias audits, RLHF.\n",
    "- Latency/cost blowouts → *Mitigation:* Model distillation, caching.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Practical Usage & Architecture Patterns\n",
    "| Use Case | Input | Process | Output | KPIs/SLAs | Notes |\n",
    "|---|---|---|---|---|---|\n",
    "| Q&A Systems | Natural query | Prompt → LLM | Answer | Accuracy, Latency | Add retrieval grounding |\n",
    "| Summarization | Long text | Chunk → Prompt → LLM | Concise summary | Rouge, Latency | Watch context length |\n",
    "| Code Generation | Partial code | Prompt → LLM | Completed code | Compile success, BLEU | Guardrails critical |\n",
    "| Conversational Agent | Dialogue | Memory + Prompt | Natural reply | CSAT, Latency <1s | Needs persona + guardrails |\n",
    "\n",
    "**Reference Architecture (Text)**\n",
    "- **Input Layer:** User prompt / system instructions  \n",
    "- **Pre-Processing:** Chunking, embedding (optional)  \n",
    "- **Core Model:** LLM (OpenAI GPT, LLaMA, Mistral)  \n",
    "- **Orchestration:** LangChain, LlamaIndex  \n",
    "- **Retrieval Layer (Optional):** FAISS, Pinecone, Chroma  \n",
    "- **Output Layer:** API, chatbot UI, agents  \n",
    "\n",
    "**Operational Hardening Checklist**\n",
    "- [ ] Prompt library versioning  \n",
    "- [ ] Token usage monitoring  \n",
    "- [ ] Eval set (factuality, toxicity, bias)  \n",
    "- [ ] Rate limiting & caching  \n",
    "- [ ] Safety filters + PII scrubbing  \n",
    "\n",
    "---\n",
    "\n",
    "## 4) Interview Questions & Model Answers\n",
    "| Question | Strong Answer (Concise) |\n",
    "|---|---|\n",
    "| What is an LLM? | A transformer-based model with billions of parameters trained on massive text data to predict and generate human-like text. |\n",
    "| How do LLMs differ from traditional NLP models? | LLMs use self-attention, scale massively, handle diverse tasks with few/zero-shot learning, unlike task-specific smaller models. |\n",
    "| Why is attention important? | It allows the model to focus on relevant parts of input sequence regardless of distance, improving context handling. |\n",
    "| How do you reduce hallucinations? | Retrieval-Augmented Generation (RAG), prompt engineering, grounding, or fine-tuning with factual data. |\n",
    "| What is RLHF? | Reinforcement Learning from Human Feedback—used to align LLMs with human preferences for safety and usefulness. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Python — Minimal Working Example (HuggingFace LLM)\n",
    "\n",
    "```python\n",
    "# pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small demo model for LLM text generation\n",
    "llm = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain why large language models are important in AI.\"\n",
    "\n",
    "# Generate\n",
    "outputs = llm(prompt, max_length=80, num_return_sequences=1)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Additional Intelligence (Tips, Benchmarks, Gotchas)\n",
    "\n",
    "* **Performance heuristics:** Use smaller distilled models for fast prototyping; larger models for accuracy.\n",
    "* **Scaling guidance:** Use LoRA / PEFT for fine-tuning instead of full retraining.\n",
    "* **Cost levers:** Cache frequent responses, reduce max token length, batch requests.\n",
    "* **Security/Compliance:** Prevent leakage of secrets/PII, log and audit prompts.\n",
    "* **Evals:** Use factuality, toxicity, helpfulness scorecards; track drift in outputs.\n",
    "* **Alternatives/Comparisons:** GPT (OpenAI), LLaMA (Meta), Claude (Anthropic), Mistral (open-source lightweight).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) One-Page Cheat Sheet\n",
    "\n",
    "* **LLM = Pretrained transformer on massive text**\n",
    "* **APIs:** `transformers.pipeline`, `openai.ChatCompletion`\n",
    "* **Failure Modes:** Hallucinations, bias, long latency\n",
    "* **Fixes:** RAG, fine-tuning, alignment methods\n",
    "* **Mental Model:** \"LLMs = Next-word predictors that scale into reasoning engines\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d6b62",
   "metadata": {},
   "source": [
    "| Type | Definition | Key Intuition | Examples | Trade-offs |\n",
    "|---|---|---|---|---|\n",
    "| **Decoder-only (Autoregressive)** | Predict next token given past | \"Write forward like autocomplete\" | GPT family, LLaMA, Mistral | +Great for generation / –Poor bidirectional context |\n",
    "| **Encoder-only** | Learn contextual representations | \"Read + understand, not generate\" | BERT, RoBERTa | +Great for embeddings / –Not generative |\n",
    "| **Encoder–Decoder (Seq2Seq)** | Encode input, then decode output | \"Translate from input space to output space\" | T5, BART | +Summarization/translation / –Heavier |\n",
    "| **General-purpose LLMs** | Trained broadly, open-domain | \"Swiss army knife\" | GPT-4, Claude, LLaMA | +Versatile / –Expensive |\n",
    "| **Domain-specific LLMs** | Trained/tuned for industries | \"Specialist doctor\" | BloombergGPT (finance), BioGPT (biomed) | +Domain accuracy / –Narrow scope |\n",
    "| **Open-source LLMs** | Public weights available | \"Customizable, local\" | LLaMA 2, Mistral, Falcon | +Control, cost / –Support burden |\n",
    "| **Closed-source LLMs** | Proprietary APIs only | \"Pay to use as a service\" | GPT-4, Claude, Gemini | +Easy access / –Vendor lock-in |\n",
    "| **Multimodal LLMs** | Handle text + images/audio/code | \"See + read + reason\" | GPT-4V, Gemini, Kosmos | +Rich tasks / –Compute-heavy |\n",
    "\n",
    "**Core Workflow**\n",
    "1. **Choose family:** Decoder-only (generation) vs Encoder-only (analysis).\n",
    "2. **Decide scope:** General-purpose vs domain-specific.\n",
    "3. **Pick accessibility:** Open vs closed.\n",
    "4. **Decide modality:** Text-only vs multimodal.\n",
    "\n",
    "**Common Pitfalls**\n",
    "- Using closed APIs without cost guardrails → *Mitigation:* caching, batching.  \n",
    "- Overusing general LLMs for domain tasks → *Mitigation:* domain fine-tuning.  \n",
    "- Ignoring privacy needs → *Mitigation:* self-hosted open-source LLMs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b599ae",
   "metadata": {},
   "source": [
    "5) Python — Minimal Working Example (Comparing Two Types)\n",
    "\n",
    "```python\n",
    "# pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Decoder-only (generation)\n",
    "gen = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(\"\\nDecoder-only Output:\")\n",
    "print(gen(\"AI in healthcare can\", max_length=40)[0][\"generated_text\"])\n",
    "\n",
    "# Encoder-only (embeddings) — using sentence-transformers style\n",
    "from sentence_transformers import SentenceTransformer\n",
    "enc = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vec = enc.encode(\"AI in healthcare can improve diagnosis.\")\n",
    "print(\"\\nEncoder-only Vector Shape:\", vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53fa99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
