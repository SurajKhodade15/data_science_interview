{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6127bb",
   "metadata": {},
   "source": [
    "# ðŸ“˜ LangChain Data Pipeline â€” End-to-End Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Executive Summary**\n",
    "- **Purpose:** GenAI apps require structured access to external knowledge. The pipeline ensures data is **ingested, split, embedded, and stored efficiently** for LLM consumption.  \n",
    "- **Stages:**  \n",
    "  1. Data Ingestion  \n",
    "  2. Text Splitting  \n",
    "  3. Embedding Generation  \n",
    "  4. Vector Store Database (Vector DB)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Data Ingestion Techniques**\n",
    "\n",
    "| Technique | Description | Example Use Case | Notes |\n",
    "|-----------|------------|-----------------|------|\n",
    "| **Document Loader** | Reads structured/unstructured documents | PDF, Word, HTML | Supports splitting and metadata extraction |\n",
    "| **Web Scraping** | Extracts data from web pages | Company FAQ, product docs | Use BeautifulSoup, Selenium |\n",
    "| **API Ingestion** | Fetch data from REST/GraphQL APIs | Financial, weather, CRM data | Requires JSON parsing and rate limiting |\n",
    "| **Database Loader** | Fetches data from SQL/NoSQL DBs | Customer records, transactions | Use pandas + SQLAlchemy, MongoClient |\n",
    "| **Custom Loader** | Build your own loader for specific formats | CSV, TXT, Logs | Useful when format is non-standard |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Text Splitting Techniques**\n",
    "\n",
    "| Technique | Description | LangChain Component | Example |\n",
    "|-----------|------------|------------------|--------|\n",
    "| **Recursive Character Splitter** | Splits text recursively by chars, respects chunk size | `RecursiveCharacterTextSplitter` | Split a long PDF into 1000-token chunks |\n",
    "| **Markdown/Text Splitter** | Splits based on headings or newlines | `MarkdownTextSplitter`, `CharacterTextSplitter` | Suitable for structured docs |\n",
    "| **Sentence Splitter** | Splits based on sentences | `NltkSentenceSplitter` or custom | Fine-grained chunking for precise retrieval |\n",
    "| **Token-based Splitter** | Split by token count (for LLM context) | `TokenTextSplitter` | Fits LLM max token limits |\n",
    "\n",
    "**Notes:**  \n",
    "- Proper splitting avoids **context loss**.  \n",
    "- Recommended chunk size: 500â€“1000 tokens for GPT-3/GPT-4.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Embedding Techniques**\n",
    "\n",
    "| Type | Description | Example Models | Use Case |\n",
    "|------|------------|---------------|---------|\n",
    "| **OpenAI Embeddings** | High-quality general-purpose text vectors | `text-embedding-3-large`, `text-embedding-3-small` | Semantic search, RAG |\n",
    "| **HuggingFace Transformers** | Local embedding models | `sentence-transformers/all-MiniLM-L6-v2` | Open-source, private deployments |\n",
    "| **Cohere Embeddings** | Cloud-based semantic embeddings | `multilingual-22-12` | Multi-language support |\n",
    "| **Custom Domain Embeddings** | Fine-tuned embeddings | BioGPT, Finance-specific | Domain-specific retrieval |\n",
    "\n",
    "**Notes:**  \n",
    "- Embeddings convert text â†’ numeric vectors for similarity search.  \n",
    "- Quality impacts RAG accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Vector Store / DB Techniques**\n",
    "\n",
    "| Vector Store | Description | Features | Example |\n",
    "|--------------|------------|---------|---------|\n",
    "| **FAISS** | Local vector similarity search | Fast, scalable, offline | Small-medium datasets |\n",
    "| **Chroma** | Open-source, local or cloud | Persistent storage, embeddings integration | Quick RAG prototyping |\n",
    "| **Pinecone** | Managed vector DB (cloud) | Auto-scaling, multi-dimensional | Production-grade retrieval |\n",
    "| **Weaviate** | Open-source + GraphQL | Schema-based, hybrid search | RAG + semantic search + metadata filtering |\n",
    "| **Milvus** | High-performance, scalable | GPU support, hybrid search | Large-scale retrieval |\n",
    "\n",
    "**Notes:**  \n",
    "- Choose based on dataset size, latency, persistence, and cloud/on-prem needs.  \n",
    "- Supports top-k similarity queries (cosine / dot-product).  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Python Example â€” Full Pipeline**\n",
    "\n",
    "```python\n",
    "# pip install langchain openai faiss-cpu PyPDF2 sentence-transformers\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1) Data Ingestion\n",
    "loader = PyPDFLoader(\"sample_doc.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2) Text Splitting\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# 3) Embedding Generation\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 4) Vector Store Query Example\n",
    "query = \"Explain LangChain architecture\"\n",
    "docs = vector_db.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\", doc.page_content[:300], \"...\\n\")\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Best Practices**\n",
    "\n",
    "* **Chunking:** Avoid context loss by overlapping chunks slightly (50â€“100 tokens).\n",
    "* **Embedding Selection:** Use domain-tuned embeddings for specialized knowledge.\n",
    "* **Vector DB:**\n",
    "\n",
    "  * FAISS â†’ Local experiments\n",
    "  * Pinecone/Weaviate â†’ Scalable production\n",
    "* **Pipeline Optimization:**\n",
    "\n",
    "  * Cache embeddings for static documents\n",
    "  * Use async ingestion for large datasets\n",
    "  * Monitor vector DB size and query latency\n",
    "\n",
    "---\n",
    "\n",
    "âœ… *Quick Review:*\n",
    "\n",
    "**LangChain Data Pipeline = Ingest â†’ Split â†’ Embed â†’ Store**\n",
    "\n",
    "* Ensures LLMs can **efficiently retrieve and reason over large datasets**.\n",
    "* Critical for **RAG workflows, chatbots, and enterprise GenAI applications**.\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed07fb0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
